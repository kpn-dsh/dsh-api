[processor]
processor-technology = "dshservice"
processor-realization-id = "replicator"
label = "Topic replicator"
description = "Replicates records from one Kafka topic to another. Allows multiple inbound and outbound topics."
version = "0.0.1-SNAPSHOT"
icon = "replicator"
tags = ["replicator"]
metrics-url = "${MONITORING_URL}"
viewer-url = "${CONSOLE_URL}/#/profiles/${TENANT}"

[inbound-junctions.inbound-kafka-topic]
label = "Source topics"
description = "Kafka topic that the replicator will consume from."
minimum-number-of-resources = 1
maximum-number-of-resources = 5
allowed-resource-types = ["dshtopic"]

[outbound-junctions.outbound-kafka-topic]
label = "Sink topics"
description = "Kafka topic that the replicator will produce to."
minimum-number-of-resources = 1
maximum-number-of-resources = 5
allowed-resource-types = ["dshtopic"]

[[deploy.parameters]]
type = "boolean"
id = "metrics-enabled"
label = "Enable metrics"
description = "Enable prometheus metrics."
default = "false"

[[deploy.parameters]]
type = "selection"
id = "kafka-offset-reset"
label = "Kafka read position"
description = "Start position of where to read from."
options = [
    { id = "earliest", label = "Beginning", description = "Start reading from the begininng of the topic." },
    { id = "latest", label = "End", description = "Start reading at the end of the topic." },
]

[dshservice]
image = 'registry.cp.kpn-dsh.com/greenbox-dev/replicator:0.0.1-SNAPSHOT'
needs-token = true
single-instance = false

[dshservice.environment-variables]
KAFKA_SOURCE_TOPIC = { type = "inbound-junction", id = "inbound-kafka-topic" }
KAFKA_TARGET_TOPIC = { type = "outbound-junction", id = "outbound-kafka-topic" }
KAFKA_AUTO_OFFSET_RESET = { type = "deployment-parameter", id = "kafka-offset-reset" }
KAFKA_CONSUMER_GROUP_TYPE = { type = "value", value = "SHARED" }
METRICS_ENABLED = { type = "deployment-parameter", id = "metrics-enabled" }

[[dshservice.profiles]]
id = "minimal"
label = "10K records per minute"
description = "Minimal profile to handle 0 to 10k records per minute."
cpus = 0.1
instances = 1
mem = 128
environment-variables.KAFKA_CONSUMER_QUEUED_BUFFERING_MAX_MESSAGES_KBYTES = { type = "value", value = "40000" }
environment-variables.KAFKA_PRODUCER_BATCH_NUM_MESSAGES = { type = "value", value = "500" }
environment-variables.KAFKA_PRODUCER_QUEUE_BUFFERING_MAX_MESSAGES = { type = "value", value = "1000" }
environment-variables.KAFKA_PRODUCER_QUEUE_BUFFERING_MAX_KBYTES = { type = "value", value = "40000" }

[[dshservice.profiles]]
id = "medium"
label = "100K records per minute"
description = "Medium profile to handle 10k to 100k records per minute."
cpus = 0.1
instances = 1
mem = 348
environment-variables.KAFKA_CONSUMER_QUEUED_BUFFERING_MAX_MESSAGES_KBYTES = { type = "value", value = "65536" }
environment-variables.KAFKA_PRODUCER_BATCH_NUM_MESSAGES = { type = "value", value = "750" }
environment-variables.KAFKA_PRODUCER_QUEUE_BUFFERING_MAX_MESSAGES = { type = "value", value = "1250" }
environment-variables.KAFKA_PRODUCER_QUEUE_BUFFERING_MAX_KBYTES = { type = "value", value = "80000" }

[[dshservice.profiles]]
id = "maximal"
label = "1 Million records per minute"
description = "Maximal profile to handle 1 million maximum records per minute."
cpus = 0.2
instances = 1 # could  increased
mem = 512
environment-variables.KAFKA_CONSUMER_QUEUED_BUFFERING_MAX_MESSAGES_KBYTES = { type = "value", value = "65536" }
environment-variables.KAFKA_PRODUCER_BATCH_NUM_MESSAGES = { type = "value", value = "1000" }
environment-variables.KAFKA_PRODUCER_QUEUE_BUFFERING_MAX_MESSAGES = { type = "value", value = "1500" }
environment-variables.KAFKA_PRODUCER_QUEUE_BUFFERING_MAX_KBYTES = { type = "value", value = "1048576" }
