[
  {
    "draft": false,
    "lastModified": 1697190601029.0,
    "payload": "{\"id\":\"kpn/keyring-kafka-database-extractor\",\"name\":\"DSH Database Extractor\",\"version\":\"0.2.4\",\"vendor\":\"KPN\",\"kind\":\"manifest\",\"apiVersion\":\"v0-alpha\",\"description\":\"Application to bulk load/sync records from a relational database to Kafka in DSH\",\"moreInfo\":\"## Database Extractor for Apache Kafka in DSH \\nAn application to import data from any relational database with a JDBC driver into an Apache Kafka topic by extending the [Kafka Connect JDBC Source connector](https://docs.confluent.io/current/connect/kafka-connect-jdbc/source-connector/index.html) project. \\n\\n**1. Data Source:** Currently, we support Teradata, Postgres (includes Yugabyte), MySQL, Oracle, SQLite, SAP Sybase, Microsoft SQL Server and IBM DB2.\\n\\n**2. Kafka JDBC Database Extractor Application:** The application can be deployed with single or multiple workers. Cluster mode ensures fault tolerancy and HA. The data extraction mode can be configured by setting the `MODE` environment variable. These can be bulk, timestamp, incrementing or timestamp+incrementing. Furthermore, the `POLL_INTERVAL` environment variable can be used to determine the degree of realtime sync or periodicity of sync. \\n\\n**3. Data Sink:** The data is stored in JSON format in the kafka topic `DATABASE_OUTPUT_TOPIC`.\\n ### More Information\\nThe _Greenbox Team_ can be reached at **greenbox@kpn.com** for more information.\",\"contact\":\"greenbox@kpn.com\",\"configuration\":{\"$schema\":\"https://json-schema.org/draft/2019-09/schema\",\"type\":\"object\",\"properties\":{\"DATABASE\":{\"description\":\"The database to connect to. Currently, we support Teradata, Postgres (includes Yugabyte), MySQL, Oracle, SQLite, SAP Sybase, Microsoft SQL Server and IBM DB2.\",\"type\":\"string\",\"enum\":[\"teradata\",\"postgresql\",\"mysql\",\"oracle\",\"sqlite\",\"sybase\",\"sqlserver\",\"db2\"],\"default\":\"teradata\"},\"DATABASE_JDBC_URL\":{\"description\":\"The database host to connect to. Ensure that the host is accessible from the application. Example: jdbc:teradata://tdp.kpn.org\",\"type\":\"string\",\"default\":\"jdbc:teradata://tdp.kpn.org\"},\"DATABASE_OUTPUT_TOPIC\":{\"description\":\"Name of the Kafka topic to write the records to.\",\"type\":\"string\",\"default\":\"scratch.database-extractor-output.greenbox\"},\"DATABASE_PASSWORD_FIELD_IN_SECRET_STORE\":{\"description\":\"Name of the field in the DSH secrets store that contains the Database password.\",\"type\":\"string\"},\"DATABASE_QUERY\":{\"description\":\"The SQL query to execute. The query should be a valid SQL query for the database specified in the `DATABASE` environment variable.\",\"type\":\"string\",\"default\":\"SELECT * FROM \\u003ctable_name\\u003e\"},\"DATABASE_TIMEZONE\":{\"description\":\"Used to recognize the timezone of timestamp values. The default is UTC. KPN Teradata uses Europe/Amsterdam.\",\"type\":\"string\",\"default\":\"UTC\"},\"DATABASE_USER\":{\"description\":\"Database user name. Make sure this user has the right permissions to execute the query.\",\"type\":\"string\"},\"INCREMENTING_COLUMN_NAME\":{\"description\":\"This is used in the `incrementing` and `timestamp+incrementing` extraction modes. Only relevant for these two modes. The column name should be a valid autoincrementing column in the database table.\",\"type\":\"string\",\"default\":\"\"},\"INSTANCES\":{\"description\":\"The number of instances to deploy in distributed mode (cluster). The default is 1.\",\"type\":\"string\",\"enum\":[\"1\",\"2\",\"3\",\"4\",\"5\"],\"default\":\"1\"},\"LOG_LEVEL\":{\"description\":\"The log level for the application. The default is INFO.\",\"type\":\"string\",\"enum\":[\"TRACE\",\"DEBUG\",\"INFO\",\"WARN\",\"ERROR\"],\"default\":\"INFO\"},\"MODE\":{\"description\":\"The data extraction mode can be `bulk`, `timestamp`, `incrementing` and `timestamp+incrementing`. Bulk is used for batch extraction whereas the other modes are used for realtime extraction.\",\"type\":\"string\",\"enum\":[\"bulk\",\"timestamp\",\"incrementing\",\"timestamp+incrementing\"],\"default\":\"timestamp\"},\"POLL_INTERVAL\":{\"description\":\"Interval in milliseconds for polling (24 hours is 86400000 milliseconds).\",\"type\":\"string\",\"default\":\"60000\"},\"TIMESTAMP_COLUMN_NAME\":{\"description\":\"This is used in the `timestamp` and `timestamp+incrementing` extraction modes. Only relevant for these two modes. The column name should be a valid date/timestamp column in the database table.\",\"type\":\"string\",\"default\":\"\"}}},\"resources\":{\"allocation/${@tenant}/application/${@name}\":{\"cpus\":0.1,\"env\":{\"DATABASE\":\"${DATABASE}\",\"DATABASE_JDBC_URL\":\"${DATABASE_JDBC_URL}\",\"DATABASE_OUTPUT_TOPIC\":\"${DATABASE_OUTPUT_TOPIC}\",\"DATABASE_QUERY\":\"${DATABASE_QUERY}\",\"DATABASE_TIMEZONE\":\"${DATABASE_TIMEZONE}\",\"DATABASE_USER\":\"${DATABASE_USER}\",\"DEPLOYMENT\":\"distributed\",\"DISTRIBUTED_MODE_CONFIG_TOPIC\":\"scratch.${@name}-config.${@tenant}\",\"DISTRIBUTED_MODE_OFFSETS_TOPIC\":\"scratch.${@name}-offset.${@tenant}\",\"DISTRIBUTED_MODE_STATUS_TOPIC\":\"scratch.${@name}-status.${@tenant}\",\"INCREMENTING_COLUMN_NAME\":\"${INCREMENTING_COLUMN_NAME}\",\"KAFKA_OPTS\":\"-Xms256M -Xmx1G\",\"LOG_LEVEL\":\"${LOG_LEVEL}\",\"MODE\":\"${MODE}\",\"POLL_INTERVAL\":\"${POLL_INTERVAL}\",\"TIMESTAMP_COLUMN_NAME\":\"${TIMESTAMP_COLUMN_NAME}\"},\"image\":\"${@appcatalog}/release/kpn/keyring-kafka-database-extractor:0.2.4\",\"imageConsole\":\"registry.cp.kpn-dsh.com/greenbox/keyring-kafka-database-extractor:0.2.4\",\"instances\":\"${INSTANCES | number}\",\"mem\":1024,\"metrics\":{\"path\":\"/\",\"port\":9009},\"name\":\"${@name}\",\"needsToken\":true,\"secrets\":[{\"injections\":[{\"env\":\"DATABASE_PASSWORD\"}],\"name\":\"${DATABASE_PASSWORD_FIELD_IN_SECRET_STORE}\"}],\"singleInstance\":false,\"user\":\"${@uid}:${@gid}\"},\"allocation/${@tenant}/application/${@name}-ui\":{\"cpus\":0.1,\"env\":{\"CONNECT_URL\":\"${@name}:8083\",\"PORT\":\"8000\"},\"exposedPorts\":{\"8000\":{\"auth\":\"system-fwd-auth@view,manage\",\"tls\":\"auto\",\"vhost\":\"{ vhost('${@name}.${@tenant}', 'public') }\"}},\"image\":\"${@appcatalog}/release/kpn/keyring-kafka-database-extractor-ui:0.2.4\",\"imageConsole\":\"registry.cp.kpn-dsh.com/greenbox/keyring-kafka-database-extractor-ui:0.2.4\",\"instances\":1,\"mem\":16,\"name\":\"${@name}\",\"needsToken\":false,\"singleInstance\":false,\"user\":\"${@uid}:${@gid}\"},\"allocation/${@tenant}/topic/${@name}-config\":{\"name\":\"${@name}-config\",\"partitions\":1,\"replicationFactor\":3},\"allocation/${@tenant}/topic/${@name}-offset\":{\"name\":\"${@name}-offset\",\"partitions\":5,\"replicationFactor\":3},\"allocation/${@tenant}/topic/${@name}-status\":{\"name\":\"${@name}-status\",\"partitions\":5,\"replicationFactor\":3},\"allocation/${@tenant}/vhost/${@name}.${@tenant}@public\":\"${@name}.${@tenant}@public\"}}"
  },
  {
    "draft": false,
    "lastModified": 1749824388133.0,
    "payload": "{\"id\":\"kpn/prometheus-scraper\",\"name\":\"Prometheus Scraper\",\"version\":\"0.1.6\",\"vendor\":\"KPN\",\"kind\":\"manifest\",\"apiVersion\":\"v0-alpha\",\"description\":\"Application to scrape Prometheus metrics to Kafka in DSH\",\"moreInfo\":\"## Prometheus Scraper to Kafka \\n\\n  \\n\\n **1. Prometheus URL**: Prometheus endpoint to scrape metrics from. If you do not change the default value `tenant-prometheus-server`, it will be automatically assigned to your tenant's Prometheus server. \\n\\n **2. Kafka Topic**: Kafka topic name for producing metrics to. Can be configured using `KAFKA_TOPIC`. \\n\\n **3. Scraper Interval Seconds**: Interval in seconds at which metrics are scraped. The default interval is 15 secs. Too frequent intervals might cause issues on Prometheus server. So, less than 15 seconds is not allowed and will be reaasigned to 15 secs as minimal interval seconds. Set the interval using `SCRAPE_INTERVAL_SECS`. \\n\\n **4. Prometheus Queries**: Comma-separated Prometheus queries. Default value: 'up,node_cpu_seconds_total'. Can be configured using `PROMETHEUS_QUERIES` variable. \\n\\n **5. Max Retries**: Maxiumum number of retries in the case of failure of scraping metrics. Set to 0 if you do not want to retry. \\n\\n **6. Scaling**: Use `CPU` \\u0026 `MEMORY` values for vertical scaling. \\n\\n ## More Information \\n The Unibox Team can be reached at unibox@kpn.com for more information.\",\"contact\":\"unibox@kpn.com\",\"configuration\":{\"$schema\":\"https://json-schema.org/draft/2019-09/schema\",\"type\":\"object\",\"properties\":{\"CPU\":{\"description\":\"Amount of CPU cores per instance.\",\"type\":\"string\",\"default\":\"0.1\"},\"KAFKA_TOPIC\":{\"description\":\"Kafka Topic name for producing metrics to.\",\"type\":\"string\",\"default\":\"scratch.scraper-metrics.connectors\"},\"LOG_LEVEL\":{\"description\":\"The log level for the application. The default is INFO.\",\"type\":\"string\",\"enum\":[\"TRACE\",\"DEBUG\",\"INFO\",\"WARN\",\"ERROR\"],\"default\":\"INFO\"},\"MAX_RETRIES\":{\"description\":\"Maximum number of retries in the case of failure of scraping metrics. Set to 0 if you do not want to retry.\",\"type\":\"string\",\"enum\":[\"0\",\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\"],\"default\":\"1\"},\"MEMORY\":{\"description\":\"Amount of memory per instance.\",\"type\":\"string\",\"default\":\"16\"},\"PROMETHEUS_ENDPOINT\":{\"description\":\"Prometheus endpoint URL. If you do not change the default value, it will be automatically assigned to your tenant Prometheus server.\",\"type\":\"string\",\"default\":\"tenant-prometheus-server\"},\"PROMETHEUS_QUERIES\":{\"description\":\"Comma-separated Prometheus queries.\",\"type\":\"string\",\"default\":\"up,node_cpu_seconds_total\"},\"SCRAPE_INTERVAL_SECS\":{\"description\":\"Interval in seconds at which metrics are scraped. The default interval is 15 secs. Too frequent intervals might cause issues on Prometheus server. So, less than 15 seconds is not allowed and will be reaasigned to 15 secs (min interval).\",\"type\":\"string\",\"default\":\"15\"}}},\"resources\":{\"allocation/${@tenant}/application/${@name}\":{\"cpus\":\"${CPU | number}\",\"env\":{\"KAFKA_TOPIC\":\"${KAFKA_TOPIC}\",\"MAX_RETRIES\":\"${MAX_RETRIES}\",\"PROMETHEUS_ENDPOINT\":\"${PROMETHEUS_ENDPOINT}\",\"PROMETHEUS_QUERIES\":\" ${PROMETHEUS_QUERIES}\",\"RUST_LOG\":\"${LOG_LEVEL}\",\"SCRAPE_INTERVAL_SECS\":\"${SCRAPE_INTERVAL_SECS}\"},\"image\":\"${@appcatalog}/release/kpn/prometheus-scraper:0.1.6\",\"imageConsole\":\"registry.cp.kpn-dsh.com/connectors/prometheus-scraper:0.1.6\",\"instances\":1,\"mem\":\"${MEMORY | number}\",\"metrics\":{\"path\":\"/metrics\",\"port\":8080},\"name\":\"${@name}\",\"needsToken\":true,\"singleInstance\":false,\"user\":\"${@uid}:${@gid}\"}}}"
  },
  {
    "draft": false,
    "lastModified": 1758308328786.0,
    "payload": "{\"id\":\"kpn/explorer\",\"name\":\"Explorer\",\"version\":\"0.2.4\",\"vendor\":\"KPN\",\"kind\":\"manifest\",\"apiVersion\":\"v0-alpha\",\"description\":\"An application that helps you explore data in your DSH tenant across various locations, such as Kafka topics or S3 buckets.\",\"moreInfo\":\"## Explorer \\nAn application to explore Apache Kafka topics and S3 buckets. \\n\\n### Features \\n- Explore Apache Kafka topics \\n- Explore S3 buckets\",\"contact\":\"unibox@kpn.com\",\"configuration\":{\"$schema\":\"https://json-schema.org/draft/2019-09/schema\",\"type\":\"object\",\"properties\":{\"CPU\":{\"description\":\"Amount of CPU cores per instance.\",\"type\":\"string\",\"default\":\"1\"},\"LOG_LEVEL\":{\"description\":\"The log level for the application. The default is INFO.\",\"type\":\"string\",\"enum\":[\"trace\",\"debug\",\"info\",\"warn\",\"error\"],\"default\":\"info\"},\"MEMORY\":{\"description\":\"Amount of memory per instance.\",\"type\":\"string\",\"default\":\"2048\"},\"VHOST_DNS_ZONE\":{\"description\":\"The DNS zone for the application: kpn.com (public) and kpn.org (private)\",\"type\":\"string\",\"enum\":[\"public\",\"private\"],\"default\":\"private\"}}},\"resources\":{\"allocation/${@tenant}/application/${@name}\":{\"cpus\":\"${CPU | number}\",\"env\":{\"DSH_ENVIRONMENT\":\"{ variables('DSH_ENVIRONMENT') }\",\"DSH_GID\":\"${@gid}\",\"DSH_PLATFORM_REGION\":\"{ variables('DSH_PLATFORM_REGION') }\",\"DSH_TENANT\":\"{ variables('DSH_TENANT') }\",\"DSH_UID\":\"${@uid}\",\"MAXIMUM_FILE_SIZE_BYTE\":\"1024000\",\"RUST_LOG\":\"${LOG_LEVEL}\",\"SERVER_IP_ADDRESS\":\"0.0.0.0:4000\"},\"exposedPorts\":{\"4000\":{\"auth\":\"system-fwd-auth@view,manage\",\"tls\":\"auto\",\"vhost\":\"{ vhost('${@name}.${@tenant}', '${VHOST_DNS_ZONE}') }\"}},\"image\":\"${@appcatalog}/release/kpn/greenbox-compact:0.2.4\",\"instances\":1,\"mem\":\"${MEMORY | number}\",\"metrics\":{\"path\":\"/\",\"port\":9009},\"name\":\"${@name}\",\"needsToken\":true,\"secrets\":[{\"injections\":[{\"env\":\"DSH_CLIENT_SECRET\"}],\"name\":\"system/rest-api-client\"},{\"injections\":[{\"env\":\"BUCKET_IDENTIFIER\"}],\"name\":\"system/objectstore/access_key_id\"},{\"injections\":[{\"env\":\"BUCKET_SECRET\"}],\"name\":\"system/objectstore/secret_access_key\"}],\"singleInstance\":true,\"user\":\"${@uid}:${@gid}\"},\"allocation/${@tenant}/bucket/${@name}\":{\"encrypted\":true,\"name\":\"${@name}\",\"versioned\":false},\"allocation/${@tenant}/vhost/${@name}.${@tenant}@${VHOST_DNS_ZONE}\":\"${@name}.${@tenant}@${VHOST_DNS_ZONE}\"}}"
  },
  {
    "draft": false,
    "lastModified": 1706273545394.0,
    "payload": "{\"id\":\"kpn/zookeeper-proxy\",\"name\":\"Zookeeper Proxy\",\"version\":\"1.2.2\",\"vendor\":\"KPN\",\"kind\":\"manifest\",\"apiVersion\":\"v0-alpha\",\"description\":\"Tenant proxy to Zookeeper\",\"moreInfo\":\"## Features  \\n- Transparent proxy to the platform-provided Zookeeper. \\n- Data you store via this proxy in Zookeeper is fully isolated to your tenant.\\n## How to connect\\nWhen you need a Zookeeper in your setup, you can directly connect to this proxy as you would to any regular Zookeeper deployment, using the Zookeeper client of your preference. \\nTo connect to this proxy, simply configure your client with the following connection endpoint: `zk-\\u003czookeeper-proxy-name\\u003e.\\u003ctenant\\u003e.marathon.mesos:2181`\\n## Release Notes  \\n- Support added for checkWatches (opcode 17) and removeWatches (opcode 18) Zookeeper operations\",\"contact\":\"dsh-appcatalog@kpn.com\",\"configuration\":{\"$schema\":\"https://json-schema.org/draft/2019-09/schema\",\"type\":\"object\",\"properties\":{\"TRACE_LOGGING\":{\"description\":\"Enable or disable trace logging\",\"type\":\"string\",\"enum\":[\"true\",\"false\"],\"default\":\"false\"}}},\"resources\":{\"allocation/${@tenant}/application/zk-${@name}\":{\"cpus\":0.1,\"env\":{\"PORT\":\"2181\",\"PREFIX\":\"__tenant_${@tenant}\",\"SERVERS\":\"tenant-zookeeper-1.dsh.marathon.mesos:2181,tenant-zookeeper-2.dsh.marathon.mesos:2181,tenant-zookeeper-3.dsh.marathon.mesos:2181\",\"TRACE\":\"${TRACE_LOGGING}\"},\"image\":\"${@appcatalog}/release/kpn/zookeeper-proxy:0.3.1\",\"instances\":1,\"mem\":256,\"name\":\"zk-${@name}\",\"needsToken\":false,\"network\":\"enclave\",\"singleInstance\":true,\"user\":\"${@uid}:${@gid}\"}}}"
  },
  {
    "draft": false,
    "lastModified": 1718716634181.0,
    "payload": "{\"id\":\"kpn/http-source-connector\",\"name\":\"HTTP Source Connector\",\"version\":\"0.5.2\",\"vendor\":\"KPN\",\"kind\":\"manifest\",\"apiVersion\":\"v0-alpha\",\"description\":\"Application to produce data to Kafka via HTTP in DSH\",\"moreInfo\":\"## HTTP Source Kafka Connector with Oauth2 \\n\\n HTTP Source Kafka Connector with OAuth2 enables streaming data to DSH Kafka via HTTP with Digital Engine's AuthZ OAuth2 authentication. \\n\\n **1. HTTP Endpoint (Vhost)**: This will be automatically created during deployment based on the application name. It can be made `public` (kpn.com) or `private` (kpn.org) using the `VHOST_DNS_ZONE` environment variable. \\n\\n **2. OAuth2 Authentication**: You can choose the authz environment via `AUTHZ_ENVIRONMENT`. If you choose ACC (acceptance), the base url will be  https://api.acc.kpn.com and for PROD (production) it will be https://api.kpn.com . The fetching token endpoint for these urls is _oauth2/v1/token_ and validating token endpoint is _authz/v1/validate_. These will be arranged inside application according to `AUTHZ_ENVIRONMENT`. Authz Client ID, and Authz Client Secret should be stored in Secrets. These secret names must be specified in the deployment page. `AUTHZ_CLIENT_ID_IN_SECRET_STORE` and `AUTHZ_CLIENT_SECRET_IN_SECRET_STORE` are the secret names for the client ID and client secret, respectively. Please make sure you are using correct client-id and client-secret for your Authz environment. `AUTHZ_SCOPE` is for scope name for your application. Authorization is ensured via scopes which validates if the authenticated user has permission to produce data. Multiple scopes can be entered comma seperated. \\n\\n **3. URL Endpoint - Destination Topic - Data Format mapping string**: This establishes multiple endpoints for sending data and maps destination topics and data formats using a colon-separated format. You need to decide what to name your endpoint which you will use in your application to send data. The topic needs to be created in Topics section. Multiple mappings should be separated by commas. This could be set using the `OUTPUT_TOPIC_FORMATS` environment variable and it should be in the format of `\\u003cendpoint1\\u003e:\\u003ctopic1\\u003e:\\u003cformat1\\u003e`. The format can be `json`, `avro`, `protobuf`, `byte`, or `legacy`. JSON, Avro, and Protobuf correspond to the respective data formats (We support avro-byte encoding and we use Confluent standard for Avro \\u0026 Protobuf serialization). Byte is for raw byte data. Legacy format is for the old data format which adds a metadata wrapper with client ID around JSON data. Example - 'publish-json:scratch.json-topic.tenant_name:json'. \\n\\n **4. Dead Letter Queue**: Topic for storing failed messages during serialization/deserialization and can be configured using `DEAD_LETTER_TOPIC`. \\n\\n **5. Scaling**: Use `INSTANCES` count for horizontal scaling and `CPU` \\u0026 `MEMORY` values for vertical scaling. \\n\\n ## More Information \\n The Unibox Team can be reached at unibox@kpn.com for more information.\",\"contact\":\"unibox@kpn.com\",\"configuration\":{\"$schema\":\"https://json-schema.org/draft/2019-09/schema\",\"type\":\"object\",\"properties\":{\"AUTHZ_CLIENT_ID_IN_SECRET_STORE\":{\"description\":\"Name of the field in the DSH secrets store that contains the Authz Client ID.\",\"type\":\"string\"},\"AUTHZ_CLIENT_SECRET_IN_SECRET_STORE\":{\"description\":\"Name of the field in the DSH secrets store that contains the Authz Client Secret.\",\"type\":\"string\"},\"AUTHZ_ENVIRONMENT\":{\"description\":\"The environment value for Authz Digital Engine.\",\"type\":\"string\",\"enum\":[\"ACC\",\"PROD\"],\"default\":\"ACC\"},\"AUTHZ_SCOPE\":{\"description\":\"DE application scopes for authorization purposes\",\"type\":\"string\",\"default\":\"scope1,scope2\"},\"CPU\":{\"description\":\"Amount of CPU cores per instance.\",\"type\":\"string\",\"default\":\"0.1\"},\"DEAD_LETTER_TOPIC\":{\"description\":\"Topic for failed messages during serialization/deseralization\",\"type\":\"string\",\"default\":\"scratch.dlq.tenant\"},\"INSTANCES\":{\"description\":\"The number of instances to deploy for scaling. The default is 1.\",\"type\":\"string\",\"enum\":[\"1\",\"2\",\"3\",\"4\",\"5\"],\"default\":\"1\"},\"LOG_LEVEL\":{\"description\":\"The log level for the application. The default is INFO.\",\"type\":\"string\",\"enum\":[\"TRACE\",\"DEBUG\",\"INFO\",\"WARN\",\"ERROR\"],\"default\":\"INFO\"},\"MEMORY\":{\"description\":\"Amount of memory per instance.\",\"type\":\"string\",\"default\":\"64\"},\"OUTPUT_TOPIC_FORMATS\":{\"description\":\"Define endpoint, topic and data format mapping to declare where to publish data to. Mapping format is like following, endpoint:topic_name:data_format can be added more sepearting with comma.\",\"type\":\"string\",\"default\":\"endpoint1:scratch.topic1.tenant:byte\"},\"VHOST_DNS_ZONE\":{\"description\":\"DNS Zone selection for Vhost (kpn.com or kpn.org)\",\"type\":\"string\",\"enum\":[\"public\",\"private\"],\"default\":\"private\"}}},\"resources\":{\"allocation/${@tenant}/application/${@name}\":{\"cpus\":\"${CPU | number}\",\"env\":{\"AUTHZ_ENVIRONMENT\":\"${AUTHZ_ENVIRONMENT}\",\"AUTHZ_SCOPE\":\"${AUTHZ_SCOPE}\",\"DEAD_LETTER_TOPIC\":\"${DEAD_LETTER_TOPIC}\",\"ENABLE_OAUTH2\":\"true\",\"LOG_LEVEL\":\"${LOG_LEVEL}\",\"OUTPUT_TOPIC_FORMATS\":\"${OUTPUT_TOPIC_FORMATS}\"},\"exposedPorts\":{\"3000\":{\"vhost\":\"{ vhost('${@name}.${@tenant}', '${VHOST_DNS_ZONE}') }\"}},\"image\":\"${@appcatalog}/release/kpn/http-kafka-connector:0.5.2\",\"imageConsole\":\"registry.cp.kpn-dsh.com/greenbox-training/http-kafka-connector:0.5.2\",\"instances\":\"${INSTANCES | number}\",\"mem\":\"${MEMORY | number}\",\"metrics\":{\"path\":\"/metrics\",\"port\":9090},\"name\":\"${@name}\",\"needsToken\":true,\"secrets\":[{\"injections\":[{\"env\":\"AUTHZ_CLIENT_ID\"}],\"name\":\"${AUTHZ_CLIENT_ID_IN_SECRET_STORE}\"},{\"injections\":[{\"env\":\"AUTHZ_CLIENT_SECRET\"}],\"name\":\"${AUTHZ_CLIENT_SECRET_IN_SECRET_STORE}\"}],\"singleInstance\":false,\"user\":\"${@uid}:${@gid}\"},\"allocation/${@tenant}/vhost/${@name}.${@tenant}@${VHOST_DNS_ZONE}\":\"${@name}.${@tenant}@${VHOST_DNS_ZONE}\"}}"
  },
  {
    "draft": false,
    "lastModified": 1757527787267.0,
    "payload": "{\"id\":\"kpn/explorer\",\"name\":\"Explorer\",\"version\":\"0.2.2\",\"vendor\":\"KPN\",\"kind\":\"manifest\",\"apiVersion\":\"v0-alpha\",\"description\":\"An application that helps you explore data in your DSH tenant across various locations, such as Kafka topics or S3 buckets.\",\"moreInfo\":\"## Explorer \\nAn application to explore Apache Kafka topics and S3 buckets. \\n\\n### Features \\n- Explore Apache Kafka topics \\n- Explore S3 buckets\",\"contact\":\"unibox@kpn.com\",\"configuration\":{\"$schema\":\"https://json-schema.org/draft/2019-09/schema\",\"type\":\"object\",\"properties\":{\"CPU\":{\"description\":\"Amount of CPU cores per instance.\",\"type\":\"string\",\"default\":\"1\"},\"LOG_LEVEL\":{\"description\":\"The log level for the application. The default is INFO.\",\"type\":\"string\",\"enum\":[\"trace\",\"debug\",\"info\",\"warn\",\"error\"],\"default\":\"info\"},\"MEMORY\":{\"description\":\"Amount of memory per instance.\",\"type\":\"string\",\"default\":\"2048\"},\"VHOST_DNS_ZONE\":{\"description\":\"The DNS zone for the application: kpn.com (public) and kpn.org (private)\",\"type\":\"string\",\"enum\":[\"public\",\"private\"],\"default\":\"private\"}}},\"resources\":{\"allocation/${@tenant}/application/${@name}\":{\"cpus\":\"${CPU | number}\",\"env\":{\"DSH_ENVIRONMENT\":\"{ variables('DSH_ENVIRONMENT') }\",\"DSH_GID\":\"${@gid}\",\"DSH_PLATFORM_REGION\":\"{ variables('DSH_PLATFORM_REGION') }\",\"DSH_TENANT\":\"{ variables('DSH_TENANT') }\",\"DSH_UID\":\"${@uid}\",\"MAXIMUM_FILE_SIZE_BYTE\":\"1024000\",\"RUST_LOG\":\"${LOG_LEVEL}\",\"SERVER_IP_ADDRESS\":\"0.0.0.0:4000\"},\"exposedPorts\":{\"3000\":{\"auth\":\"system-fwd-auth@view,manage\",\"tls\":\"auto\",\"vhost\":\"{ vhost('${@name}.${@tenant}', '${VHOST_DNS_ZONE}') }\"}},\"image\":\"${@appcatalog}/release/kpn/greenbox-compact:0.2.2\",\"instances\":1,\"mem\":\"${MEMORY | number}\",\"metrics\":{\"path\":\"/\",\"port\":9009},\"name\":\"${@name}\",\"needsToken\":true,\"secrets\":[{\"injections\":[{\"env\":\"DSH_CLIENT_SECRET\"}],\"name\":\"system/rest-api-client\"},{\"injections\":[{\"env\":\"BUCKET_IDENTIFIER\"}],\"name\":\"system/objectstore/access_key_id\"},{\"injections\":[{\"env\":\"BUCKET_SECRET\"}],\"name\":\"system/objectstore/secret_access_key\"}],\"singleInstance\":true,\"user\":\"${@uid}:${@gid}\"},\"allocation/${@tenant}/bucket/${@name}\":{\"encrypted\":true,\"name\":\"${@name}\",\"versioned\":false},\"allocation/${@tenant}/vhost/${@name}.${@tenant}@${VHOST_DNS_ZONE}\":\"${@name}.${@tenant}@${VHOST_DNS_ZONE}\"}}"
  },
  {
    "draft": false,
    "lastModified": 1726836522793.0,
    "payload": "{\"id\":\"kpn/dsh-database-ingester\",\"name\":\"DSH Database Ingester\",\"version\":\"0.4.6\",\"vendor\":\"KPN\",\"kind\":\"manifest\",\"apiVersion\":\"v0-alpha\",\"description\":\"Application to stream events from a Kafka topic to a relational database.\",\"moreInfo\":\"## Database Ingester for DSH \\nAn application to stream structured data from a Kafka topic in DSH to any relational database with a JDBC driver by extending the [Kafka Connect JDBC Sink Connector](https://docs.confluent.io/current/connect/kafka-connect-jdbc/sink-connector/index.html). \\n\\n**1. Data Source:** The source topic is configured via `DATABASE_SOURCE_TOPIC` and the `SCHEMA_TYPE` can be Avro, JSON or Protobuf and the schema should be registered in DSH Schema Store. A flat schema is recommended for the source topic so that the data can be easily mapped to the database table. If not, it will be flattened. Dynamic arrays are not supported. \\n\\n**2. Kafka JDBC Database Ingester Application:** The application can be deployed with single or multiple workers. Cluster mode ensures fault tolerancy and HA. Furthermore, vertical scalability can also be achieved by increasing the number of tasks (`TASK_COUNT`). \\n\\n**3. Data Sink:** Currently, we support SQL Server, Postgres, MySQL, Oracle, SQLite, Sybase and DB2 for `DATABASE_SINK_TABLE`. The application expects the table to be created beforehand with matching columns to that of the schema. Table auto creation and evolution is disabled by default but can be enabled via the UI provided. A `DEAD_LETTER_QUEUE_TOPIC` can also be configured. \\n ### More Information\\nThe _Unibox Team_ can be reached at **unibox@kpn.com** for more information.\",\"contact\":\"unibox@kpn.com\",\"configuration\":{\"$schema\":\"https://json-schema.org/draft/2019-09/schema\",\"type\":\"object\",\"properties\":{\"CPU\":{\"description\":\"Amount of CPU cores per instance.\",\"type\":\"string\",\"default\":\"0.1\"},\"DATABASE\":{\"description\":\"The database to connect to. Currently, we support Postgres (includes Yugabyte), MySQL, Oracle, SQLite, SAP Sybase, Microsoft SQL Server and IBM DB2.\",\"type\":\"string\",\"enum\":[\"teradata\",\"postgresql\",\"mysql\",\"oracle\",\"sqlite\",\"sybase\",\"sqlserver\",\"db2\"],\"default\":\"sqlserver\"},\"DATABASE_JDBC_URL\":{\"description\":\"The database host to connect to. Ensure that the host is accessible from the application. Example: jdbc:teradata://tdp.kpn.org\",\"type\":\"string\",\"default\":\"jdbc:teradata://tdp.kpn.org\"},\"DATABASE_PASSWORD_FIELD_IN_SECRET_STORE\":{\"description\":\"Name of the field in the DSH secrets store that contains the Database password.\",\"type\":\"string\"},\"DATABASE_SINK_TABLE\":{\"description\":\"Name of the table in the database to stream the records to.\",\"type\":\"string\",\"default\":\"TABLE_NAME\"},\"DATABASE_SOURCE_TOPIC\":{\"description\":\"Name of the Kafka topic to stream the records from.\",\"type\":\"string\",\"default\":\"scratch.database-ingester-source.greenbox\"},\"DATABASE_TIMEZONE\":{\"description\":\"Used to recognize the timezone of timestamp values. The default is UTC. KPN Teradata uses Europe/Amsterdam.\",\"type\":\"string\",\"default\":\"UTC\"},\"DATABASE_USER\":{\"description\":\"Database user name. Make sure this user has the right permissions to execute the query.\",\"type\":\"string\"},\"DEAD_LETTER_QUEUE_TOPIC\":{\"description\":\"Name of the Kafka topic to stream the dead letter records to in case of errors.\",\"type\":\"string\",\"default\":\"scratch.dead-letter-queue.greenbox\"},\"INSTANCES\":{\"description\":\"The number of instances to deploy in distributed mode (cluster). The default is 1.\",\"type\":\"string\",\"enum\":[\"1\",\"2\",\"3\",\"4\",\"5\"],\"default\":\"1\"},\"LOG_LEVEL\":{\"description\":\"The log level for the application. The default is INFO.\",\"type\":\"string\",\"enum\":[\"TRACE\",\"DEBUG\",\"INFO\",\"WARN\",\"ERROR\"],\"default\":\"INFO\"},\"MEMORY\":{\"description\":\"Amount of memory per instance.\",\"type\":\"string\",\"default\":\"1024\"},\"SCHEMA_TYPE\":{\"description\":\"Schema type of the serialized data in the topic.\",\"type\":\"string\",\"enum\":[\"avro\",\"json\",\"protobuf\"],\"default\":\"avro\"},\"TASK_COUNT\":{\"description\":\"The number of tasks to deploy for better parallelism\",\"type\":\"string\",\"enum\":[\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\"],\"default\":\"3\"}}},\"resources\":{\"allocation/${@tenant}/application/${@name}\":{\"cpus\":\"${CPU | number}\",\"env\":{\"DATABASE\":\"${DATABASE}\",\"DATABASE_JDBC_URL\":\"${DATABASE_JDBC_URL}\",\"DATABASE_SINK_TABLE\":\"${DATABASE_SINK_TABLE}\",\"DATABASE_SOURCE_TOPIC\":\"${DATABASE_SOURCE_TOPIC}\",\"DATABASE_TIMEZONE\":\"${DATABASE_TIMEZONE}\",\"DATABASE_USER\":\"${DATABASE_USER}\",\"DEAD_LETTER_QUEUE_TOPIC\":\"${DEAD_LETTER_QUEUE_TOPIC}\",\"DEPLOYMENT\":\"distributed\",\"DISTRIBUTED_MODE_CONFIG_TOPIC\":\"scratch.${@name}-config.${@tenant}\",\"DISTRIBUTED_MODE_OFFSETS_TOPIC\":\"scratch.${@name}-offset.${@tenant}\",\"DISTRIBUTED_MODE_STATUS_TOPIC\":\"scratch.${@name}-status.${@tenant}\",\"KAFKA_OPTS\":\"-Xms256M -Xmx1000M\",\"LOG_LEVEL\":\"${LOG_LEVEL}\",\"SCHEMA_REGISTRY_URL\":\"https://api.schema-store.dsh.marathon.mesos:8443\",\"SCHEMA_TYPE\":\"${SCHEMA_TYPE}\",\"TASK_COUNT\":\"${TASK_COUNT}\"},\"image\":\"${@appcatalog}/release/kpn/database-ingester-app:0.4.6\",\"imageConsole\":\"registry.cp.kpn-dsh.com/aep-dev/database-ingester-app:0.4.6\",\"instances\":\"${INSTANCES | number}\",\"mem\":\"${MEMORY | number}\",\"metrics\":{\"path\":\"/\",\"port\":9009},\"name\":\"${@name}\",\"needsToken\":true,\"secrets\":[{\"injections\":[{\"env\":\"DATABASE_PASSWORD\"}],\"name\":\"${DATABASE_PASSWORD_FIELD_IN_SECRET_STORE}\"}],\"singleInstance\":false,\"user\":\"${@uid}:${@gid}\"},\"allocation/${@tenant}/application/${@name}-ui\":{\"cpus\":0.1,\"env\":{\"CONNECT_URL\":\"${@name}:8083\",\"PORT\":\"8000\"},\"exposedPorts\":{\"8000\":{\"auth\":\"system-fwd-auth@view,manage\",\"tls\":\"auto\",\"vhost\":\"{ vhost('${@name}.${@tenant}', 'public') }\"}},\"image\":\"${@appcatalog}/release/kpn/database-ingester-app-ui:0.4.6\",\"imageConsole\":\"registry.cp.kpn-dsh.com/greenbox/database-ingester-app-ui:0.4.6\",\"instances\":1,\"mem\":16,\"name\":\"${@name}\",\"needsToken\":false,\"singleInstance\":false,\"user\":\"${@uid}:${@gid}\"},\"allocation/${@tenant}/topic/${@name}-config\":{\"kafkaProperties\":{\"cleanup.policy\":\"compact\"},\"name\":\"${@name}-config\",\"partitions\":1,\"replicationFactor\":3},\"allocation/${@tenant}/topic/${@name}-offset\":{\"kafkaProperties\":{\"cleanup.policy\":\"compact\"},\"name\":\"${@name}-offset\",\"partitions\":1,\"replicationFactor\":3},\"allocation/${@tenant}/topic/${@name}-status\":{\"kafkaProperties\":{\"cleanup.policy\":\"compact\"},\"name\":\"${@name}-status\",\"partitions\":1,\"replicationFactor\":3},\"allocation/${@tenant}/vhost/${@name}.${@tenant}@public\":\"${@name}.${@tenant}@public\"}}"
  },
  {
    "draft": false,
    "lastModified": 1751551797527.0,
    "payload": "{\"id\":\"kpn/whoami\",\"name\":\"Who Am I\",\"version\":\"0.0.7\",\"vendor\":\"KPN\",\"kind\":\"manifest\",\"apiVersion\":\"v0-alpha\",\"description\":\"Webserver that prints OS information and HTTP request to output\",\"moreInfo\":\"https://klarrio.com\",\"contact\":\"dsh-appcatalog@kpn.com\",\"configuration\":{\"$schema\":\"https://json-schema.org/draft/2019-09/schema\",\"type\":\"object\",\"properties\":{\"DNS_ZONE\":{\"description\":\"DNS zone name for the vhost\",\"type\":\"dns-zone\"},\"LOG_LEVEL\":{\"description\":\"Log level\",\"type\":\"string\",\"enum\":[\"error\",\"warn\",\"info\"],\"default\":\"info\"}}},\"resources\":{\"allocation/${@tenant}/application/${@name}\":{\"cpus\":0.1,\"env\":{\"LOG_LEVEL\":\"${LOG_LEVEL}\"},\"exposedPorts\":{\"8080\":{\"vhost\":\"{ vhost('${@name}.${@tenant}', '${DNS_ZONE}') }\"}},\"image\":\"${@appcatalog}/release/klarrio/whoami:v1.11.0-KLARRIO-1\",\"instances\":1,\"mem\":128,\"name\":\"${@name}\",\"needsToken\":false,\"singleInstance\":false,\"user\":\"${@uid}:${@gid}\"},\"allocation/${@tenant}/vhost/${@name}.${@tenant}@${DNS_ZONE}\":\"${@name}.${@tenant}@${DNS_ZONE}\"}}"
  },
  {
    "draft": false,
    "lastModified": 1742471420270.0,
    "payload": "{\"id\":\"kpn/keyring-service\",\"name\":\"keyring-service\",\"version\":\"0.6.3\",\"vendor\":\"KPN\",\"kind\":\"manifest\",\"apiVersion\":\"v0-alpha\",\"description\":\"KPN keyring service.\",\"moreInfo\":\"# keyring service\\n\\nThe **keyring service** is a service that exposes a REST-service that maps identifiers from a *domain* to a *codomain*. E.g. it can map BOSS identifiers to KRN identifiers.\\n\\n* The **keyring service** runs in your own tenant environment, under your own control.\\n\\n* It optionally provides an OpenAPI specification of the REST-service to import into Postman or a similar application.\\n\\n* It optionally provides a Swagger UI to explore and test the REST-service.\\n\\n* It can expose metrics for a Prometheus scraper.\\n\\nBe sure to check the [keyring documentation](https://keyring.dsh-prod.dsh.prod.aws.kpn.org/using/app-catalog.html) for more and important information.\\n\",\"contact\":\"greenbox@kpn.com\",\"configuration\":{\"$schema\":\"https://json-schema.org/draft/2019-09/schema\",\"type\":\"object\",\"properties\":{\"CODOMAINS\":{\"description\":\"enter the codomain(s) for the relation, separated by commas (allowed codomain names: BOSS, CPS, ECID, ETKN, KIDH, KRN, SBCM)\",\"type\":\"string\"},\"DOMAINS\":{\"description\":\"enter the domain(s) for the relation, separated by commas (allowed domain names: BOSS, ECID, ETKN, KIDH, KRN, SBCM)\",\"type\":\"string\"},\"ENABLE_METRICS_EXPORTER\":{\"description\":\"select whether to enable exporting metrics\",\"type\":\"string\",\"enum\":[\"false\",\"true\"],\"default\":\"true\"},\"ENABLE_OPEN_API_SPECIFICATION\":{\"description\":\"select whether to enable the openapi specification\",\"type\":\"string\",\"enum\":[\"false\",\"true\"],\"default\":\"true\"},\"ENABLE_SWAGGER_UI\":{\"description\":\"select whether to enable the swagger ui\",\"type\":\"string\",\"enum\":[\"false\",\"true\"],\"default\":\"true\"},\"FORBIDDEN_CODOMAINS\":{\"description\":\"enter codomain(s) that will not be used as intermediate codomains, separated by commas (allowed codomain names: BOSS, CPS, ECID, ETKN, KIDH, KRN, SBCM)\",\"type\":\"string\",\"default\":\"\"},\"FORBIDDEN_DOMAINS\":{\"description\":\"specify domain(s) that will not be used as intermediate domains, separated by commas (allowed domain names: BOSS, ECID, ETKN, KIDH, KRN, SBCM)\",\"type\":\"string\",\"default\":\"\"},\"FORBIDDEN_RELATIONS\":{\"description\":\"specify relation(s) that will not be used as intermediate relations, separated by commas (recognized relation names: bk, bc, bki, eet, ek, kik, kib, etk, etb, kiet, ete, etki, ke, kb, kki, ks, sk, sc)\",\"type\":\"string\",\"default\":\"\"}}},\"resources\":{\"allocation/${@tenant}/application/${@name}\":{\"cpus\":0.5,\"env\":{\"CACHE_METRICS_EXPORTER_ENABLED\":\"${ENABLE_METRICS_EXPORTER}\",\"CODOMAINS\":\"${CODOMAINS}\",\"DOMAINS\":\"${DOMAINS}\",\"HEAP_MEMORY\":\"768\",\"KAFKA_GROUP_ID_PREFIX\":\"${@tenant}_\",\"KEYRING_CONFIGURATION_RESOURCE\":\"application.conf\",\"KEYRING_INSTALLED_BASE_RESOURCE\":\"installed-base-20250319.conf\",\"MASTER_RELATION_METRICS_EXPORTER_ENABLED\":\"${ENABLE_METRICS_EXPORTER}\",\"RELATIONS_ENGINE_FORBIDDEN_CODOMAINS\":\"${FORBIDDEN_CODOMAINS}\",\"RELATIONS_ENGINE_FORBIDDEN_DOMAINS\":\"${FORBIDDEN_DOMAINS}\",\"RELATIONS_ENGINE_FORBIDDEN_MASTER_RELATIONS\":\"${FORBIDDEN_RELATIONS}\",\"RELATIONS_ENGINE_METRICS_EXPORTER_ENABLED\":\"${ENABLE_METRICS_EXPORTER}\",\"SERVICE_JVM_METRICS_EXPORTER_ENABLED\":\"${ENABLE_METRICS_EXPORTER}\",\"SERVICE_METRICS_EXPORTER_ENABLED\":\"${ENABLE_METRICS_EXPORTER}\",\"SERVICE_OPEN_API_SPECIFICATION_ENABLED\":\"${ENABLE_OPEN_API_SPECIFICATION}\",\"SERVICE_SWAGGER_UI_CONTACT_NAME\":\"Unibox Team\",\"SERVICE_SWAGGER_UI_CONTACT_URL\":\"https://confluence.kpn.org/display/GBDocs/Greenbox\",\"SERVICE_SWAGGER_UI_DOCUMENTATION_URL\":\"https://keyring.dsh-dev.dsh.np.aws.kpn.org\",\"SERVICE_SWAGGER_UI_ENABLED\":\"${ENABLE_SWAGGER_UI}\"},\"exposedPorts\":{\"8088\":{\"auth\":\"system-fwd-auth@view,manage\",\"tls\":\"auto\",\"vhost\":\"{ vhost('${@name}.${@tenant}', 'public') }\"}},\"image\":\"${@appstore}/release/kpn/keyring-service:0.6.3\",\"imageConsole\":\"registry.cp.kpn-dsh.com/greenbox/keyring-service:0.6.3\",\"instances\":1,\"mem\":3000,\"metrics\":{\"path\":\"/\",\"port\":9585},\"name\":\"${@name}\",\"needsToken\":true,\"singleInstance\":false,\"user\":\"${@uid}:${@gid}\"},\"allocation/${@tenant}/vhost/${@name}.${@tenant}@public\":\"${@name}.${@tenant}@public\"}}"
  },
  {
    "draft": false,
    "lastModified": 1750695184757.0,
    "payload": "{\"id\":\"kpn/prometheus-scraper\",\"name\":\"Prometheus Scraper\",\"version\":\"0.1.7\",\"vendor\":\"KPN\",\"kind\":\"manifest\",\"apiVersion\":\"v0-alpha\",\"description\":\"Application to scrape Prometheus metrics to Kafka in DSH\",\"moreInfo\":\"## Prometheus Scraper to Kafka \\n\\n  \\n\\n **1. Prometheus URL**: Prometheus endpoint to scrape metrics from. If you do not change the default value `tenant-prometheus-server`, it will be automatically assigned to your tenant's Prometheus server. \\n\\n **2. Kafka Topic**: Kafka topic name for producing metrics to. Can be configured using `KAFKA_TOPIC`. \\n\\n **3. Scraper Interval Seconds**: Interval in seconds at which metrics are scraped. The default interval is 15 secs. Too frequent intervals might cause issues on Prometheus server. So, less than 15 seconds is not allowed and will be reaasigned to 15 secs as minimal interval seconds. Set the interval using `SCRAPE_INTERVAL_SECS`. \\n\\n **4. Prometheus Queries**: Semicolon-separated metric names or Prometheus queries. Double quotes in the queries should be escaped with triple backslash. Default value: 'up;node_cpu_seconds_total'. Can be configured using `PROMETHEUS_QUERIES` variable. \\n\\n **5. Max Retries**: Maxiumum number of retries in the case of failure of scraping metrics. Set to 0 if you do not want to retry. \\n\\n **6. Scaling**: Use `CPU` \\u0026 `MEMORY` values for vertical scaling. \\n\\n ## More Information \\n The Unibox Team can be reached at unibox@kpn.com for more information.\",\"contact\":\"unibox@kpn.com\",\"configuration\":{\"$schema\":\"https://json-schema.org/draft/2019-09/schema\",\"type\":\"object\",\"properties\":{\"CPU\":{\"description\":\"Amount of CPU cores per instance.\",\"type\":\"string\",\"default\":\"0.1\"},\"KAFKA_TOPIC\":{\"description\":\"Kafka Topic name for producing metrics to.\",\"type\":\"string\",\"default\":\"scratch.scraper-metrics.connectors\"},\"LOG_LEVEL\":{\"description\":\"The log level for the application. The default is INFO.\",\"type\":\"string\",\"enum\":[\"TRACE\",\"DEBUG\",\"INFO\",\"WARN\",\"ERROR\"],\"default\":\"INFO\"},\"MAX_RETRIES\":{\"description\":\"Maximum number of retries in the case of failure of scraping metrics. Set to 0 if you do not want to retry.\",\"type\":\"string\",\"enum\":[\"0\",\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\"],\"default\":\"1\"},\"MEMORY\":{\"description\":\"Amount of memory per instance.\",\"type\":\"string\",\"default\":\"16\"},\"PROMETHEUS_ENDPOINT\":{\"description\":\"Prometheus endpoint URL. If you do not change the default value, it will be automatically assigned to your tenant Prometheus server.\",\"type\":\"string\",\"default\":\"tenant-prometheus-server\"},\"PROMETHEUS_QUERIES\":{\"description\":\"Semicolon-separated Prometheus queries (or metric names). Double quotes in the queries should be escaped with triple backslash.\",\"type\":\"string\",\"default\":\"up;node_cpu_seconds_total\"},\"SCRAPE_INTERVAL_SECS\":{\"description\":\"Interval in seconds at which metrics are scraped. The default interval is 15 secs. Too frequent intervals might cause issues on Prometheus server. So, less than 15 seconds is not allowed and will be reaasigned to 15 secs (min interval).\",\"type\":\"string\",\"default\":\"15\"}}},\"resources\":{\"allocation/${@tenant}/application/${@name}\":{\"cpus\":\"${CPU | number}\",\"env\":{\"KAFKA_TOPIC\":\"${KAFKA_TOPIC}\",\"MAX_RETRIES\":\"${MAX_RETRIES}\",\"PROMETHEUS_ENDPOINT\":\"${PROMETHEUS_ENDPOINT}\",\"PROMETHEUS_QUERIES\":\" ${PROMETHEUS_QUERIES}\",\"RUST_LOG\":\"${LOG_LEVEL}\",\"SCRAPE_INTERVAL_SECS\":\"${SCRAPE_INTERVAL_SECS}\"},\"image\":\"${@appcatalog}/release/kpn/prometheus-scraper:0.1.7\",\"instances\":1,\"mem\":\"${MEMORY | number}\",\"metrics\":{\"path\":\"/metrics\",\"port\":8080},\"name\":\"${@name}\",\"needsToken\":true,\"singleInstance\":false,\"user\":\"${@uid}:${@gid}\"}}}"
  },
  {
    "draft": false,
    "lastModified": 1696866783198.0,
    "payload": "{\"id\":\"kpn/schema-store-ui\",\"name\":\"Schema Store UI (beta)\",\"version\":\"0.0.11-beta\",\"vendor\":\"KPN\",\"kind\":\"manifest\",\"apiVersion\":\"v0-alpha\",\"description\":\"Swagger UI documentation of the Schema Store API\",\"moreInfo\":\"\",\"contact\":\"dsh-appcatalog@kpn.com\",\"configuration\":{\"$schema\":\"https://json-schema.org/draft/2019-09/schema\",\"type\":\"object\",\"properties\":{\"DNS_ZONE\":{\"description\":\"DNS zone for the vhost\",\"type\":\"string\",\"enum\":[\"public\",\"private\"],\"default\":\"public\"}}},\"resources\":{\"allocation/${@tenant}/application/${@name}\":{\"cpus\":0.1,\"env\":{\"SR_DOMAIN\":\"api.schema-store.dsh.marathon.mesos\",\"SR_PATH\":\"/\",\"SR_PORT\":\"8443\"},\"exposedPorts\":{\"8080\":{\"auth\":\"system-fwd-auth@view,manage\",\"vhost\":\"{ vhost('${@name}.${@tenant}','${DNS_ZONE}') }\"}},\"image\":\"${@appcatalog}/release/kpn/schema-store-ui:0.0.11\",\"instances\":1,\"mem\":256,\"name\":\"${@name}\",\"needsToken\":true,\"singleInstance\":false,\"user\":\"${@uid}:${@gid}\"},\"allocation/${@tenant}/vhost/${@name}.${@tenant}@public\":\"${@name}.${@tenant}@${DNS_ZONE}\"}}"
  },
  {
    "draft": false,
    "lastModified": 1697190450742.0,
    "payload": "{\"id\":\"kpn/kafka2kafka\",\"name\":\"kafka2kafka\",\"version\":\"1.0.0\",\"vendor\":\"KPN\",\"kind\":\"manifest\",\"apiVersion\":\"v0-alpha\",\"description\":\"KPN Kafka to Kafka service.\",\"moreInfo\":\"# kafka2kafka service\\n\\nThe **kafka2kafka** is a service allows for a quick setup of a Kakfa read stream to kafka write stream application. E.g. it can consume from internal DSH streams, but also from external streams.\\n If you want to connect to an external stream set the flag AUTHENTICATE_EXTERNAL_SSL_MODE to true and add the corresponding certificates as secrets in your service config.\",\"contact\":\"dsh-lfm@kpn.com\",\"configuration\":{\"$schema\":\"https://json-schema.org/draft/2019-09/schema\",\"type\":\"object\",\"properties\":{\"CONSUMER_BOOTSTRAP_SERVERS\":{\"description\":\"Bootstrap servers of the stream where messages are read from\",\"type\":\"string\"},\"DNS_ZONE\":{\"description\":\"DNS zone name for the vhost\",\"type\":\"string\",\"enum\":[\"private – dsh-prod.dsh.prod.aws.kpn.org\",\"public – dsh-prod.dsh.prod.aws.kpn\"]},\"ERROR_ROUTING\":{\"description\":\"Name of the read stream and what stream to route messages to create errors to\",\"type\":\"string\"},\"GROUP_ID\":{\"description\":\"Group ID of the consumer group\",\"type\":\"string\"},\"MESSAGE_ROUTING\":{\"description\":\"Name of the read stream and write stream. In the following format: read_stream:write_stream\",\"type\":\"string\"},\"PRODUCER_BOOTSTRAP_SERVERS\":{\"description\":\"Bootstrap servers of the stream where messages are produced on\",\"type\":\"string\"},\"READ_TOPICS\":{\"description\":\"The topics to read messages from\",\"type\":\"string\"}}},\"resources\":{\"allocation/${@tenant}/application/${@name}\":{\"cpus\":0.2,\"env\":{\"APP_NAME\":\"${@name}\",\"BUFFERED_MSG_SIZE\":\"1000\",\"CONSUMER_BOOTSTRAP_SERVERS\":\"${CONSUMER_BOOTSTRAP_SERVERS}\",\"DSH_LOG_LEVEL\":\"DEBUG\",\"ERROR_ROUTING\":\"${ERROR_ROUTING}\",\"GROUP_ID\":\"${GROUP_ID}\",\"MESSAGE_ROUTING\":\"${MESSAGE_ROUTING}\",\"PARSING_PARALLELISM\":\"4\",\"PIPELINE_LOG_LEVEL\":\"DEBUG\",\"PRODUCER_BOOTSTRAP_SERVERS\":\"${PRODUCER_BOOTSTRAP_SERVERS}\",\"READ_TOPICS\":\"${READ_TOPICS}\"},\"image\":\"${@appstore}/release/kpn/kafka2kafka:1\",\"instances\":1,\"mem\":512,\"metrics\":{\"path\":\"/metrics\",\"port\":9090},\"name\":\"${@name}\",\"needsToken\":true,\"singleInstance\":false,\"user\":\"${@uid}:${@gid}\"},\"allocation/${@tenant}/vhost/${@name}.${@tenant}@${DNS_ZONE}\":\"${@name}.${@tenant}@${DNS_ZONE}\"}}"
  },
  {
    "draft": false,
    "lastModified": 1701271078961.0,
    "payload": "{\"id\":\"kpn/sql-database\",\"name\":\"SQL Database\",\"version\":\"1.1.3\",\"vendor\":\"KPN\",\"kind\":\"manifest\",\"apiVersion\":\"v0-alpha\",\"description\":\"Managed YugabyteDB 2.11, a PostgreSQL compatible relational database\",\"moreInfo\":\"## SQL Database as a Service (beta release)\\n### Introduction\\nFor some use cases, a relational database is indispensable. With SQL Database as a Service, DSH offers a scalable, secure, managed, relational database.\\n\\nIt's a managed YugabyteDB that provides the following features:\\n*   YugabyteDB version `2.11`\\n*   PostgreSQL compatibility\\n    \\n*   Optional extensions: PostGIS, Foreign Data Wrapper\\n    \\n*   Automated backups\\n    \\n*   Secure data (TLS connections, volume encryption)\\n### Configuration\\nWhen you configure a new managed database, there are some settings you must configure.\\n**WARNING:** You can't change these options after the initial deployment of the database. Consider your choices carefully.\\n#### `INSTANCES`\\nThe number of YugabyteDB tablet servers. Each table is split into multiple tablets and replicated 3 times. These tablets are spread out over all tablet servers.\\n\\n_Due to the replication factor of 3, the database requires at least 3 instances_.\\n#### `CPU`\\nThe number of CPUs per YugabyteDB tablet server. Behaves the same as the CPU value for other services. Keep in mind that this setting is per instance, for example, 3 instances x 2 CPU = 6 CPU in total.\\n\\n_The minimum is 0.3 CPU for a lightly used database. For optimal performance, between 1.0 - 6.0 CPUs per instance are recommended._\\n#### `MEMORY`\\nThe amount of memory in MB per YugabyteDB tablet server. Behaves the same as the memory value for other services. Keep in mind that this setting is per instance, for example, 3 instances x 4096 MB = 12 GB in total. Don’t set this too low if you want good performance.\\n\\n_We recommend 3072 MB (3 GB) or more. The minimum is 2048 MB._\\n#### `VOLUME_SIZE`\\nThe amount of disk space in GB per YugabyteDB tablet server. This setting is per instance, but keep in mind all data is replicated 3 times. For example, if you want a database that can grow to 20 GB, and you have chosen 4 instances, then you need to set the volume size to 20 GB x 3 replicas / 4 instances = 15 GB.\\n\\n_We recommend you start with big enough volumes since you can’t resize your database (the minimum is 5 GB)._\\n#### `EXTENSIONS`\\nYugabyteDB allows the addition of PostgreSQL extensions. Currently, three extensions are part of the DSH DBaaS offering. PostgreSQL extensions must be enabled when the database is deployed.\\n\\n- `postgis` PostGIS is a spatial database extension for PostgreSQL databases. It adds SQL support for geographic objects and location queries. This extension requires at least 2048 MB of memory per tablet server. _The minimum memory requirement with this extension enabled is 3072 MB._\\n\\n- `postgres_fdw` A foreign data wrapper (FDW) is a library that can communicate with an external data source, hiding the details of connecting to the data source and obtaining data from it. Multiple tenants can configure this to read tables from each others databases.\\n\\n- `uuid-ossp` The uuid-ossp module provides functions to generate universally unique identifiers (UUIDs) using one of several standard algorithms.\\n#### `SNAPSHOT_INTERVAL`\\nThe interval in seconds with which full snapshots of the database are taken.\\n\\n_Typical intervals are 1 hour (3600, the shortest allowed interval) or 1 day (86400). Set the interval to 0 to turn this feature off._\\n\\n### SQL connection in app definitions\\nUse a PostgreSQL compatible driver in your service code, for example, `JDBC`, `Npgsql`, `psycopg2`, ...\\n\\nThe name you give to a new DBaaS allocation is not the same as the PostgreSQL role, database or tablespace that gets created in the process. These get a unique generated name. There’s a set of app definition macros to get those identifiers.\\n\\nIn the service definition of your application, the _hostname_, _username_, and _database_ name can be injected with the following macros: \\n\\n```json\\n\\\"env\\\": {\\n  \\\"HOSTNAME\\\": \\\"{ database_host('example1') }\\\",\\n  \\\"USERNAME\\\": \\\"{ database_user('example1') }\\\",\\n  \\\"DATABASE\\\": \\\"{ database_id('example1') }\\\"\\n}\\n```\\nThe `DATABASE` and `USERNAME` variables are generated. They are not equal to the name of the DBaaS allocation you chose at deployment. The password can be injected with this secret:  \\n\\n```json\\n\\\"secrets\\\": [{\\n  \\\"name\\\": \\\"system/dbaas/example1_password\\\",\\n  \\\"injections\\\": [{\\n    \\\"env\\\": \\\"PASSWORD\\\"\\n  }]\\n}]\\n```\\nFor convenience there is a *databases* dropdown in the _INSERT_ toolbar of the service definition editor.\\n\\nAll four variables are needed to set up a connection with YugabyteDB. For example, when using a JDBC driver, construct the connection string: `postgresql://$USERNAME:$PASSWORD@$HOSTNAME:5432/$DATABASE`\\n\\nThe standard PostgreSQL port `5432` is used.\\n### More information\\nConsult the _DSH User Documentation_ by clicking the _docs_ button in the top left corner and searching for the term _database_.\\n\",\"contact\":\"dsh-appcatalog@kpn.com\",\"configuration\":{\"$schema\":\"https://json-schema.org/draft/2019-09/schema\",\"type\":\"object\",\"properties\":{\"CPU\":{\"description\":\"The number of CPU cores per YugabyteDB tablet server (minimum 0.3, we recommend 1.0 or more)\",\"type\":\"number\",\"default\":\"0.3\"},\"EXTENSIONS\":{\"description\":\"Comma separated list of extensions (postgis, postgres_fdw, uuid-ossp)\",\"type\":\"string\",\"default\":\"\"},\"INSTANCES\":{\"description\":\"The number of YugabyteDB tablet servers (minimum 3)\",\"type\":\"number\",\"default\":\"3\"},\"MEMORY\":{\"description\":\"The amount of memory in MB per YugabyteDB tablet server (minimum 2048, we recommend 3072 or more)\",\"type\":\"number\",\"default\":\"2048\"},\"SNAPSHOT_INTERVAL\":{\"description\":\"The interval in seconds with which full snapshots of the database are taken (minimum 3600, set to 0 to turn off)\",\"type\":\"number\",\"default\":\"0\"},\"VOLUME_SIZE\":{\"description\":\"The amount of disk space in GB per YugabyteDB tablet server (minimum 5 GB)\",\"type\":\"number\",\"default\":\"5\"}}},\"resources\":{\"allocation/${@tenant}/database/${@name}\":{\"cpus\":\"${CPU | number}\",\"extensions\":[\"${EXTENSIONS}\"],\"instances\":\"${INSTANCES | number}\",\"mem\":\"${MEMORY | number}\",\"name\":\"${@name}\",\"snapshotInterval\":\"${SNAPSHOT_INTERVAL | number}\",\"version\":\"2.11.2.0-1\",\"volumeSize\":\"${VOLUME_SIZE | number}\"}}}"
  },
  {
    "draft": false,
    "lastModified": 1733993055220.0,
    "payload": "{\"id\":\"kpn/dsh-ollama\",\"name\":\"Ollama\",\"version\":\"0.5.0-gemma\",\"vendor\":\"KPN\",\"kind\":\"manifest\",\"apiVersion\":\"v0-alpha\",\"description\":\"LLM package manager and runtime with a web UI and a DSH chat bot\",\"moreInfo\":\"## Ollama: Run \\u0026 Manage Your LLMs in DSH\\n\\n[Ollama](https://ollama.com/), developed by Meta, is a package manager and runtime for your LLMs with a standardized REST interface to simplify your LLM interaction.\\n\\n### Key Features\\n\\n- **Web Chat Interface:** Interact with your models directly through a user-friendly web interface.\\n- **DSH Chatbot Integration:** Includes a built-in DSH chatbot powered by the Retrieval-Augmented Generation (RAG) system.\\n- **Multiple LLM Integration:** Supports various LLMs, including Mistral (Nvidia), Phi (Microsoft), Gemma (Google).\\n\\n### API Usage:\\n\\nThe [API Documentation](https://github.com/ollama/ollama/blob/main/docs/api.md) provides the list available end points.\\n\\n```bash\\n\\ncurl http://ollama-server.\\u003ctenant\\u003e.marathon.mesos:11434/api/generate -d '{\\n  \\\"model\\\": \\\"mistral:7b\\\",\\n  \\\"prompt\\\": \\\"Why is the sky blue?\\\",\\n  \\\"options\\\": {\\n    \\\"seed\\\": 42,\\n    \\\"temperature\\\": 0.9\\n  }\\n}'\\n\\n```\\n\\n### More Information\\n\\nThe _Unibox Team_ can be reached at **unibox@kpn.com** for more information.\",\"contact\":\"unibox@kpn.com\",\"configuration\":{\"$schema\":\"https://json-schema.org/draft/2019-09/schema\",\"type\":\"object\",\"properties\":{\"CPU\":{\"description\":\"Amount of CPU cores per instance.\",\"type\":\"string\",\"default\":\"3\"},\"INSTANCES\":{\"description\":\"Number of instances to deploy for horizontal scaling. The default is 1.\",\"type\":\"string\",\"enum\":[\"1\",\"2\",\"3\",\"4\",\"5\",\"6\"],\"default\":\"1\"},\"MEMORY\":{\"description\":\"Amount of memory per instance.\",\"type\":\"string\",\"default\":\"10240\"},\"VHOST_DNS_ZONE\":{\"description\":\"DNS Zone selection for Vhost (kpn.com or kpn.org)\",\"type\":\"string\",\"enum\":[\"public\",\"private\"],\"default\":\"private\"}}},\"resources\":{\"allocation/${@tenant}/application/${@name}-server\":{\"cpus\":\"${CPU | number}\",\"env\":{},\"image\":\"${@appcatalog}/release/kpn/ollama-server:gemma\",\"imageConsole\":\"registry.cp.kpn-dsh.com/connectors/ollama-server:gemma\",\"instances\":\"${INSTANCES | number}\",\"mem\":\"${MEMORY | number}\",\"name\":\"${@name}-server\",\"needsToken\":false,\"singleInstance\":true,\"user\":\"${@uid}:${@gid}\"},\"allocation/${@tenant}/application/${@name}-ui\":{\"cpus\":0.1,\"env\":{\"OLLAMA_BASE_URL\":\"http://${@name}-server:11434\"},\"exposedPorts\":{\"8080\":{\"auth\":\"system-fwd-auth@view,manage\",\"tls\":\"auto\",\"vhost\":\"{ vhost('${@name}.${@tenant}', '${VHOST_DNS_ZONE}') }\"}},\"image\":\"${@appcatalog}/release/kpn/ollama-ui:0.5.0-gemma\",\"imageConsole\":\"registry.cp.kpn-dsh.com/connectors/ollama-ui:0.5.0-gemma\",\"instances\":1,\"mem\":3000,\"name\":\"${@name}-ui\",\"needsToken\":false,\"singleInstance\":true,\"user\":\"${@uid}:${@gid}\",\"volumes\":{\"/app/backend/data\":{\"name\":\"{ volume('${@name}-ollama')}\"}}},\"allocation/${@tenant}/vhost/${@name}.${@tenant}@${VHOST_DNS_ZONE}\":\"${@name}.${@tenant}@${VHOST_DNS_ZONE}\",\"allocation/${@tenant}/volume/${@name}-ollama\":{\"name\":\"${@name}-ollama\",\"size\":5}}}"
  },
  {
    "draft": false,
    "lastModified": 1697188721568.0,
    "payload": "{\"id\":\"kpn/sql-database\",\"name\":\"SQL Database\",\"version\":\"1.1.2\",\"vendor\":\"KPN\",\"kind\":\"manifest\",\"apiVersion\":\"v0-alpha\",\"description\":\"Managed YugabyteDB 2.11, a PostgreSQL compatible relational database\",\"moreInfo\":\"## SQL Database as a Service (beta release)\\n### Introduction\\nFor some use cases, a relational database is indispensable. With SQL Database as a Service, DSH offers a scalable, secure managed relational database.\\n\\nIt's a managed YugabyteDB which provides the following features:\\n*   YugabyteDB version `2.11`\\n*   PostgreSQL compatibility\\n    \\n*   Optional extensions: PostGIS, Foreign Data Wrapper\\n    \\n*   Automated backups\\n    \\n*   Secure data (TLS connections, volume encryption)\\n### Configuration\\nWhen you configure a new managed database, there are some settings you must configure.\\n**WARNING:** You can't change these options after the initial deployment of the database. Consider your choices carefully.\\n#### `INSTANCES`\\nThe number of YugabyteDB tablet servers. Each table is split into multiple tablets and replicated 3 times. These tablets are spread out over all tablet servers.\\n\\n_Due to the replication factor of 3 the database requires at least 3 instances_.\\n#### `CPU`\\nThe number of CPU per YugabyteDB tablet server. Behaves the same as the CPU value for other services. Keep in mind that this setting is per instance, e.g. 3 instances x 2 CPU = 6 CPU in total.\\n\\n_The minimum is 0.3 CPU for a lightly used database. For optimal performance between 1.0 - 6.0 CPUs per instance are recommended._\\n#### `MEMORY`\\nThe amount of memory in MB per YugabyteDB tablet server. Behaves the same as the memory value for other services. Keep in mind that this setting is per instance, e.g. 3 instances x 4096 MB = 12 GB in total. Don’t set this too low if you want good performance.\\n\\n_We recommend 3072 MB (3 GB) or more. The minimum is 2048 MB._\\n#### `VOLUME_SIZE`\\nThe amount of disk space in GB per YugabyteDB tablet server. This setting is per instance, but keep in mind all data is replicated 3 times. For example, say you want a database that can grow to 20 GB and you have chosen 4 instances, then you need to set the volume size to 20 GB x 3 replica / 4 instances = 15 GB.\\n\\n_We recommend you start with big enough volumes since you can’t resize your database (the minimum is 5 GB)._\\n#### `EXTENSIONS`\\nYugabyteDB allows to add PostgreSQL extensions. Currently three extensions are part of the DSH DBaaS offering. PostgreSQL extensions must be enabled when the database is deployed.\\n\\n- `postgis` PostGIS is a spatial database extension for PostgreSQL databases. It adds SQL support for geographic objects and location queries. This extension requires at least 2048 MB of memory per tablet server. _The minimum memory requirement with this extension enabled is 3072 MB._\\n\\n- `postgres_fdw` A foreign data wrapper (FDW) is a library that can communicate with an external data source, hiding the details of connecting to the data source and obtaining data from it. Multiple tenants can configure this to read tables from each others databases.\\n\\n- `uuid-ossp` The uuid-ossp module provides functions to generate universally unique identifiers (UUIDs) using one of several standard algorithms.\\n#### `SNAPSHOT_INTERVAL`\\nThe interval in seconds with which full snapshots of the database are taken.\\n\\n_Typical intervals are 1 hour (3600, the shortest allowed interval) or 1 day (86400). Set the interval to 0 to turn this feature off._\\n\\n### SQL connection in app definitions\\nUse a PostgreSQL compatible driver in your service code, for example `JDBC`, `Npgsql`, `psycopg2`, ...\\n\\nThe name you give to a new DBaaS allocation is not the same as the PostgreSQL role, database or tablespace that gets created in the process. These get a unique generated name. There’s a set of app definition macros to get those identifiers.\\n\\nIn the service definition of your application the _hostname_, _username_ and _database_ name can be injected with the following macros: \\n\\n```json\\n\\\"env\\\": {\\n  \\\"HOSTNAME\\\": \\\"{ database_host('example1') }\\\",\\n  \\\"USERNAME\\\": \\\"{ database_user('example1') }\\\",\\n  \\\"DATABASE\\\": \\\"{ database_id('example1') }\\\"\\n}\\n```\\nThe `DATABASE` and `USERNAME` variables are generated. They are not equal to the name of the DBaaS allocation you chose at deployment. The password can be injected with this secret:  \\n\\n```json\\n\\\"secrets\\\": [{\\n  \\\"name\\\": \\\"system/dbaas/example1_password\\\",\\n  \\\"injections\\\": [{\\n    \\\"env\\\": \\\"PASSWORD\\\"\\n  }]\\n}]\\n```\\nFor convenience there’s a *databases* dropdown in the _INSERT_ toolbar of the service definition editor.\\n\\nAll four variables are needed to set up a connection with YugabyteDB. For example, when using a JDBC driver, construct the connection string: `postgresql://$USERNAME:$PASSWORD@$HOSTNAME:5432/$DATABASE`\\n\\nThe standard PostgreSQL port `5432` is used.\\n### More information\\nConsult the _DSH User Documentation_ by clicking the _docs_ button in the top left corner and searching for the term _database_.\\n\",\"contact\":\"dsh-appcatalog@kpn.com\",\"configuration\":{\"$schema\":\"https://json-schema.org/draft/2019-09/schema\",\"type\":\"object\",\"properties\":{\"CPU\":{\"description\":\"The number of CPU cores per YugabyteDB tablet server (minimum 0.3, we recommend 1.0 or more)\",\"type\":\"number\",\"default\":\"0.3\"},\"EXTENSIONS\":{\"description\":\"Comma separated list of extensions (postgis, postgres_fdw, uuid-ossp)\",\"type\":\"string\",\"default\":\"\"},\"INSTANCES\":{\"description\":\"The number of YugabyteDB tablet servers (minimum 3)\",\"type\":\"number\",\"default\":\"3\"},\"MEMORY\":{\"description\":\"The amount of memory in MB per YugabyteDB tablet server (minimum 2048, we recommend 3072 or more)\",\"type\":\"number\",\"default\":\"2048\"},\"SNAPSHOT_INTERVAL\":{\"description\":\"The interval in seconds with which full snapshots of the database are taken (minimum 3600, set to 0 to turn off)\",\"type\":\"number\",\"default\":\"0\"},\"VOLUME_SIZE\":{\"description\":\"The amount of disk space in GB per YugabyteDB tablet server (minimum 5 GB)\",\"type\":\"number\",\"default\":\"5\"}}},\"resources\":{\"allocation/${@tenant}/database/${@name}\":{\"cpus\":\"${CPU | number}\",\"extensions\":[\"${EXTENSIONS}\"],\"instances\":\"${INSTANCES | number}\",\"mem\":\"${MEMORY | number}\",\"name\":\"${@name}\",\"snapshotInterval\":\"${SNAPSHOT_INTERVAL | number}\",\"version\":\"2.11.2.0-1\",\"volumeSize\":\"${VOLUME_SIZE | number}\"}}}"
  },
  {
    "draft": false,
    "lastModified": 1724058429209.0,
    "payload": "{\"id\":\"kpn/eavesdropper\",\"name\":\"eavesdropper\",\"version\":\"0.9.2\",\"vendor\":\"KPN\",\"kind\":\"manifest\",\"apiVersion\":\"v0-alpha\",\"description\":\"Web application to visualize messages on Kafka topics in realtime\",\"moreInfo\":\"## Realtime record visualization\\n\\nThis app enables you to view records on your DSH Kafka topics in realtime. It can show the record's keys, values and headers in json, text or binary format, and also recognizes some custom formats for some special topics. Furthermore, it allows you to:\\n\\n* unwrap a record from the envelope that the DSH platform enforces on stream topics, showing the envelope metadata and the envelope payload separately,\\n* filter records based on regular expressions and/or throttling,\\n* show records individually or in list view,\\n* download/copy record values in json, text or binary format.\\n\\nWhen started from the App Catalog the Eavesdropper will be available with the same SSO authorization as the DSH console, and will expose all topics that your tenant is entitled to see.\",\"contact\":\"wilbert.schelvis@kpn.com\",\"configuration\":{\"$schema\":\"https://json-schema.org/draft/2019-09/schema\",\"type\":\"object\",\"properties\":{\"ALLOW_CUSTOM_GROUP_ID\":{\"description\":\"allow custom group id\",\"type\":\"string\",\"enum\":[\"true\",\"false\"],\"default\":\"true\"},\"ENVELOPE_DESERIALIZERS\":{\"description\":\"enable envelope deserializers\",\"type\":\"string\",\"enum\":[\"\",\"dsh-envelope\",\"gzip\",\"dsh-envelope,gzip\"],\"default\":\"\"},\"LOG_LEVEL\":{\"description\":\"application log level\",\"type\":\"string\",\"enum\":[\"error\",\"warn\",\"info\"],\"default\":\"info\"},\"REPRESENTATION_DESERIALIZERS\":{\"description\":\"enable representation deserializers\",\"type\":\"string\",\"enum\":[\"\",\"codomain-values-record\",\"greenbox-ri-avro,greenbox-ri-protobuf\",\"codomain-values-record,greenbox-ri-avro,greenbox-ri-protobuf\"],\"default\":\"\"}}},\"resources\":{\"allocation/${@tenant}/application/${@name}\":{\"cpus\":0.1,\"env\":{\"ALLOW_CUSTOM_GROUP_ID\":\"${ALLOW_CUSTOM_GROUP_ID}\",\"CONSUMER_BUILDER\":\"dsh-datastreams-properties\",\"DEFAULT_GROUP_IDS\":\"*\",\"ENVELOPE_DESERIALIZERS\":\"${ENVELOPE_DESERIALIZERS}\",\"EXCLUDED_TOPICS\":\"\",\"EXCLUDED_TOPICS_REGEX\":\"\",\"GROUP_ID_PREFIX\":\"${@tenant}_\",\"INCLUDED_TOPICS\":\"\",\"INCLUDED_TOPICS_REGEX\":\"\",\"INCLUDE___CONSUMER_OFFSETS_TOPIC\":\"false\",\"INSTANCE_IDENTIFIER\":\"${@tenant}_${@name}\",\"LOG_LEVEL\":\"${LOG_LEVEL}\",\"LOG_LEVEL_ENTRYPOINT\":\"${LOG_LEVEL}\",\"LOG_LEVEL_GREENBOX\":\"error\",\"LOG_LEVEL_KAFKA_CLIENT\":\"error\",\"LOG_LEVEL_MONITOR\":\"${LOG_LEVEL}\",\"LOG_LEVEL_RECDES\":\"info\",\"LOG_LEVEL_SERVICE\":\"${LOG_LEVEL}\",\"REPRESENTATION_DESERIALIZERS\":\"${REPRESENTATION_DESERIALIZERS}\"},\"exposedPorts\":{\"8081\":{\"auth\":\"system-fwd-auth@view,manage\",\"tls\":\"auto\",\"vhost\":\"{ vhost('${@name}.${@tenant}', 'public') }\"}},\"image\":\"${@appcatalog}/release/kpn/eavesdropper:0.9.2\",\"instances\":1,\"mem\":384,\"name\":\"${@name}\",\"needsToken\":true,\"singleInstance\":false,\"user\":\"${@uid}:${@gid}\"},\"allocation/${@tenant}/vhost/${@name}.${@tenant}@public\":\"${@name}.${@tenant}@public\"}}"
  },
  {
    "draft": false,
    "lastModified": 1733392922491.0,
    "payload": "{\"id\":\"kpn/dsh-ollama\",\"name\":\"Ollama\",\"version\":\"0.5.0-all\",\"vendor\":\"KPN\",\"kind\":\"manifest\",\"apiVersion\":\"v0-alpha\",\"description\":\"LLM package manager and runtime with a web UI and a DSH chat bot\",\"moreInfo\":\"## Ollama: Run \\u0026 Manage Your LLMs in DSH\\n\\n[Ollama](https://ollama.com/), developed by Meta, is a package manager and runtime for your LLMs with a standardized REST interface to simplify your LLM interaction.\\n\\n### Key Features\\n\\n- **Web Chat Interface:** Interact with your models directly through a user-friendly web interface.\\n- **DSH Chatbot Integration:** Includes a built-in DSH chatbot powered by the Retrieval-Augmented Generation (RAG) system.\\n- **Multiple LLM Integration:** Supports various LLMs, including Mistral (Nvidia), Phi (Microsoft), Gemma (Google).\\n\\n### API Usage:\\n\\nThe [API Documentation](https://github.com/ollama/ollama/blob/main/docs/api.md) provides the list available end points.\\n\\n```bash\\n\\ncurl http://ollama-server.\\u003ctenant\\u003e.marathon.mesos:11434/api/generate -d '{\\n  \\\"model\\\": \\\"mistral:7b\\\",\\n  \\\"prompt\\\": \\\"Why is the sky blue?\\\",\\n  \\\"options\\\": {\\n    \\\"seed\\\": 42,\\n    \\\"temperature\\\": 0.9\\n  }\\n}'\\n\\n```\\n\\n### More Information\\n\\nThe _Unibox Team_ can be reached at **unibox@kpn.com** for more information.\",\"contact\":\"unibox@kpn.com\",\"configuration\":{\"$schema\":\"https://json-schema.org/draft/2019-09/schema\",\"type\":\"object\",\"properties\":{\"CPU\":{\"description\":\"Amount of CPU cores per instance.\",\"type\":\"string\",\"default\":\"3\"},\"INSTANCES\":{\"description\":\"Number of instances to deploy for horizontal scaling. The default is 1.\",\"type\":\"string\",\"enum\":[\"1\",\"2\",\"3\",\"4\",\"5\",\"6\"],\"default\":\"1\"},\"MEMORY\":{\"description\":\"Amount of memory per instance.\",\"type\":\"string\",\"default\":\"10240\"},\"VHOST_DNS_ZONE\":{\"description\":\"DNS Zone selection for Vhost (kpn.com or kpn.org)\",\"type\":\"string\",\"enum\":[\"public\",\"private\"],\"default\":\"private\"}}},\"resources\":{\"allocation/${@tenant}/application/${@name}-server\":{\"cpus\":\"${CPU | number}\",\"env\":{},\"image\":\"${@appcatalog}/release/kpn/ollama-server:all\",\"imageConsole\":\"registry.cp.kpn-dsh.com/connectors/ollama-server:all\",\"instances\":\"${INSTANCES | number}\",\"mem\":\"${MEMORY | number}\",\"name\":\"${@name}-server\",\"needsToken\":false,\"singleInstance\":true,\"user\":\"${@uid}:${@gid}\"},\"allocation/${@tenant}/application/${@name}-ui\":{\"cpus\":0.1,\"env\":{\"OLLAMA_BASE_URL\":\"http://${@name}-server:11434\"},\"exposedPorts\":{\"8080\":{\"auth\":\"system-fwd-auth@view,manage\",\"tls\":\"auto\",\"vhost\":\"{ vhost('${@name}.${@tenant}', '${VHOST_DNS_ZONE}') }\"}},\"image\":\"${@appcatalog}/release/kpn/ollama-ui:0.5.0\",\"imageConsole\":\"registry.cp.kpn-dsh.com/connectors/ollama-ui:0.5.0\",\"instances\":1,\"mem\":3000,\"name\":\"${@name}-ui\",\"needsToken\":false,\"singleInstance\":true,\"user\":\"${@uid}:${@gid}\",\"volumes\":{\"/app/backend/data\":{\"name\":\"{ volume('${@name}-ollama')}\"}}},\"allocation/${@tenant}/vhost/${@name}.${@tenant}@${VHOST_DNS_ZONE}\":\"${@name}.${@tenant}@${VHOST_DNS_ZONE}\",\"allocation/${@tenant}/volume/${@name}-ollama\":{\"name\":\"${@name}-ollama\",\"size\":5}}}"
  },
  {
    "draft": false,
    "lastModified": 1691140701080.0,
    "payload": "{\"id\":\"kpn/keyring-service\",\"name\":\"keyring-service\",\"version\":\"0.4.2\",\"vendor\":\"KPN\",\"kind\":\"manifest\",\"apiVersion\":\"v0-alpha\",\"description\":\"KPN keyring service.\",\"moreInfo\":\"# keyring service\\n\\nThe **keyring service** is a service that exposes a REST-service that maps identifiers from a *domain* to a *codomain*. E.g. it can map Boss identifiers to KRN identifiers. The **keyring service** runs in your own tenant environment, under your own control. It optionally provides an OpenAPI specification of the REST-service and a Swagger UI to explore and test the REST-service and it can expose metrics for a Prometheus scraper.\\n\\nBe sure to check the [keyring documentation](https://keyring.dsh-dev.dsh.np.aws.kpn.org/using/app-catalog.html) for more and important information.\\n\",\"contact\":\"greenbox@kpn.com\",\"configuration\":{\"$schema\":\"https://json-schema.org/draft/2019-09/schema\",\"type\":\"object\",\"properties\":{\"CODOMAINS\":{\"description\":\"select the codomain(s) for the relation separated by commas (allowed codomain names: BOSS, ECID, ETKN, KIDH, KRN, SBCM)\",\"type\":\"string\",\"enum\":[\"BOSS\",\"KRN\",\"SBCM\",\"BOSS,KRN,SBCM\",\"ECID\",\"ETKN\",\"KIDH\",\"ECID,ETKN,KIDH\"]},\"DOMAINS\":{\"description\":\"select the domain(s) for the relation separated by commas (allowed domain names: BOSS, ECID, ETKN, KIDH, KRN, SBCM)\",\"type\":\"string\",\"enum\":[\"BOSS\",\"KRN\",\"SBCM\",\"BOSS,KRN,SBCM\",\"ECID\",\"ETKN\",\"KIDH\",\"ECID,ETKN,KIDH\"]},\"ENABLE_METRICS_EXPORTER\":{\"description\":\"select whether to enable exporting metrics\",\"type\":\"string\",\"enum\":[\"false\",\"true\"],\"default\":\"true\"},\"ENABLE_OPEN_API_SPECIFICATION\":{\"description\":\"select whether to enable the openapi specification\",\"type\":\"string\",\"enum\":[\"false\",\"true\"],\"default\":\"true\"},\"ENABLE_SWAGGER_UI\":{\"description\":\"select whether to enable the swagger ui\",\"type\":\"string\",\"enum\":[\"false\",\"true\"],\"default\":\"true\"}}},\"resources\":{\"allocation/${@tenant}/application/${@name}\":{\"cpus\":1,\"env\":{\"BASE_DOMAINS_CONFIG_FILE\":\"base-domains.conf\",\"CODOMAINS\":\"${CODOMAINS}\",\"DEFAULT_CACHE_TYPE\":\"chronicle\",\"DOMAINS\":\"${DOMAINS}\",\"ENABLE_DERIVED_RELATIONS\":\"true\",\"ENABLE_JVM_METRICS_EXPORTER\":\"${ENABLE_METRICS_EXPORTER}\",\"ENABLE_METRICS_EXPORTER\":\"${ENABLE_METRICS_EXPORTER}\",\"ENABLE_OPEN_API_SPECIFICATION\":\"${ENABLE_OPEN_API_SPECIFICATION}\",\"ENABLE_SWAGGER_UI\":\"${ENABLE_SWAGGER_UI}\",\"GROUP_ID_PREFIX\":\"${@tenant}_\",\"HEAP_MEMORY\":\"384\",\"KEYRING_SERVICE_PORT\":\"8088\",\"LOG_LEVEL\":\"info\",\"LOG_LEVEL_AKKA\":\"error\",\"LOG_LEVEL_CACHE\":\"info\",\"LOG_LEVEL_CODOMAIN_VALUES\":\"info\",\"LOG_LEVEL_ENGINE\":\"info\",\"LOG_LEVEL_ENTRYPOINT\":\"error\",\"LOG_LEVEL_MASTER_RELATION\":\"info\",\"LOG_LEVEL_SERVICE\":\"info\",\"MASTER_RELATIONS_CONFIG_FILE\":\"master-relations.conf\",\"METRICS_EXPORTER_PREFIX\":\"\",\"SWAGGER_UI_CONTACT_NAME\":\"Greenbox Team\",\"SWAGGER_UI_CONTACT_URL\":\"https://confluence.kpn.org/display/KEYR/Keyring\",\"SWAGGER_UI_DOCUMENTATION_URL\":\"https://keyring.dsh-dev.dsh.np.aws.kpn.org\"},\"exposedPorts\":{\"8088\":{\"auth\":\"system-fwd-auth@view,manage\",\"tls\":\"auto\",\"vhost\":\"{ vhost('${@name}.${@tenant}', 'public') }\"}},\"image\":\"${@appstore}/release/kpn/keyring-service:0.4.2\",\"imageConsole\":\"registry.cp.kpn-dsh.com/greenbox-dev/keyring-service:0.4.2\",\"instances\":1,\"mem\":2048,\"metrics\":{\"path\":\"/\",\"port\":9585},\"name\":\"${@name}\",\"needsToken\":true,\"singleInstance\":false,\"user\":\"${@uid}:${@gid}\"},\"allocation/${@tenant}/vhost/${@name}.${@tenant}@public\":\"${@name}.${@tenant}@public\"}}"
  },
  {
    "draft": false,
    "lastModified": 1699605666478.0,
    "payload": "{\"id\":\"kpn/keyring-kafka-database-extractor\",\"name\":\"DSH Database Extractor\",\"version\":\"0.3.0\",\"vendor\":\"KPN\",\"kind\":\"manifest\",\"apiVersion\":\"v0-alpha\",\"description\":\"Application to bulk load/sync records from a relational database to Kafka in DSH\",\"moreInfo\":\"## Database Extractor for Apache Kafka in DSH \\nAn application to import data from any relational database with a JDBC driver into an Apache Kafka topic by extending the [Kafka Connect JDBC Source connector](https://docs.confluent.io/current/connect/kafka-connect-jdbc/source-connector/index.html) project. \\n\\n**1. Data Source:** Currently, we support Teradata, Postgres (includes Yugabyte), MySQL, Oracle, SQLite, SAP Sybase, Microsoft SQL Server and IBM DB2.\\n\\n**2. Kafka JDBC Database Extractor Application:** The application can be deployed with single or multiple workers. Cluster mode ensures fault tolerancy and HA. The data extraction mode can be configured by setting the `MODE` environment variable. These can be bulk, timestamp, incrementing or timestamp+incrementing. Furthermore, the `POLL_INTERVAL` environment variable can be used to determine the degree of realtime sync or periodicity of sync. \\n\\n**3. Data Sink:** The data is stored in JSON format in the kafka topic `DATABASE_OUTPUT_TOPIC`.\\n ### More Information\\nThe _Greenbox Team_ can be reached at **greenbox@kpn.com** for more information.\",\"contact\":\"greenbox@kpn.com\",\"configuration\":{\"$schema\":\"https://json-schema.org/draft/2019-09/schema\",\"type\":\"object\",\"properties\":{\"DATABASE\":{\"description\":\"The database to connect to. Currently, we support Teradata, Postgres (includes Yugabyte), MySQL, Oracle, SQLite, SAP Sybase, Microsoft SQL Server and IBM DB2.\",\"type\":\"string\",\"enum\":[\"teradata\",\"postgresql\",\"mysql\",\"oracle\",\"sqlite\",\"sybase\",\"sqlserver\",\"db2\"],\"default\":\"teradata\"},\"DATABASE_JDBC_URL\":{\"description\":\"The database host to connect to. Ensure that the host is accessible from the application. Example: jdbc:teradata://tdp.kpn.org\",\"type\":\"string\",\"default\":\"jdbc:teradata://tdp.kpn.org\"},\"DATABASE_OUTPUT_TOPIC\":{\"description\":\"Name of the Kafka topic to write the records to.\",\"type\":\"string\",\"default\":\"scratch.database-extractor-output.greenbox\"},\"DATABASE_PASSWORD_FIELD_IN_SECRET_STORE\":{\"description\":\"Name of the field in the DSH secrets store that contains the Database password.\",\"type\":\"string\"},\"DATABASE_QUERY\":{\"description\":\"The SQL query to execute. The query should be a valid SQL query for the database specified in the `DATABASE` environment variable.\",\"type\":\"string\",\"default\":\"SELECT * FROM \\u003ctable_name\\u003e\"},\"DATABASE_TIMEZONE\":{\"description\":\"Used to recognize the timezone of timestamp values. The default is UTC. KPN Teradata uses Europe/Amsterdam.\",\"type\":\"string\",\"default\":\"UTC\"},\"DATABASE_USER\":{\"description\":\"Database user name. Make sure this user has the right permissions to execute the query.\",\"type\":\"string\"},\"INCREMENTING_COLUMN_NAME\":{\"description\":\"This is used in the `incrementing` and `timestamp+incrementing` extraction modes. Only relevant for these two modes. The column name should be a valid autoincrementing column in the database table.\",\"type\":\"string\",\"default\":\"\"},\"INSTANCES\":{\"description\":\"The number of instances to deploy in distributed mode (cluster). The default is 1.\",\"type\":\"string\",\"enum\":[\"1\",\"2\",\"3\",\"4\",\"5\"],\"default\":\"1\"},\"LOG_LEVEL\":{\"description\":\"The log level for the application. The default is INFO.\",\"type\":\"string\",\"enum\":[\"TRACE\",\"DEBUG\",\"INFO\",\"WARN\",\"ERROR\"],\"default\":\"INFO\"},\"MEMORY\":{\"description\":\"Amount of memory per instance.\",\"type\":\"string\",\"default\":\"1024\"},\"MODE\":{\"description\":\"The data extraction mode can be `bulk`, `timestamp`, `incrementing` and `timestamp+incrementing`. Bulk is used for batch extraction whereas the other modes are used for realtime extraction.\",\"type\":\"string\",\"enum\":[\"bulk\",\"timestamp\",\"incrementing\",\"timestamp+incrementing\"],\"default\":\"timestamp\"},\"POLL_INTERVAL\":{\"description\":\"Interval in milliseconds for polling (24 hours is 86400000 milliseconds).\",\"type\":\"string\",\"default\":\"60000\"},\"TIMESTAMP_COLUMN_NAME\":{\"description\":\"This is used in the `timestamp` and `timestamp+incrementing` extraction modes. Only relevant for these two modes. The column name should be a valid date/timestamp column in the database table.\",\"type\":\"string\",\"default\":\"\"}}},\"resources\":{\"allocation/${@tenant}/application/${@name}\":{\"cpus\":0.1,\"env\":{\"DATABASE\":\"${DATABASE}\",\"DATABASE_JDBC_URL\":\"${DATABASE_JDBC_URL}\",\"DATABASE_OUTPUT_TOPIC\":\"${DATABASE_OUTPUT_TOPIC}\",\"DATABASE_QUERY\":\"${DATABASE_QUERY}\",\"DATABASE_TIMEZONE\":\"${DATABASE_TIMEZONE}\",\"DATABASE_USER\":\"${DATABASE_USER}\",\"DEPLOYMENT\":\"distributed\",\"DISTRIBUTED_MODE_CONFIG_TOPIC\":\"scratch.${@name}-config.${@tenant}\",\"DISTRIBUTED_MODE_OFFSETS_TOPIC\":\"scratch.${@name}-offset.${@tenant}\",\"DISTRIBUTED_MODE_STATUS_TOPIC\":\"scratch.${@name}-status.${@tenant}\",\"INCREMENTING_COLUMN_NAME\":\"${INCREMENTING_COLUMN_NAME}\",\"KAFKA_OPTS\":\"-Xms256M -Xmx1000M\",\"LOG_LEVEL\":\"${LOG_LEVEL}\",\"MODE\":\"${MODE}\",\"POLL_INTERVAL\":\"${POLL_INTERVAL}\",\"TIMESTAMP_COLUMN_NAME\":\"${TIMESTAMP_COLUMN_NAME}\"},\"image\":\"${@appcatalog}/release/kpn/keyring-kafka-database-extractor:0.3.0\",\"imageConsole\":\"registry.cp.kpn-dsh.com/greenbox/keyring-kafka-database-extractor:0.3.0\",\"instances\":\"${INSTANCES | number}\",\"mem\":\"${MEMORY | number}\",\"metrics\":{\"path\":\"/\",\"port\":9009},\"name\":\"${@name}\",\"needsToken\":true,\"secrets\":[{\"injections\":[{\"env\":\"DATABASE_PASSWORD\"}],\"name\":\"${DATABASE_PASSWORD_FIELD_IN_SECRET_STORE}\"}],\"singleInstance\":false,\"user\":\"${@uid}:${@gid}\"},\"allocation/${@tenant}/application/${@name}-ui\":{\"cpus\":0.1,\"env\":{\"CONNECT_URL\":\"${@name}:8083\",\"PORT\":\"8000\"},\"exposedPorts\":{\"8000\":{\"auth\":\"system-fwd-auth@view,manage\",\"tls\":\"auto\",\"vhost\":\"{ vhost('${@name}.${@tenant}', 'public') }\"}},\"image\":\"${@appcatalog}/release/kpn/keyring-kafka-database-extractor-ui:0.3.0\",\"imageConsole\":\"registry.cp.kpn-dsh.com/greenbox/keyring-kafka-database-extractor-ui:0.3.0\",\"instances\":1,\"mem\":16,\"name\":\"${@name}\",\"needsToken\":false,\"singleInstance\":false,\"user\":\"${@uid}:${@gid}\"},\"allocation/${@tenant}/topic/${@name}-config\":{\"name\":\"${@name}-config\",\"partitions\":1,\"replicationFactor\":3},\"allocation/${@tenant}/topic/${@name}-offset\":{\"name\":\"${@name}-offset\",\"partitions\":1,\"replicationFactor\":3},\"allocation/${@tenant}/topic/${@name}-status\":{\"name\":\"${@name}-status\",\"partitions\":1,\"replicationFactor\":3},\"allocation/${@tenant}/vhost/${@name}.${@tenant}@public\":\"${@name}.${@tenant}@public\"}}"
  },
  {
    "draft": false,
    "lastModified": 1706273487658.0,
    "payload": "{\"id\":\"kpn/zookeeper-proxy\",\"name\":\"Zookeeper Proxy\",\"version\":\"1.2.1\",\"vendor\":\"KPN\",\"kind\":\"manifest\",\"apiVersion\":\"v0-alpha\",\"description\":\"Tenant proxy to Zookeeper\",\"moreInfo\":\"## Features  \\n- Transparent proxy to the platform-provided Zookeeper. \\n- Data you store via this proxy in Zookeeper is fully isolated to your tenant.\\n## How to connect\\nWhen you need a Zookeeper in your setup, you can directly connect to this proxy as you would to any regular Zookeeper deployment, using the Zookeeper client of your preference. \\nTo connect to this proxy, simply configure your client with the following connection endpoint: `zk-\\u003czookeeper-proxy-name\\u003e.\\u003ctenant\\u003e.marathon.mesos:2181`\\n## Release Notes  \\n- support added for checkWatches (opcode 17) and removeWatches (opcode 18) Zookeeper operations\",\"contact\":\"dsh-appcatalog@kpn.com\",\"configuration\":{\"$schema\":\"https://json-schema.org/draft/2019-09/schema\",\"type\":\"object\",\"properties\":{\"TRACE_LOGGING\":{\"description\":\"enable or disable trace logging\",\"type\":\"string\",\"enum\":[\"true\",\"false\"],\"default\":\"false\"}}},\"resources\":{\"allocation/${@tenant}/application/zk-${@name}\":{\"cpus\":0.1,\"env\":{\"PORT\":\"2181\",\"PREFIX\":\"__tenant_${@tenant}\",\"SERVERS\":\"tenant-zookeeper-1.dsh.marathon.mesos:2181,tenant-zookeeper-2.dsh.marathon.mesos:2181,tenant-zookeeper-3.dsh.marathon.mesos:2181\",\"TRACE\":\"${TRACE_LOGGING}\"},\"image\":\"${@appcatalog}/release/kpn/zookeeper-proxy:0.3.1\",\"instances\":1,\"mem\":256,\"name\":\"zk-${@name}\",\"needsToken\":false,\"network\":\"enclave\",\"singleInstance\":true,\"user\":\"${@uid}:${@gid}\"}}}"
  },
  {
    "draft": false,
    "lastModified": 1697188834647.0,
    "payload": "{\"id\":\"kpn/sql-database-viewer\",\"name\":\"SQL Database Viewer\",\"version\":\"1.1.2\",\"vendor\":\"KPN\",\"kind\":\"manifest\",\"apiVersion\":\"v0-alpha\",\"description\":\"Managed Pgweb 0.13.1, a web-based database browser for PostgreSQL, written in Go.\",\"moreInfo\":\"https://github.com/sosedoff/pgweb\",\"contact\":\"dsh-appcatalog@kpn.com\",\"configuration\":{\"$schema\":\"https://json-schema.org/draft/2019-09/schema\",\"type\":\"object\",\"properties\":{\"DATABASE_NAME\":{\"description\":\"database name\",\"type\":\"string\"},\"DNS_ZONE\":{\"description\":\"DNS zone name for the vhost\",\"type\":\"dns-zone\"}}},\"resources\":{\"allocation/${@tenant}/application/${@name}\":{\"cpus\":0.1,\"env\":{\"DB_HOST\":\"{ database_host('${DATABASE_NAME}') }\",\"DB_ID\":\"{ database_id('${DATABASE_NAME}') }\",\"DB_USER\":\"{ database_user('${DATABASE_NAME}') }\"},\"exposedPorts\":{\"8081\":{\"auth\":\"system-fwd-auth@view,manage\",\"tls\":\"auto\",\"vhost\":\"{ vhost('${@name}.${@tenant}', '${DNS_ZONE}') }\"}},\"image\":\"${@appcatalog}/release/kpn/pgweb:0.13.1-6\",\"instances\":1,\"mem\":256,\"name\":\"${@name}\",\"needsToken\":false,\"secrets\":[{\"injections\":[{\"env\":\"DB_PASSWORD\"}],\"name\":\"system/dbaas/${DATABASE_NAME}_password\"}],\"singleInstance\":false,\"user\":\"${@uid}:${@gid}\"},\"allocation/${@tenant}/vhost/${@name}.${@tenant}@${DNS_ZONE}\":\"${@name}.${@tenant}@${DNS_ZONE}\"}}"
  },
  {
    "draft": false,
    "lastModified": 1697814490111.0,
    "payload": "{\"id\":\"kpn/eavesdropper\",\"name\":\"eavesdropper\",\"version\":\"0.7.1\",\"vendor\":\"KPN\",\"kind\":\"manifest\",\"apiVersion\":\"v0-alpha\",\"description\":\"Web application to visualise messages on Kafka streams in realtime.\",\"moreInfo\":\"## View data in realtime on Data Streams  \\nVia a visual interface you can easily subscribe on a data stream to follow the data in realtime. You can even filter on keys or values, to only see a selection of the data that you're interested in.  \\n## Built-in Web Authentication  \\nShares a SSO login with the Console, which allows you to login with the same credentials. Also, if you're logged in to the Console you'll also have access to the Eavesdropper.  \\nOnce deployed, the Eavesdropper will become available on a vhost of your choosing.  \\n## Built-in Kafka Authentication  \\nRuns in your tenant environment and automatically connects to Kafka as your tenant, meaning it has the same access rights as your other Kafka applications.\",\"contact\":\"dsh-appcatalog@kpn.com\",\"configuration\":{\"$schema\":\"https://json-schema.org/draft/2019-09/schema\",\"type\":\"object\",\"properties\":{\"DNS_ZONE\":{\"description\":\"DNS zone name for the vhost\",\"type\":\"dns-zone\"},\"LOG_LEVEL\":{\"description\":\"application log level\",\"type\":\"string\",\"enum\":[\"error\",\"warn\",\"info\"],\"default\":\"info\"},\"LOG_LEVEL_MONITOR\":{\"description\":\"monitoring log level\",\"type\":\"string\",\"enum\":[\"error\",\"warn\",\"info\"],\"default\":\"info\"},\"LOG_LEVEL_SERVICE\":{\"description\":\"topic tracing log level\",\"type\":\"string\",\"enum\":[\"error\",\"warn\",\"info\"],\"default\":\"info\"}}},\"resources\":{\"allocation/${@tenant}/application/${@name}\":{\"cpus\":0.5,\"env\":{\"GROUP_ID_PREFIX\":\"${@tenant}_\",\"LOG_LEVEL\":\"${LOG_LEVEL}\",\"LOG_LEVEL_MONITOR\":\"${LOG_LEVEL_MONITOR}\",\"LOG_LEVEL_SERVICE\":\"${LOG_LEVEL_SERVICE}\"},\"exposedPorts\":{\"8081\":{\"auth\":\"system-fwd-auth@view,manage\",\"tls\":\"auto\",\"vhost\":\"{ vhost('${@name}.${@tenant}', '${DNS_ZONE}') }\"}},\"image\":\"${@appstore}/release/kpn/eavesdropper:0.7.0\",\"instances\":1,\"mem\":512,\"name\":\"${@name}\",\"needsToken\":true,\"singleInstance\":false,\"user\":\"${@uid}:${@gid}\"},\"allocation/${@tenant}/vhost/${@name}.${@tenant}@${DNS_ZONE}\":\"${@name}.${@tenant}@${DNS_ZONE}\"}}"
  },
  {
    "draft": false,
    "lastModified": 1716296099236.0,
    "payload": "{\"id\":\"kpn/airflow-persistent\",\"name\":\"Airflow persistent\",\"version\":\"1.0.1\",\"vendor\":\"KPN\",\"kind\":\"manifest\",\"apiVersion\":\"v0-alpha\",\"description\":\"Airflow stack with a persistent database to run your pipelines.\",\"moreInfo\":\"## Airflow persistent\\nThe airflow persistent instance provides an environment to run Airflow DAGs. \\nThe python code (DAGs) is pulled in using a gitsync. \\nAll state of the airflow instance is stored in a separate postgres database and the logs are stored in an S3 bucket.\\n\\nVersion: 1.0.1\\n\\n- Airflow:  2.9.1  \\n- Python:   3.12  \\n- Gitsync:  3.6.5  \\n- Postgres: 15      \\n\\n\\n### Before you start \\nAdd the following secrets to the DSH secret store: \\n1. airflow admin user password (create a password that you will use for the airflow ui) \\n2. git sync password (can also be a personal access token) \\n3. aws access key id for remote logging (might also use the system/objectstore/access_key_id)\\n4. aws secret key for remote logging (might also use the system/objectstore/secret_access_key)\\n5. postgres airflow user password\\n\\nThese secrets will be used for the container configuration. \\n\\n### Airflow \\nThe airflow app will persist all logs and state. The logs are stored in an S3 bucket and the state is stored in a postgres database.\\n\\nTo login to the airflow instance, the default user is `admin` the password is provided by the tenant using the `TENANT_AIRFLOW_ADMIN_USER_PASSWORD_FIELD_IN_SECRET_STORE` environment variable. Which takes the secret you have created earlier from the DSH secrets store and injects it into the container. \\n\\n#### DAGs\\nThe core concept of airflow is a DAG (Directed Acyclic Graph), collecting Tasks together, organized with dependencies and relationships to say how they should run. [airflow dags](https://airflow.apache.org/docs/apache-airflow/stable/core-concepts/dags.html)\\n\\nAll of the code specified in airflow is being wrapped around in tasks and orchestrated by the DAGs. To understand how they work, have a look at the following [Fundamental concepts](https://airflow.apache.org/docs/apache-airflow/stable/tutorial/fundamentals.html). The page also provides a turorial with example DAGs that you can try out.\\n\\n#### Logging\\nAll logs are stored in an S3 bucket, to make sure that all logs are persisted. Airflow is able to show these logs in the Airflow UI. It is also possible to retrieve these logs from the S3 bucket without Airflow, by usinge an s3 client.\\n\\nFor the configuration of the remote logging make sure to set the following variables:\\n- DEX_LOGGING_REMOTE_CONN_ID: connection id used by airflow to connect and configure remote logging.\\n- DEX_LOGGING_REMOTE_BASE_LOG_FOLDER: S3 address to write the logs to.\\n- DEX_LOGGING_AWS_REGION: region where the bucket is located.\\n- DEX_LOGGING_AWS_ACCESS_KEY_ID_FIELD_IN_SECRET_STORE: field in the secret store that contains the aws access key id to connect to the S3 bucket.\\n- DEX_LOGGING_AWS_SECRET_KEY_FIELD_IN_SECRET_STORE: field in the secret store that contains the aws secret key to connect to the S3 bucket.\\n- DEX_LOGGING_REMOTE_ENCRYPT_S3_LOGS: encrypt the airflow logs in the S3 bucket.\\n\\n#### Importing packages\\nAt the moment the airflow setup from the app catalog is limited to the pip dependencies that came pre-installed with it. In the future we will provide the option for tenants to install packages to the airflow app. For now you can make use of the [PythonVirtualenvOperator](https://airflow.apache.org/docs/apache-airflow/stable/howto/operator/python.html#pythonvirtualenvoperator)\\n\\nWith the PythonVirtualenvOperator you can pull-in any public package.\\n\\n**Caution: Tenants are responsible for the python packages they import into their airflow container. Please follow the KPN Security Policy ([KSP](https://ciso-ksp.kpnnet.org/welcome/KSP)) as a guideline.** \\n\\n### Gitsync \\nThe gitsync will pull in a git branch and put the code in the specified directory. For the gitsync configuration see all variables with the prefix `GIT_SYNC`. You can put all of your DAGs and python code in this git repository, here the python project structure applies.\\n\\nFor the configuration, make sure to set the following variables to the right values:\\n- GIT_SYNC_DEST: the destination on the container where you would like to have the dags pushed to.  e.g. `/opt/airflow/dags/dex`\\n- GIT_SYNC_REPO: url of the git repo to sync.\\n- GIT_SYNC_BRANCH: git branch to pull in.\\n- GIT_SYNC_WAIT: number of seconds between syncs.\\n- GIT_SYNC_ROOT: where on the root should you do the git operations. the default is sufficient.\\n- GIT_SYNC_ONE_TIME: specifies if the gitsync should continuously pull or exit after the first pull. Default is false.\\n- GIT_SYNC_USERNAME: username of the user to pull in the git repo.\\n- GIT_SYNC_PASSWORD_FIELD_IN_SECRET_STORE: the field in the secret store that contains the password of your git account.\\n\\nFor some more background have a look at the [gitsync](https://github.com/kubernetes/git-sync/tree/v3.6.5?tab=readme-ov-file#primary-flags) repo. \\n\\n### Database\\nFor all of the state we make use of a Postgres database. It is the Postgresql database 15.6.\\n\\nThe configuration for the database can be found in the following environment variables:\\n- POSTGRESQL_DATABASE The database to use for airflow\\n- POSTGRESQL_USERNAME The username of the airflow user.\\n- POSTGRESQL_PASSWORD_FIELD_IN_SECRET_STORE the field in the secret store that contains the password for the default postgres user.\\n\\n### More Information \\nThe _DEX Team_ can be reached at **dex@kpn.com** for more information.\",\"contact\":\"dex@kpn.com\",\"configuration\":{\"$schema\":\"https://json-schema.org/draft/2019-09/schema\",\"type\":\"object\",\"properties\":{\"AIRFLOW_CPU\":{\"description\":\"Amount of CPU cores per instance.\",\"type\":\"string\",\"default\":\"2\"},\"AIRFLOW_INSTANCES\":{\"description\":\"The number of instances to deploy in distributed mode (cluster). The default is 1.\",\"type\":\"string\",\"enum\":[\"1\",\"2\",\"3\",\"4\",\"5\"],\"default\":\"1\"},\"AIRFLOW_MEMORY\":{\"description\":\"Amount of memory per instance.\",\"type\":\"string\",\"default\":\"4096\"},\"DEX_LOGGING_AWS_ACCESS_KEY_ID_FIELD_IN_SECRET_STORE\":{\"description\":\"Field in the secret store that contains the aws access key id to connect to the S3 bucket.\",\"type\":\"string\",\"default\":\"system/objectstore/access_key_id\"},\"DEX_LOGGING_AWS_REGION\":{\"description\":\"region where the bucket is located.\",\"type\":\"string\",\"default\":\"eu-west-1\"},\"DEX_LOGGING_AWS_SECRET_KEY_FIELD_IN_SECRET_STORE\":{\"description\":\"Field in the secret store that contains the aws secret key to connect to the S3 bucket.\",\"type\":\"string\",\"default\":\"system/objectstore/secret_access_key\"},\"DEX_LOGGING_REMOTE_BASE_LOG_FOLDER\":{\"description\":\"S3 address to write the logs to.\",\"type\":\"string\",\"default\":\"s3://BUCKET_NAME/airflow-persistent/logs\"},\"DEX_LOGGING_REMOTE_CONN_ID\":{\"description\":\"connection id used by airflow to connect and configure remote logging.\",\"type\":\"string\",\"default\":\"s3_logging_conn\"},\"DEX_LOGGING_REMOTE_ENCRYPT_S3_LOGS\":{\"description\":\"encrypt the airflow logs in the S3 bucket.\",\"type\":\"string\",\"enum\":[\"true\",\"false\"],\"default\":\"false\"},\"GIT_SYNC_BRANCH\":{\"description\":\"git branch to pull in.\",\"type\":\"string\",\"default\":\"dev\"},\"GIT_SYNC_DEST\":{\"description\":\"Folder destination on the host machine where to pull git repo towards.\",\"type\":\"string\",\"default\":\"/opt/airflow/dags/repo_name\"},\"GIT_SYNC_ONE_TIME\":{\"description\":\"specify to only pull on startup or continuously, if true the gitsync exits after the first sync.\",\"type\":\"string\",\"enum\":[\"true\",\"false\"],\"default\":\"false\"},\"GIT_SYNC_PASSWORD_FIELD_IN_SECRET_STORE\":{\"description\":\"Name of the field in the DSH secrets store that contains the Git Sync password.\",\"type\":\"string\"},\"GIT_SYNC_REPO\":{\"description\":\"url of the git repo to sync.\",\"type\":\"string\",\"default\":\"\"},\"GIT_SYNC_ROOT\":{\"description\":\"the root directory for git-sync operations, under which --dest will be created.\",\"type\":\"string\",\"default\":\"/tmp/git\"},\"GIT_SYNC_USERNAME\":{\"description\":\"username of the user to pull in the git repo.\",\"type\":\"string\",\"default\":\"user@github.com\"},\"GIT_SYNC_WAIT\":{\"description\":\"the number of seconds between syncs.\",\"type\":\"string\",\"default\":\"60\"},\"POSTGRESQL_DATABASE\":{\"description\":\"name of the database to be used in postgresql for airflow\",\"type\":\"string\",\"default\":\"airflow\"},\"POSTGRESQL_PASSWORD_FIELD_IN_SECRET_STORE\":{\"description\":\"the field in the secret store that contains the password for the default postgres user.\",\"type\":\"string\"},\"POSTGRESQL_USERNAME\":{\"description\":\"The username of the airflow user.\",\"type\":\"string\",\"default\":\"airflow\"},\"POSTGRES_CPU\":{\"description\":\"Amount of CPU cores per instance. For the postgres database.\",\"type\":\"string\",\"default\":\"1\"},\"POSTGRES_INSTANCES\":{\"description\":\"The number of instances to deploy in distributed mode (cluster). For the postgres database. The default is 1.\",\"type\":\"string\",\"enum\":[\"1\",\"2\",\"3\",\"4\",\"5\"],\"default\":\"1\"},\"POSTGRES_MEMORY\":{\"description\":\"Amount of memory per instance. For the postgres database.\",\"type\":\"string\",\"default\":\"2048\"},\"POSTGRES_VOLUME_SIZE\":{\"description\":\"The amount of disk space in GB, for the postgres database.\",\"type\":\"string\",\"default\":\"5\"},\"TENANT_AIRFLOW_ADMIN_USER_PASSWORD_FIELD_IN_SECRET_STORE\":{\"description\":\"Name of the field in the DSH secrets store that contains the airflow admin user password.\",\"type\":\"string\"}}},\"resources\":{\"allocation/${@tenant}/application/${@name}\":{\"cpus\":\"${AIRFLOW_CPU | number}\",\"env\":{\"AIRFLOW__CORE__LOAD_EXAMPLES\":\"false\",\"APPLICATION_NAME\":\"${@name}\",\"DEX_LOGGING_AWS_REGION\":\"${DEX_LOGGING_AWS_REGION}\",\"DEX_LOGGING_REMOTE_BASE_LOG_FOLDER\":\"${DEX_LOGGING_REMOTE_BASE_LOG_FOLDER}\",\"DEX_LOGGING_REMOTE_CONN_ID\":\"${DEX_LOGGING_REMOTE_CONN_ID}\",\"DEX_LOGGING_REMOTE_ENCRYPT_S3_LOGS\":\"${DEX_LOGGING_REMOTE_ENCRYPT_S3_LOGS}\",\"GIT_SYNC_BRANCH\":\"${GIT_SYNC_BRANCH}\",\"GIT_SYNC_DEST\":\"${GIT_SYNC_DEST}\",\"GIT_SYNC_ONE_TIME\":\"${GIT_SYNC_ONE_TIME}\",\"GIT_SYNC_REPO\":\"${GIT_SYNC_REPO}\",\"GIT_SYNC_ROOT\":\"${GIT_SYNC_ROOT}\",\"GIT_SYNC_USERNAME\":\"${GIT_SYNC_USERNAME}\",\"GIT_SYNC_WAIT\":\"${GIT_SYNC_WAIT}\",\"POSTGRESQL_DATABASE\":\"${POSTGRESQL_DATABASE}\",\"POSTGRESQL_HOSTNAME\":\"${@name}-postgres\",\"POSTGRESQL_USERNAME\":\"${POSTGRESQL_USERNAME}\"},\"exposedPorts\":{\"8080\":{\"auth\":\"system-fwd-auth@view,manage\",\"vhost\":\"{ vhost('${@name}.${@tenant}','public') }\"}},\"image\":\"${@appcatalog}/release/kpn/airflow-persistent:1.0.1\",\"imageConsole\":\"registry.cp.kpn-dsh.com/dex-dev/airflow-persistent:1.0.1\",\"instances\":\"${AIRFLOW_INSTANCES | number}\",\"mem\":\"${AIRFLOW_MEMORY | number}\",\"name\":\"${@name}\",\"needsToken\":true,\"secrets\":[{\"injections\":[{\"env\":\"GIT_SYNC_PASSWORD\"}],\"name\":\"${GIT_SYNC_PASSWORD_FIELD_IN_SECRET_STORE}\"},{\"injections\":[{\"env\":\"TENANT_AIRFLOW_ADMIN_USER_PASSWORD\"}],\"name\":\"${TENANT_AIRFLOW_ADMIN_USER_PASSWORD_FIELD_IN_SECRET_STORE}\"},{\"injections\":[{\"env\":\"DEX_LOGGING_AWS_ACCESS_KEY_ID\"}],\"name\":\"${DEX_LOGGING_AWS_ACCESS_KEY_ID_FIELD_IN_SECRET_STORE}\"},{\"injections\":[{\"env\":\"DEX_LOGGING_AWS_SECRET_KEY\"}],\"name\":\"${DEX_LOGGING_AWS_SECRET_KEY_FIELD_IN_SECRET_STORE}\"},{\"injections\":[{\"env\":\"POSTGRESQL_PASSWORD\"}],\"name\":\"${POSTGRESQL_PASSWORD_FIELD_IN_SECRET_STORE}\"}],\"singleInstance\":false,\"user\":\"${@uid}:${@gid}\"},\"allocation/${@tenant}/application/${@name}-postgres\":{\"cpus\":\"${POSTGRES_CPU | number}\",\"env\":{\"POSTGRESQL_DATABASE\":\"${POSTGRESQL_DATABASE}\",\"POSTGRESQL_USERNAME\":\"${POSTGRESQL_USERNAME}\"},\"image\":\"${@appcatalog}/release/kpn/postgresql:1.0.1\",\"imageConsole\":\"registry.cp.kpn-dsh.com/dex-dev/bitnami/postgresql:1.0.1\",\"instances\":\"${POSTGRES_INSTANCES | number}\",\"mem\":\"${POSTGRES_MEMORY | number}\",\"name\":\"${@name}-postgres\",\"needsToken\":true,\"secrets\":[{\"injections\":[{\"env\":\"POSTGRESQL_PASSWORD\"}],\"name\":\"${POSTGRESQL_PASSWORD_FIELD_IN_SECRET_STORE}\"}],\"singleInstance\":false,\"user\":\"${@uid}:${@gid}\",\"volumes\":{\"/bitnami/postgresql/\":{\"name\":\"{ volume('${@name}-postgres')}\"}}},\"allocation/${@tenant}/vhost/${@name}.${@tenant}@public\":\"${@name}.${@tenant}@public\",\"allocation/${@tenant}/volume/${@name}-postgres\":{\"name\":\"${@name}-postgres\",\"size\":\"${POSTGRES_VOLUME_SIZE | number}\"}}}"
  },
  {
    "draft": false,
    "lastModified": 1733993083718.0,
    "payload": "{\"id\":\"kpn/dsh-ollama\",\"name\":\"Ollama\",\"version\":\"0.5.0-mistral\",\"vendor\":\"KPN\",\"kind\":\"manifest\",\"apiVersion\":\"v0-alpha\",\"description\":\"LLM package manager and runtime with a web UI and a DSH chat bot\",\"moreInfo\":\"## Ollama: Run \\u0026 Manage Your LLMs in DSH\\n\\n[Ollama](https://ollama.com/), developed by Meta, is a package manager and runtime for your LLMs with a standardized REST interface to simplify your LLM interaction.\\n\\n### Key Features\\n\\n- **Web Chat Interface:** Interact with your models directly through a user-friendly web interface.\\n- **DSH Chatbot Integration:** Includes a built-in DSH chatbot powered by the Retrieval-Augmented Generation (RAG) system.\\n- **Multiple LLM Integration:** Supports various LLMs, including Mistral (Nvidia), Phi (Microsoft), Gemma (Google).\\n\\n### API Usage:\\n\\nThe [API Documentation](https://github.com/ollama/ollama/blob/main/docs/api.md) provides the list available end points.\\n\\n```bash\\n\\ncurl http://ollama-server.\\u003ctenant\\u003e.marathon.mesos:11434/api/generate -d '{\\n  \\\"model\\\": \\\"mistral:7b\\\",\\n  \\\"prompt\\\": \\\"Why is the sky blue?\\\",\\n  \\\"options\\\": {\\n    \\\"seed\\\": 42,\\n    \\\"temperature\\\": 0.9\\n  }\\n}'\\n\\n```\\n\\n### More Information\\n\\nThe _Unibox Team_ can be reached at **unibox@kpn.com** for more information.\",\"contact\":\"unibox@kpn.com\",\"configuration\":{\"$schema\":\"https://json-schema.org/draft/2019-09/schema\",\"type\":\"object\",\"properties\":{\"CPU\":{\"description\":\"Amount of CPU cores per instance.\",\"type\":\"string\",\"default\":\"3\"},\"INSTANCES\":{\"description\":\"Number of instances to deploy for horizontal scaling. The default is 1.\",\"type\":\"string\",\"enum\":[\"1\",\"2\",\"3\",\"4\",\"5\",\"6\"],\"default\":\"1\"},\"MEMORY\":{\"description\":\"Amount of memory per instance.\",\"type\":\"string\",\"default\":\"10240\"},\"VHOST_DNS_ZONE\":{\"description\":\"DNS Zone selection for Vhost (kpn.com or kpn.org)\",\"type\":\"string\",\"enum\":[\"public\",\"private\"],\"default\":\"private\"}}},\"resources\":{\"allocation/${@tenant}/application/${@name}-server\":{\"cpus\":\"${CPU | number}\",\"env\":{},\"image\":\"${@appcatalog}/release/kpn/ollama-server:mistral\",\"imageConsole\":\"registry.cp.kpn-dsh.com/connectors/ollama-server:mistral\",\"instances\":\"${INSTANCES | number}\",\"mem\":\"${MEMORY | number}\",\"name\":\"${@name}-server\",\"needsToken\":false,\"singleInstance\":true,\"user\":\"${@uid}:${@gid}\"},\"allocation/${@tenant}/application/${@name}-ui\":{\"cpus\":0.1,\"env\":{\"OLLAMA_BASE_URL\":\"http://${@name}-server:11434\"},\"exposedPorts\":{\"8080\":{\"auth\":\"system-fwd-auth@view,manage\",\"tls\":\"auto\",\"vhost\":\"{ vhost('${@name}.${@tenant}', '${VHOST_DNS_ZONE}') }\"}},\"image\":\"${@appcatalog}/release/kpn/ollama-ui:0.5.0-mistral\",\"imageConsole\":\"registry.cp.kpn-dsh.com/connectors/ollama-ui:0.5.0-mistral\",\"instances\":1,\"mem\":3000,\"name\":\"${@name}-ui\",\"needsToken\":false,\"singleInstance\":true,\"user\":\"${@uid}:${@gid}\",\"volumes\":{\"/app/backend/data\":{\"name\":\"{ volume('${@name}-ollama')}\"}}},\"allocation/${@tenant}/vhost/${@name}.${@tenant}@${VHOST_DNS_ZONE}\":\"${@name}.${@tenant}@${VHOST_DNS_ZONE}\",\"allocation/${@tenant}/volume/${@name}-ollama\":{\"name\":\"${@name}-ollama\",\"size\":5}}}"
  },
  {
    "draft": false,
    "lastModified": 1695894369565.0,
    "payload": "{\"id\":\"kpn/cmdline\",\"name\":\"Cmd Line\",\"version\":\"1.1.5\",\"vendor\":\"KPN\",\"kind\":\"manifest\",\"apiVersion\":\"v0-alpha\",\"description\":\"A browser based terminal with kafka command line utilities\",\"moreInfo\":\"Access a command line terminal on dsh through your browser. Includes the following tools:\\n  - openssl\\n  - curl\\n  - jq\\n  - kcl\\n  - dshkcl\\n  - nc\\n  - get_signed_certificate.sh\\n\\nkcl is an open source, 'one stop shop' to do anything with kafka. - https://github.com/twmb/kcl\\ndshkcl is a wrapper for kcl to deserialize consumed messages from dsh's custom envelope into json.\\njq is available for json data manipulation - https://stedolan.github.io/jq/\\nnetcat-openbsd \\\"the tcp/ip swiss army knife\\\" - https://man.openbsd.org/nc.1\",\"contact\":\"dsh-appcatalog@kpn.com\",\"configuration\":{\"$schema\":\"https://json-schema.org/draft/2019-09/schema\",\"type\":\"object\",\"properties\":{\"DNS_ZONE\":{\"description\":\"DNS zone name for the vhost\",\"type\":\"dns-zone\"}}},\"resources\":{\"allocation/${@tenant}/application/${@name}\":{\"cpus\":0.25,\"env\":{},\"exposedPorts\":{\"8080\":{\"auth\":\"system-fwd-auth@view,manage\",\"tls\":\"auto\",\"vhost\":\"{ vhost('${@name}.${@tenant}', '${DNS_ZONE}') }\"}},\"image\":\"${@appcatalog}/release/kpn/cmdline:1.1.4\",\"instances\":1,\"mem\":256,\"name\":\"${@name}\",\"needsToken\":true,\"singleInstance\":false,\"user\":\"${@uid}:${@gid}\"},\"allocation/${@tenant}/vhost/${@name}.${@tenant}@${DNS_ZONE}\":\"${@name}.${@tenant}@${DNS_ZONE}\"}}"
  },
  {
    "draft": false,
    "lastModified": 1733392958269.0,
    "payload": "{\"id\":\"kpn/dsh-ollama\",\"name\":\"Ollama\",\"version\":\"0.5.0-phi\",\"vendor\":\"KPN\",\"kind\":\"manifest\",\"apiVersion\":\"v0-alpha\",\"description\":\"LLM package manager and runtime with a web UI and a DSH chat bot\",\"moreInfo\":\"## Ollama: Run \\u0026 Manage Your LLMs in DSH\\n\\n[Ollama](https://ollama.com/), developed by Meta, is a package manager and runtime for your LLMs with a standardized REST interface to simplify your LLM interaction.\\n\\n### Key Features\\n\\n- **Web Chat Interface:** Interact with your models directly through a user-friendly web interface.\\n- **DSH Chatbot Integration:** Includes a built-in DSH chatbot powered by the Retrieval-Augmented Generation (RAG) system.\\n- **Multiple LLM Integration:** Supports various LLMs, including Mistral (Nvidia), Phi (Microsoft), Gemma (Google).\\n\\n### API Usage:\\n\\nThe [API Documentation](https://github.com/ollama/ollama/blob/main/docs/api.md) provides the list available end points.\\n\\n```bash\\n\\ncurl http://ollama-server.\\u003ctenant\\u003e.marathon.mesos:11434/api/generate -d '{\\n  \\\"model\\\": \\\"mistral:7b\\\",\\n  \\\"prompt\\\": \\\"Why is the sky blue?\\\",\\n  \\\"options\\\": {\\n    \\\"seed\\\": 42,\\n    \\\"temperature\\\": 0.9\\n  }\\n}'\\n\\n```\\n\\n### More Information\\n\\nThe _Unibox Team_ can be reached at **unibox@kpn.com** for more information.\",\"contact\":\"unibox@kpn.com\",\"configuration\":{\"$schema\":\"https://json-schema.org/draft/2019-09/schema\",\"type\":\"object\",\"properties\":{\"CPU\":{\"description\":\"Amount of CPU cores per instance.\",\"type\":\"string\",\"default\":\"3\"},\"INSTANCES\":{\"description\":\"Number of instances to deploy for horizontal scaling. The default is 1.\",\"type\":\"string\",\"enum\":[\"1\",\"2\",\"3\",\"4\",\"5\",\"6\"],\"default\":\"1\"},\"MEMORY\":{\"description\":\"Amount of memory per instance.\",\"type\":\"string\",\"default\":\"10240\"},\"VHOST_DNS_ZONE\":{\"description\":\"DNS Zone selection for Vhost (kpn.com or kpn.org)\",\"type\":\"string\",\"enum\":[\"public\",\"private\"],\"default\":\"private\"}}},\"resources\":{\"allocation/${@tenant}/application/${@name}-server\":{\"cpus\":\"${CPU | number}\",\"env\":{},\"image\":\"${@appcatalog}/release/kpn/ollama-server:phi\",\"imageConsole\":\"registry.cp.kpn-dsh.com/connectors/ollama-server:phi\",\"instances\":\"${INSTANCES | number}\",\"mem\":\"${MEMORY | number}\",\"name\":\"${@name}-server\",\"needsToken\":false,\"singleInstance\":true,\"user\":\"${@uid}:${@gid}\"},\"allocation/${@tenant}/application/${@name}-ui\":{\"cpus\":0.1,\"env\":{\"OLLAMA_BASE_URL\":\"http://${@name}-server:11434\"},\"exposedPorts\":{\"8080\":{\"auth\":\"system-fwd-auth@view,manage\",\"tls\":\"auto\",\"vhost\":\"{ vhost('${@name}.${@tenant}', '${VHOST_DNS_ZONE}') }\"}},\"image\":\"${@appcatalog}/release/kpn/ollama-ui:0.5.0\",\"imageConsole\":\"registry.cp.kpn-dsh.com/connectors/ollama-ui:0.5.0\",\"instances\":1,\"mem\":3000,\"name\":\"${@name}-ui\",\"needsToken\":false,\"singleInstance\":true,\"user\":\"${@uid}:${@gid}\",\"volumes\":{\"/app/backend/data\":{\"name\":\"{ volume('${@name}-ollama')}\"}}},\"allocation/${@tenant}/vhost/${@name}.${@tenant}@${VHOST_DNS_ZONE}\":\"${@name}.${@tenant}@${VHOST_DNS_ZONE}\",\"allocation/${@tenant}/volume/${@name}-ollama\":{\"name\":\"${@name}-ollama\",\"size\":5}}}"
  },
  {
    "draft": false,
    "lastModified": 1760441225812.0,
    "payload": "{\"id\":\"kpn/eavesdropper\",\"name\":\"Eavesdropper\",\"version\":\"0.10.0\",\"vendor\":\"KPN\",\"kind\":\"manifest\",\"apiVersion\":\"v0-alpha\",\"description\":\"Web application to visualize messages on Kafka topics in realtime\",\"moreInfo\":\"## Realtime record visualization\\n\\nThis app enables you to view records on your DSH Kafka topics in realtime. It can show the record's keys, values and headers in json, text or binary format, and also recognizes some custom formats for some special topics. Furthermore, it allows you to:\\n\\n* unwrap a record from the envelope that the DSH platform enforces on stream topics, showing the envelope metadata and the envelope payload separately,\\n* filter records based on regular expressions and/or sample ratio,\\n* show records individually or in list view,\\n* download/copy record values in json, text or binary format.\\n\\nWhen started from the App Catalog the Eavesdropper will be available with the same SSO authorization as the DSH console, and will expose all topics that your tenant is entitled to see.\",\"contact\":\"unibox@kpn.com\",\"configuration\":{\"$schema\":\"https://json-schema.org/draft/2019-09/schema\",\"type\":\"object\",\"properties\":{\"ALLOW_CUSTOM_GROUP_ID\":{\"description\":\"allow custom group id\",\"type\":\"string\",\"enum\":[\"true\",\"false\"],\"default\":\"true\"},\"ENVELOPE_DESERIALIZER_DSH_ENVELOPE\":{\"description\":\"enable or disable dsh envelope deserializer\",\"type\":\"string\",\"enum\":[\"disabled\",\"enabled\"],\"default\":\"enabled\"},\"ENVELOPE_DESERIALIZER_GZIP\":{\"description\":\"enable or disable gzip deserializer\",\"type\":\"string\",\"enum\":[\"disabled\",\"enabled\"],\"default\":\"enabled\"},\"LOG_LEVEL\":{\"description\":\"application log level\",\"type\":\"string\",\"enum\":[\"error\",\"warn\",\"info\"],\"default\":\"info\"},\"REPRESENTATION_DESERIALIZER_CODOMAIN_VALUES_RECORD\":{\"description\":\"enable keyring codomain values record deserializer\",\"type\":\"string\",\"enum\":[\"disabled\",\"enabled\"],\"default\":\"disabled\"},\"REPRESENTATION_DESERIALIZER_SCHEMA_STORE\":{\"description\":\"enable schema store deserializer\",\"type\":\"string\",\"enum\":[\"disabled\",\"enabled\"],\"default\":\"disabled\"},\"VHOST_DNS_ZONE\":{\"description\":\"application dns zone\",\"type\":\"string\",\"enum\":[\"private\",\"public\"],\"default\":\"private\"}}},\"resources\":{\"allocation/${@tenant}/application/${@name}\":{\"cpus\":0.1,\"env\":{\"ALLOW_CUSTOM_GROUP_ID\":\"${ALLOW_CUSTOM_GROUP_ID}\",\"CONSUMER_BUILDER\":\"dsh-datastreams-properties\",\"DEFAULT_GROUP_IDS\":\"*\",\"ENVELOPE_DESERIALIZER_DSH_ENVELOPE\":\"${ENVELOPE_DESERIALIZER_DSH_ENVELOPE}\",\"ENVELOPE_DESERIALIZER_GZIP\":\"${ENVELOPE_DESERIALIZER_GZIP}\",\"EXCLUDED_TOPICS\":\"\",\"EXCLUDED_TOPICS_REGEX\":\"\",\"GROUP_ID_PREFIX\":\"${@tenant}_\",\"INCLUDED_TOPICS\":\"\",\"INCLUDED_TOPICS_REGEX\":\"\",\"INCLUDE___CONSUMER_OFFSETS_TOPIC\":\"false\",\"INSTANCE_IDENTIFIER\":\"${@tenant}_${@name}\",\"LOG_LEVEL\":\"${LOG_LEVEL}\",\"LOG_LEVEL_APPLICATION\":\"${LOG_LEVEL}\",\"LOG_LEVEL_DESERIALIZER\":\"info\",\"LOG_LEVEL_DEVELOPMENT\":\"off\",\"LOG_LEVEL_ENTRYPOINT\":\"${LOG_LEVEL}\",\"LOG_LEVEL_GREENBOX\":\"error\",\"LOG_LEVEL_KAFKA_ADMIN_CLIENT\":\"off\",\"LOG_LEVEL_KAFKA_CLIENT\":\"error\",\"LOG_LEVEL_KAFKA_CONSUMER\":\"error\",\"LOG_LEVEL_MONITOR\":\"${LOG_LEVEL}\",\"LOG_LEVEL_MONITOR_DOWNSTREAM\":\"error\",\"LOG_LEVEL_MONITOR_DOWNSTREAM_STATUS\":\"error\",\"LOG_LEVEL_MONITOR_KEEP_ALIVE\":\"off\",\"LOG_LEVEL_MONITOR_UPSTREAM\":\"info\",\"LOG_LEVEL_RECDES\":\"info\",\"LOG_LEVEL_SERVICE\":\"${LOG_LEVEL}\",\"LOG_LEVEL_SERVICE_DOWNSTREAM\":\"info\",\"LOG_LEVEL_SERVICE_DOWNSTREAM_RECORD\":\"info\",\"LOG_LEVEL_SERVICE_KEEP_ALIVE\":\"off\",\"LOG_LEVEL_SERVICE_UPSTREAM\":\"info\",\"LOG_TIMESTAMPS\":\"disabled\",\"REPRESENTATION_DESERIALIZER_CODOMAIN_VALUES_RECORD\":\"${REPRESENTATION_DESERIALIZER_CODOMAIN_VALUES_RECORD}\",\"REPRESENTATION_DESERIALIZER_SCHEMA_STORE\":\"${REPRESENTATION_DESERIALIZER_SCHEMA_STORE}\",\"TITLE\":\"${@name}/${@tenant}\"},\"exposedPorts\":{\"8081\":{\"auth\":\"system-fwd-auth@view,manage\",\"tls\":\"auto\",\"vhost\":\"{ vhost('${@name}.${@tenant}', '${VHOST_DNS_ZONE}') }\"}},\"image\":\"${@appcatalog}/release/kpn/eavesdropper:0.10.0\",\"instances\":1,\"mem\":384,\"name\":\"${@name}\",\"needsToken\":true,\"singleInstance\":false,\"user\":\"${@uid}:${@gid}\"},\"allocation/${@tenant}/vhost/${@name}.${@tenant}@${VHOST_DNS_ZONE}\":\"${@name}.${@tenant}@${VHOST_DNS_ZONE}\"}}"
  },
  {
    "draft": false,
    "lastModified": 1717069850817.0,
    "payload": "{\"id\":\"kpn/http-source-connector\",\"name\":\"HTTP Source Connector\",\"version\":\"0.5.0\",\"vendor\":\"KPN\",\"kind\":\"manifest\",\"apiVersion\":\"v0-alpha\",\"description\":\"Application to produce data to Kafka via HTTP in DSH\",\"moreInfo\":\"## HTTP Source Kafka Connector with Oauth2 \\n\\n HTTP Source Kafka Connector with OAuth2 enables streaming data to DSH Kafka via HTTP with Digital Engine's AuthZ OAuth2 authentication. \\n\\n **1. HTTP Endpoint (Vhost)**: This will be automatically created during deployment based on the application name. It can be made `public` (kpn.com) or `private` (kpn.org) using the `VHOST_DNS_ZONE` environment variable. \\n\\n **2. OAuth2 Authentication**: You can choose the authz environment via `AUTHZ_ENVIRONMENT`. If you choose ACC (acceptance), the base url will be  https://api.acc.kpn.com and for PROD (production) it will be https://api.kpn.com . The fetching token endpoint for these urls is _oauth2/v1/token_ and validating token endpoint is _authz/v1/validate_. These will be arranged inside application according to `AUTHZ_ENVIRONMENT`. Authz Client ID, and Authz Client Secret should be stored in Secrets. These secret names must be specified in the deployment page. `AUTHZ_CLIENT_ID_IN_SECRET_STORE` and `AUTHZ_CLIENT_SECRET_IN_SECRET_STORE` are the secret names for the client ID and client secret, respectively. Please make sure you are using correct client-id and client-secret for your Authz environment. \\n\\n **3. URL Endpoint - Destination Topic - Data Format mapping string**: This establishes multiple endpoints for sending data and maps destination topics and data formats using a colon-separated format. You need to decide what to name your endpoint which you will use in your application to send data. The topic needs to be created in Topics section. Multiple mappings should be separated by commas. This could be set using the `OUTPUT_TOPIC_FORMATS` environment variable and it should be in the format of `\\u003cendpoint1\\u003e:\\u003ctopic1\\u003e:\\u003cformat1\\u003e`. The format can be `json`, `avro`, `protobuf`, `byte`, or `legacy`. JSON, Avro, and Protobuf correspond to the respective data formats (We support avro-byte encoding and we use Confluent standard for Avro \\u0026 Protobuf serialization). Byte is for raw byte data. Legacy format is for the old data format which adds a metadata wrapper with client ID around JSON data. Example - 'publish-json:scratch.json-topic.tenant_name:json'. \\n\\n **4. Dead Letter Queue**: Topic for storing failed messages during serialization/deserialization and can be configured using `DEAD_LETTER_TOPIC`. \\n\\n **5. Scaling**: Use `INSTANCES` count for horizontal scaling and `CPU` \\u0026 `MEMORY` values for vertical scaling. \\n\\n ## More Information \\n The Unibox Team can be reached at unibox@kpn.com for more information.\",\"contact\":\"unibox@kpn.com\",\"configuration\":{\"$schema\":\"https://json-schema.org/draft/2019-09/schema\",\"type\":\"object\",\"properties\":{\"AUTHZ_CLIENT_ID_IN_SECRET_STORE\":{\"description\":\"Name of the field in the DSH secrets store that contains the Authz Client ID.\",\"type\":\"string\"},\"AUTHZ_CLIENT_SECRET_IN_SECRET_STORE\":{\"description\":\"Name of the field in the DSH secrets store that contains the Authz Client Secret.\",\"type\":\"string\"},\"AUTHZ_ENVIRONMENT\":{\"description\":\"The environment value for Authz Digital Engine.\",\"type\":\"string\",\"enum\":[\"ACC\",\"PROD\"],\"default\":\"ACC\"},\"CPU\":{\"description\":\"Amount of CPU cores per instance.\",\"type\":\"string\",\"default\":\"0.1\"},\"DEAD_LETTER_TOPIC\":{\"description\":\"Topic for failed messages during serialization/deseralization\",\"type\":\"string\",\"default\":\"scratch.dlq.tenant\"},\"INSTANCES\":{\"description\":\"The number of instances to deploy for scaling. The default is 1.\",\"type\":\"string\",\"enum\":[\"1\",\"2\",\"3\",\"4\",\"5\"],\"default\":\"1\"},\"LOG_LEVEL\":{\"description\":\"The log level for the application. The default is INFO.\",\"type\":\"string\",\"enum\":[\"TRACE\",\"DEBUG\",\"INFO\",\"WARN\",\"ERROR\"],\"default\":\"INFO\"},\"MEMORY\":{\"description\":\"Amount of memory per instance.\",\"type\":\"string\",\"default\":\"64\"},\"OUTPUT_TOPIC_FORMATS\":{\"description\":\"Define endpoint, topic and data format mapping to declare where to publish data to. Mapping format is like following, endpoint:topic_name:data_format can be added more sepearting with comma.\",\"type\":\"string\",\"default\":\"endpoint1:scratch.topic1.tenant:byte\"},\"VHOST_DNS_ZONE\":{\"description\":\"DNS Zone selection for Vhost (kpn.com or kpn.org)\",\"type\":\"string\",\"enum\":[\"public\",\"private\"],\"default\":\"private\"}}},\"resources\":{\"allocation/${@tenant}/application/${@name}\":{\"cpus\":\"${CPU | number}\",\"env\":{\"AUTHZ_ENVIRONMENT\":\"${AUTHZ_ENVIRONMENT}\",\"DEAD_LETTER_TOPIC\":\"${DEAD_LETTER_TOPIC}\",\"ENABLE_OAUTH2\":\"true\",\"LOG_LEVEL\":\"${LOG_LEVEL}\",\"OUTPUT_TOPIC_FORMATS\":\"${OUTPUT_TOPIC_FORMATS}\"},\"exposedPorts\":{\"3000\":{\"vhost\":\"{ vhost('${@name}.${@tenant}', '${VHOST_DNS_ZONE}') }\"}},\"image\":\"${@appcatalog}/release/kpn/http-kafka-connector:0.5.0\",\"imageConsole\":\"registry.cp.kpn-dsh.com/greenbox-training/http-kafka-connector:0.5.0\",\"instances\":\"${INSTANCES | number}\",\"mem\":\"${MEMORY | number}\",\"metrics\":{\"path\":\"/metrics\",\"port\":9090},\"name\":\"${@name}\",\"needsToken\":true,\"secrets\":[{\"injections\":[{\"env\":\"AUTHZ_CLIENT_ID\"}],\"name\":\"${AUTHZ_CLIENT_ID_IN_SECRET_STORE}\"},{\"injections\":[{\"env\":\"AUTHZ_CLIENT_SECRET\"}],\"name\":\"${AUTHZ_CLIENT_SECRET_IN_SECRET_STORE}\"}],\"singleInstance\":false,\"user\":\"${@uid}:${@gid}\"},\"allocation/${@tenant}/vhost/${@name}.${@tenant}@${VHOST_DNS_ZONE}\":\"${@name}.${@tenant}@${VHOST_DNS_ZONE}\"}}"
  },
  {
    "draft": false,
    "lastModified": 1748273901268.0,
    "payload": "{\"id\":\"kpn/prometheus-scraper\",\"name\":\"Prometheus Scraper\",\"version\":\"0.1.5\",\"vendor\":\"KPN\",\"kind\":\"manifest\",\"apiVersion\":\"v0-alpha\",\"description\":\"Application to scrape Prometheus metrics to Kafka in DSH\",\"moreInfo\":\"## Prometheus Scraper to Kafka \\n\\n  \\n\\n **1. Prometheus URL**: Prometheus endpoint to scrape metrics from. If you do not change the default value `tenant-prometheus-server`, it will be automatically assigned to your tenant's Prometheus server. \\n\\n **2. Kafka Topic**: Kafka topic name for producing metrics to. Can be configured using `KAFKA_TOPIC`. \\n\\n **3. Scraper Interval Seconds**: Interval in seconds at which metrics are scraped. The default interval is 15 secs. Too frequent intervals might cause issues on Prometheus server. So, less than 15 seconds is not allowed and will be reaasigned to 15 secs as minimal interval seconds. Set the interval using `SCRAPE_INTERVAL_SECS`. \\n\\n **4. Prometheus Queries**: Comma-separated Prometheus queries. Default value: 'up,node_cpu_seconds_total'. Can be configured using `PROMETHEUS_QUERIES` variable. \\n\\n **5. Max Retries**: Maxiumum number of retries in the case of failure of scraping metrics. Set to 0 if you do not want to retry. \\n\\n **6. Scaling**: Use `CPU` \\u0026 `MEMORY` values for vertical scaling. \\n\\n ## More Information \\n The Unibox Team can be reached at unibox@kpn.com for more information.\",\"contact\":\"unibox@kpn.com\",\"configuration\":{\"$schema\":\"https://json-schema.org/draft/2019-09/schema\",\"type\":\"object\",\"properties\":{\"CPU\":{\"description\":\"Amount of CPU cores per instance.\",\"type\":\"string\",\"default\":\"0.1\"},\"KAFKA_TOPIC\":{\"description\":\"Kafka Topic name for producing metrics to.\",\"type\":\"string\",\"default\":\"scratch.scraper-metrics.connectors\"},\"LOG_LEVEL\":{\"description\":\"The log level for the application. The default is INFO.\",\"type\":\"string\",\"enum\":[\"TRACE\",\"DEBUG\",\"INFO\",\"WARN\",\"ERROR\"],\"default\":\"INFO\"},\"MAX_RETRIES\":{\"description\":\"Maximum number of retries in the case of failure of scraping metrics. Set to 0 if you do not want to retry.\",\"type\":\"string\",\"enum\":[\"0\",\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\"],\"default\":\"1\"},\"MEMORY\":{\"description\":\"Amount of memory per instance.\",\"type\":\"string\",\"default\":\"16\"},\"PROMETHEUS_ENDPOINT\":{\"description\":\"Prometheus endpoint URL. If you do not change the default value, it will be automatically assigned to your tenant Prometheus server.\",\"type\":\"string\",\"default\":\"tenant-prometheus-server\"},\"PROMETHEUS_QUERIES\":{\"description\":\"Comma-separated Prometheus queries.\",\"type\":\"string\",\"default\":\"up,node_cpu_seconds_total\"},\"SCRAPE_INTERVAL_SECS\":{\"description\":\"Interval in seconds at which metrics are scraped. The default interval is 15 secs. Too frequent intervals might cause issues on Prometheus server. So, less than 15 seconds is not allowed and will be reaasigned to 15 secs (min interval).\",\"type\":\"string\",\"default\":\"15\"}}},\"resources\":{\"allocation/${@tenant}/application/${@name}\":{\"cpus\":\"${CPU | number}\",\"env\":{\"KAFKA_TOPIC\":\"${KAFKA_TOPIC}\",\"MAX_RETRIES\":\"${MAX_RETRIES}\",\"PROMETHEUS_ENDPOINT\":\"${PROMETHEUS_ENDPOINT}\",\"PROMETHEUS_QUERIES\":\" ${PROMETHEUS_QUERIES}\",\"RUST_LOG\":\"${LOG_LEVEL}\",\"SCRAPE_INTERVAL_SECS\":\"${SCRAPE_INTERVAL_SECS}\"},\"image\":\"${@appcatalog}/release/kpn/prometheus-scraper:0.1.5\",\"imageConsole\":\"registry.cp.kpn-dsh.com/connectors/prometheus-scraper:0.1.5\",\"instances\":1,\"mem\":\"${MEMORY | number}\",\"metrics\":{\"path\":\"/metrics\",\"port\":8080},\"name\":\"${@name}\",\"needsToken\":true,\"singleInstance\":false,\"user\":\"${@uid}:${@gid}\"}}}"
  },
  {
    "draft": false,
    "lastModified": 1701347612011.0,
    "payload": "{\"id\":\"kpn/keyring-service\",\"name\":\"Keyring-Service\",\"version\":\"0.5.1\",\"vendor\":\"KPN\",\"kind\":\"manifest\",\"apiVersion\":\"v0-alpha\",\"description\":\"KPN keyring service\",\"moreInfo\":\"# keyring service\\n\\nThe **keyring service** is a service that exposes a REST-service that maps identifiers from a *domain* to a *codomain*. E.g. it can map Boss identifiers to KRN identifiers.\\n\\n* The **keyring service** runs in your own tenant environment, under your own control.\\n\\n* It optionally provides an OpenAPI specification of the REST-service to import into Postman or a similar application.\\n\\n* It optionally provides a Swagger UI to explore and test the REST-service.\\n\\n* It can expose metrics for a Prometheus scraper.\\n\\nBe sure to check the [keyring documentation](https://keyring.dsh-prod.dsh.prod.aws.kpn.org/using/app-catalog.html) for more and important information.\\n\",\"contact\":\"greenbox@kpn.com\",\"configuration\":{\"$schema\":\"https://json-schema.org/draft/2019-09/schema\",\"type\":\"object\",\"properties\":{\"CODOMAINS\":{\"description\":\"Select the codomain(s) for the relation separated by commas (allowed codomain names: BOSS, ECID, ETKN, GPID, KIDH, KRN, SBCM)\",\"type\":\"string\",\"enum\":[\"BOSS\",\"KRN\",\"SBCM\",\"BOSS,KRN,SBCM\",\"ECID\",\"ETKN\",\"GPID\",\"KIDH\",\"ECID,ETKN,GPID,KIDH\",\"BOSS,ECID,ETKN,GPID,KIDH,KRN,SBCM\"]},\"DOMAINS\":{\"description\":\"Select the domain(s) for the relation separated by commas (allowed domain names: BOSS, ECID, ETKN, GPID, KIDH, KRN, SBCM)\",\"type\":\"string\",\"enum\":[\"BOSS\",\"KRN\",\"SBCM\",\"BOSS,KRN,SBCM\",\"ECID\",\"ETKN\",\"GPID\",\"KIDH\",\"ECID,ETKN,GPID,KIDH\",\"BOSS,ECID,ETKN,GPID,KIDH,KRN,SBCM\"]},\"ENABLE_METRICS_EXPORTER\":{\"description\":\"Select whether to enable exporting metrics\",\"type\":\"string\",\"enum\":[\"false\",\"true\"],\"default\":\"true\"},\"ENABLE_OPEN_API_SPECIFICATION\":{\"description\":\"Select whether to enable the openapi specification\",\"type\":\"string\",\"enum\":[\"false\",\"true\"],\"default\":\"true\"},\"ENABLE_SWAGGER_UI\":{\"description\":\"Select whether to enable the swagger ui\",\"type\":\"string\",\"enum\":[\"false\",\"true\"],\"default\":\"true\"}}},\"resources\":{\"allocation/${@tenant}/application/${@name}\":{\"cpus\":1,\"env\":{\"BASE_DOMAINS_CONFIG_FILE\":\"base-domains.conf\",\"CODOMAINS\":\"${CODOMAINS}\",\"DEFAULT_CACHE_TYPE\":\"chronicle\",\"DOMAINS\":\"${DOMAINS}\",\"ENABLE_DERIVED_RELATIONS\":\"true\",\"ENABLE_JVM_METRICS_EXPORTER\":\"${ENABLE_METRICS_EXPORTER}\",\"ENABLE_METRICS_EXPORTER\":\"${ENABLE_METRICS_EXPORTER}\",\"ENABLE_OPEN_API_SPECIFICATION\":\"${ENABLE_OPEN_API_SPECIFICATION}\",\"ENABLE_SWAGGER_UI\":\"${ENABLE_SWAGGER_UI}\",\"GROUP_ID_PREFIX\":\"${@tenant}_\",\"HEAP_MEMORY\":\"384\",\"KEYRING_SERVICE_PORT\":\"8088\",\"LOG_LEVEL\":\"info\",\"LOG_LEVEL_CACHE\":\"info\",\"LOG_LEVEL_CODOMAIN_VALUES\":\"info\",\"LOG_LEVEL_ENGINE\":\"info\",\"LOG_LEVEL_ENTRYPOINT\":\"error\",\"LOG_LEVEL_MASTER_RELATION\":\"info\",\"LOG_LEVEL_NETTY\":\"error\",\"LOG_LEVEL_SERVICE\":\"info\",\"MASTER_RELATIONS_CONFIG_FILE\":\"master-relations.conf\",\"METRICS_EXPORTER_PREFIX\":\"\",\"SWAGGER_UI_CONTACT_NAME\":\"Greenbox Team\",\"SWAGGER_UI_CONTACT_URL\":\"https://confluence.kpn.org/display/KEYR/Keyring\",\"SWAGGER_UI_DOCUMENTATION_URL\":\"https://keyring.dsh-prod.dsh.prod.aws.kpn.org\"},\"exposedPorts\":{\"8088\":{\"auth\":\"system-fwd-auth@view,manage\",\"tls\":\"auto\",\"vhost\":\"{ vhost('${@name}.${@tenant}', 'public') }\"}},\"image\":\"${@appstore}/release/kpn/keyring-service:0.5.0\",\"imageConsole\":\"registry.cp.kpn-dsh.com/greenbox-dev/keyring-service:0.5.0\",\"instances\":1,\"mem\":2048,\"metrics\":{\"path\":\"/\",\"port\":9585},\"name\":\"${@name}\",\"needsToken\":true,\"singleInstance\":false,\"user\":\"${@uid}:${@gid}\"},\"allocation/${@tenant}/vhost/${@name}.${@tenant}@public\":\"${@name}.${@tenant}@public\"}}"
  },
  {
    "draft": false,
    "lastModified": 1752588206915.0,
    "payload": "{\"id\":\"kpn/airflow-persistent\",\"name\":\"Airflow persistent\",\"version\":\"3.0.2\",\"vendor\":\"KPN\",\"kind\":\"manifest\",\"apiVersion\":\"v0-alpha\",\"description\":\"Airflow stack with a persistent database to run your pipelines.\",\"moreInfo\":\"## Airflow persistent\\n\\nThe airflow persistent instance provides an environment to run Airflow DAGs.\\nThe python code (DAGs) is pulled in using a gitsync.\\nAll state of the airflow instance is stored in a separate postgres database and the logs are stored in an S3 bucket.\\n\\nVersion: 3.0.2\\n\\n- Airflow:  3.0.2\\n- Python:   3.12\\n- Gitsync:  4.4.0\\n- Postgres: 16\\n- Statsd exporter: 0.28.0\\n\\n### Before you start\\n\\nAdd the following secrets to the DSH secret store:\\n\\n1. airflow admin user password (create a password that you will use for the airflow ui)\\n2. git sync password (can also be a personal access token)\\n3. aws access key id for remote logging (might also use the system/objectstore/access_key_id)\\n4. aws secret key for remote logging (might also use the system/objectstore/secret_access_key)\\n5. postgres airflow user password\\n\\nThese secrets will be used for the container configuration.\\n\\n### Airflow\\n\\nThe airflow app will persist all logs and state. The logs are stored in an S3 bucket and the state is stored in a postgres database.\\n\\nTo login to the airflow instance, the default user is `admin` the password is provided by the tenant using the `TENANT_AIRFLOW_ADMIN_USER_PASSWORD_FIELD_IN_SECRET_STORE` environment variable. Which takes the secret you have created earlier from the DSH secrets store and injects it into the container.\\n\\n#### DAGs\\n\\nThe core concept of airflow is a DAG (Directed Acyclic Graph), collecting Tasks together, organized with dependencies and relationships to say how they should run. [airflow dags](https://airflow.apache.org/docs/apache-airflow/stable/core-concepts/dags.html)\\n\\nAll of the code specified in airflow is being wrapped around in tasks and orchestrated by the DAGs. To understand how they work, have a look at the following [Fundamental concepts](https://airflow.apache.org/docs/apache-airflow/stable/tutorial/fundamentals.html). The page also provides a turorial with example DAGs that you can try out.\\n\\n#### Logging\\n\\nAll logs are stored in an S3 bucket, to make sure that all logs are persisted. Airflow is able to show these logs in the Airflow UI. It is also possible to retrieve these logs from the S3 bucket without Airflow, by usinge an s3 client.\\n\\nFor the configuration of the remote logging make sure to set the following variables:\\n\\n- DSH_LOGGING_REMOTE_CONN_ID: connection id used by airflow to connect and configure remote logging.\\n- DSH_LOGGING_REMOTE_BASE_LOG_FOLDER: S3 address to write the logs to.\\n- DSH_LOGGING_AWS_REGION: region where the bucket is located.\\n- DSH_LOGGING_AWS_ACCESS_KEY_ID_FIELD_IN_SECRET_STORE: field in the secret store that contains the aws access key id to connect to the S3 bucket.\\n- DSH_LOGGING_AWS_SECRET_KEY_FIELD_IN_SECRET_STORE: field in the secret store that contains the aws secret key to connect to the S3 bucket.\\n- DSH_LOGGING_REMOTE_ENCRYPT_S3_LOGS: encrypt the airflow logs in the S3 bucket.\\n\\n#### Importing packages\\n\\nAt the moment the airflow setup from the app catalog is limited to the pip dependencies that came pre-installed with it. In the future we will provide the option for tenants to install packages to the airflow app. For now you can make use of the [PythonVirtualenvOperator](https://airflow.apache.org/docs/apache-airflow/stable/howto/operator/python.html#pythonvirtualenvoperator)\\n\\nWith the PythonVirtualenvOperator you can pull-in any public package.\\n\\n**Caution: Tenants are responsible for the python packages they import into their airflow container. Please follow the KPN Security Policy ([KSP](https://ciso-ksp.kpnnet.org/welcome/KSP)) as a guideline.**\\n\\n### Gitsync\\n\\nThe gitsync will pull in a git branch and put the code in the specified directory. For the gitsync configuration see all variables with the prefix `GIT_SYNC`. You can put all of your DAGs and python code in this git repository, here the python project structure applies.\\n\\nFor the configuration, make sure to set the following variables to the right values:\\n\\n- GIT_SYNC_DEST: the destination on the container where you would like to have the dags pushed to.  e.g. `/opt/airflow/dags/dex`\\n- GITSYNC_REPO: url of the git repo to sync.\\n- GITSYNC_REF: git branch to pull in.\\n- GITSYNC_PERIOD: number of seconds between syncs.\\n- GITSYNC_ROOT: where on the root should you do the git operations. the default is sufficient.\\n- GITSYNC_ONE_TIME: specifies if the gitsync should continuously pull or exit after the first pull. Default is false.\\n- GITSYNC_USERNAME: username of the user to pull in the git repo.\\n- GITSYNC_PASSWORD_FIELD_IN_SECRET_STORE: the field in the secret store that contains the password of your git account.\\n\\nFor some more background have a look at the [gitsync](https://github.com/kubernetes/git-sync/tree/v3.6.5?tab=readme-ov-file#primary-flags) repo.\\n\\n### Database\\n\\nFor all of the state we make use of a Postgres database. It is the Postgresql database 15.6.\\n\\nThe configuration for the database can be found in the following environment variables:\\n\\n- POSTGRESQL_DATABASE The database to use for airflow\\n- POSTGRESQL_USERNAME The username of the airflow user.\\n- POSTGRESQL_PASSWORD_FIELD_IN_SECRET_STORE the field in the secret store that contains the password for the default postgres user.\\n\\n### More Information\\n\\nThe _DEX Team_ can be reached at **\\u003cdex@kpn.com\\u003e** for more information.\",\"contact\":\"dex@kpn.com\",\"configuration\":{\"$schema\":\"https://json-schema.org/draft/2019-09/schema\",\"type\":\"object\",\"properties\":{\"AIRFLOW_CPU\":{\"description\":\"Amount of CPU cores per instance.\",\"type\":\"string\",\"default\":\"1\"},\"AIRFLOW_MEMORY\":{\"description\":\"Amount of memory per instance.\",\"type\":\"string\",\"default\":\"4096\"},\"DSH_LOGGING_AWS_ACCESS_KEY_ID_FIELD_IN_SECRET_STORE\":{\"description\":\"Field in the secret store that contains the aws access key id to connect to the S3 bucket.\",\"type\":\"string\",\"default\":\"system/objectstore/access_key_id\"},\"DSH_LOGGING_AWS_REGION\":{\"description\":\"region where the bucket is located.\",\"type\":\"string\",\"default\":\"eu-west-1\"},\"DSH_LOGGING_AWS_SECRET_KEY_FIELD_IN_SECRET_STORE\":{\"description\":\"Field in the secret store that contains the aws secret key to connect to the S3 bucket.\",\"type\":\"string\",\"default\":\"system/objectstore/secret_access_key\"},\"DSH_LOGGING_REMOTE_BASE_LOG_FOLDER\":{\"description\":\"S3 address to write the logs to.\",\"type\":\"string\",\"default\":\"s3://BUCKET_NAME/airflow-persistent/logs\"},\"DSH_LOGGING_REMOTE_ENCRYPT_S3_LOGS\":{\"description\":\"encrypt the airflow logs in the S3 bucket.\",\"type\":\"string\",\"enum\":[\"true\",\"false\"],\"default\":\"false\"},\"GITSYNC_LINK\":{\"description\":\"Folder destination on the host machine where to pull git repo towards.\",\"type\":\"string\",\"default\":\"/opt/airflow/dags/repo_name\"},\"GITSYNC_ONE_TIME\":{\"description\":\"specify to only pull on startup or continuously, if true the gitsync exits after the first sync.\",\"type\":\"string\",\"enum\":[\"true\",\"false\"],\"default\":\"false\"},\"GITSYNC_PASSWORD_FIELD_IN_SECRET_STORE\":{\"description\":\"Name of the field in the DSH secrets store that contains the Git Sync password.\",\"type\":\"string\"},\"GITSYNC_PERIOD\":{\"description\":\"the number of seconds between syncs.\",\"type\":\"string\",\"default\":\"60s\"},\"GITSYNC_REF\":{\"description\":\"git branch to pull in.\",\"type\":\"string\",\"default\":\"main\"},\"GITSYNC_REPO\":{\"description\":\"url of the git repo to sync.\",\"type\":\"string\",\"default\":\"\"},\"GITSYNC_USERNAME\":{\"description\":\"username of the user to pull in the git repo. In github the PAT indicates the username, so username will be oauth2.\",\"type\":\"string\",\"default\":\"oauth2\"},\"POSTGRESQL_DATABASE\":{\"description\":\"name of the database to be used in postgresql for airflow\",\"type\":\"string\",\"default\":\"airflow\"},\"POSTGRESQL_PASSWORD_FIELD_IN_SECRET_STORE\":{\"description\":\"the field in the secret store that contains the password for the default postgres user.\",\"type\":\"string\"},\"POSTGRESQL_USERNAME\":{\"description\":\"The username of the airflow user.\",\"type\":\"string\",\"default\":\"airflow\"},\"POSTGRES_CPU\":{\"description\":\"Amount of CPU cores per instance. For the postgres database.\",\"type\":\"string\",\"default\":\"1\"},\"POSTGRES_MEMORY\":{\"description\":\"Amount of memory per instance. For the postgres database.\",\"type\":\"string\",\"default\":\"2048\"},\"POSTGRES_VOLUME_SIZE\":{\"description\":\"The amount of disk space in GB, for the postgres database.\",\"type\":\"string\",\"default\":\"5\"},\"TENANT_AIRFLOW_ADMIN_USER_PASSWORD_FIELD_IN_SECRET_STORE\":{\"description\":\"Name of the field in the DSH secrets store that contains the airflow admin user password.\",\"type\":\"string\"}}},\"resources\":{\"allocation/${@tenant}/application/${@name}\":{\"cpus\":\"${AIRFLOW_CPU | number}\",\"env\":{\"AIRFLOW__CORE__LOAD_EXAMPLES\":\"false\",\"AIRFLOW__SCHEDULER__SCHEDULER_ZOMBIE_TASK_THRESHOLD\":\"600\",\"APPLICATION_NAME\":\"${@name}\",\"DSH_GIT_SYNC_ENABLED\":\"true\",\"DSH_LOGGING_AWS_REGION\":\"${DSH_LOGGING_AWS_REGION}\",\"DSH_LOGGING_REMOTE_BASE_LOG_FOLDER\":\"${DSH_LOGGING_REMOTE_BASE_LOG_FOLDER}\",\"DSH_LOGGING_REMOTE_CONN_ID\":\"s3_logging_conn\",\"DSH_LOGGING_REMOTE_ENCRYPT_S3_LOGS\":\"${DSH_LOGGING_REMOTE_ENCRYPT_S3_LOGS}\",\"GITSYNC_LINK\":\"${GITSYNC_LINK}\",\"GITSYNC_ONE_TIME\":\"${GITSYNC_ONE_TIME}\",\"GITSYNC_PASSWORD_FILE\":\"/home/dsh/git-pat-sdp\",\"GITSYNC_PERIOD\":\"${GITSYNC_PERIOD}\",\"GITSYNC_REF\":\"${GITSYNC_REF}\",\"GITSYNC_REPO\":\"${GITSYNC_REPO}\",\"GITSYNC_ROOT\":\"/tmp/git\",\"GITSYNC_USERNAME\":\"${GITSYNC_USERNAME}\",\"POSTGRESQL_DATABASE\":\"${POSTGRESQL_DATABASE}\",\"POSTGRESQL_HOSTNAME\":\"${@name}-postgres\",\"POSTGRESQL_USERNAME\":\"${POSTGRESQL_USERNAME}\"},\"exposedPorts\":{\"8080\":{\"auth\":\"system-fwd-auth@view,manage\",\"vhost\":\"{ vhost('${@name}.${@tenant}','private') }\"}},\"image\":\"${@appcatalog}/release/kpn/airflow-persistent:3.0.2\",\"imageConsole\":\"registry.cp.kpn-dsh.com/dex-dev/airflow-persistent:3.0.2\",\"instances\":1,\"mem\":\"${AIRFLOW_MEMORY | number}\",\"name\":\"${@name}\",\"needsToken\":true,\"secrets\":[{\"injections\":[{\"env\":\"GH_PAT_TOKEN\"}],\"name\":\"${GITSYNC_PASSWORD_FIELD_IN_SECRET_STORE}\"},{\"injections\":[{\"env\":\"TENANT_AIRFLOW_ADMIN_USER_PASSWORD\"}],\"name\":\"${TENANT_AIRFLOW_ADMIN_USER_PASSWORD_FIELD_IN_SECRET_STORE}\"},{\"injections\":[{\"env\":\"DSH_LOGGING_AWS_ACCESS_KEY_ID\"}],\"name\":\"${DSH_LOGGING_AWS_ACCESS_KEY_ID_FIELD_IN_SECRET_STORE}\"},{\"injections\":[{\"env\":\"DSH_LOGGING_AWS_SECRET_KEY\"}],\"name\":\"${DSH_LOGGING_AWS_SECRET_KEY_FIELD_IN_SECRET_STORE}\"},{\"injections\":[{\"env\":\"POSTGRESQL_PASSWORD\"}],\"name\":\"${POSTGRESQL_PASSWORD_FIELD_IN_SECRET_STORE}\"}],\"singleInstance\":true,\"user\":\"${@uid}:${@gid}\"},\"allocation/${@tenant}/application/${@name}-postgres\":{\"cpus\":\"${POSTGRES_CPU | number}\",\"env\":{\"POSTGRESQL_DATABASE\":\"${POSTGRESQL_DATABASE}\",\"POSTGRESQL_USERNAME\":\"${POSTGRESQL_USERNAME}\"},\"image\":\"${@appcatalog}/release/kpn/postgresql:16\",\"imageConsole\":\"registry.cp.kpn-dsh.com/dex-dev/bitnami/postgresql:16\",\"instances\":1,\"mem\":\"${POSTGRES_MEMORY | number}\",\"name\":\"${@name}-postgres\",\"needsToken\":true,\"secrets\":[{\"injections\":[{\"env\":\"POSTGRESQL_PASSWORD\"}],\"name\":\"${POSTGRESQL_PASSWORD_FIELD_IN_SECRET_STORE}\"}],\"singleInstance\":true,\"user\":\"${@uid}:${@gid}\",\"volumes\":{\"/bitnami/postgresql/\":{\"name\":\"{ volume('${@name}-postgres')}\"}}},\"allocation/${@tenant}/vhost/${@name}.${@tenant}@private\":\"${@name}.${@tenant}@private\",\"allocation/${@tenant}/volume/${@name}-postgres\":{\"name\":\"${@name}-postgres\",\"size\":\"${POSTGRES_VOLUME_SIZE | number}\"}}}"
  },
  {
    "draft": false,
    "lastModified": 1758727825402.0,
    "payload": "{\"id\":\"kpn/dsh-labs-kernel\",\"name\":\"dsh-labs-kernel\",\"version\":\"0.0.3-python3-stateless\",\"vendor\":\"KPN\",\"kind\":\"manifest\",\"apiVersion\":\"v0-alpha\",\"description\":\"Kernels for DSH Labs\",\"moreInfo\":\"# This APP is managed by DSH labs and should not be deployed manually\\n\\n\\u003cKERNEL_SPECS\\u003e {\\\"name\\\": \\\"python3\\\", \\\"language\\\": \\\"python\\\", \\\"display_name\\\": \\\"Python 3 (ephemeral)\\\", \\\"description\\\": \\\"Python 3 kernel (ephemeral).\\\\nThis kernel is great for exploring and some small analysis which does not use a lot of resources.\\\\n\\\\nNOTE: This kernel has limited space (1gb) to install packages and save files to disk. If the container is restarted it will lose all state\\\", \\\"argv\\\": [],  \\\"env\\\": {\\\"KEY\\\": \\\"value\\\"}, \\\"help_links\\\": [] } \\u003c/KERNEL_SPECS\\u003e\",\"contact\":\"unibox@kpn.com\",\"configuration\":{\"$schema\":\"https://json-schema.org/draft/2019-09/schema\",\"type\":\"object\",\"properties\":{\"BACKEND_SERVER_DNS_NAME\":{\"description\":\"DNS Name of backend server name\",\"type\":\"string\"},\"CONNECTION_LIMIT\":{\"description\":\"Total amount of notebooks that are allowed on this node. Multiple kernels can be deployed on this node (allows multiple notebooks to run on this node to save resources)\\n\\nNOTE: SECRETS and VARIABLES are always part of a kernel and are not accessible by other kernels. Saved files to disk are accessible by all kernels on the node.\",\"type\":\"number\",\"enum\":[\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\"],\"default\":\"5\"},\"CPU\":{\"description\":\"CPU limit for the kernel container\",\"type\":\"number\",\"default\":\"0.25\"},\"LOG_LEVEL\":{\"description\":\"Log level for kernel handler, which is responsible for handling the communication between server and kernel container\",\"type\":\"string\",\"enum\":[\"error\",\"warn\",\"info\",\"debug\",\"trace\"],\"default\":\"info\"},\"MEMORY\":{\"description\":\"Memory limit for the kernel container\",\"type\":\"number\",\"default\":\"1024\"}}},\"resources\":{\"allocation/${@tenant}/application/${@name}\":{\"cpus\":\"${CPU | number}\",\"env\":{\"BACKEND_SERVER_DNS_NAME\":\"${BACKEND_SERVER_DNS_NAME}\",\"CONNECTION_LIMIT\":\"${CONNECTION_LIMIT}\",\"DSH_ENVIRONMENT\":\"{ variables('DSH_ENVIRONMENT') }\",\"RUST_LOG\":\"${LOG_LEVEL}\"},\"image\":\"${@appcatalog}/release/kpn/dsh-labs-kernel-python3:0.0.3\",\"imageConsole\":\"registry.cp.kpn-dsh.com/dsh-studio/dsh-labs-kernel-python3:0.0.3\",\"instances\":1,\"mem\":\"${MEMORY | number}\",\"name\":\"${@name}\",\"needsToken\":true,\"secrets\":[{\"injections\":[{\"env\":\"REST_API_CLIENT_KEY\"}],\"name\":\"system/rest-api-client\"}],\"singleInstance\":true,\"user\":\"${@uid}:${@gid}\"}}}"
  },
  {
    "draft": false,
    "lastModified": 1747640570010.0,
    "payload": "{\"id\":\"kpn/oidc-fwd-auth\",\"name\":\"OIDC forward-auth\",\"version\":\"1.0.0\",\"vendor\":\"KPN\",\"kind\":\"manifest\",\"apiVersion\":\"v0-alpha\",\"description\":\"Use custom OIDC forward auth for your services.\",\"moreInfo\":\"# OIDC Forward-Auth\\n\\nUse OIDC forward authentication for your custom services on the DSH.\\n\\n## Description\\n\\nThis app is a custom deployment of [thomseddon/traefik-forward-auth](https://github.com/thomseddon/traefik-forward-auth).\\n\\nThe OIDC Forward-Auth app functions as authentication middleware between your custom service on the DSH, and your OpenID Connect (OIDC) provider:\\n\\n- An external client sends a request to your custom service.\\n- The custom service forwards the request to the OIDC Forward-Auth app, using the latter's endpoint in the service definition.\\n- The OIDC Forward-Auth app checks the request with the OIDC provider.\\n- The OIDC Forward-Auth app forwards the response from the OIDC provider to your custom service.\\n\\nSee [Vhost authentication](https://docs.kpn-dsh.com/kpn-dsh-user-doc-console/custom-services-management/the-service-definition/#vhost-authentication) in the DSH's manual for more information.\\n\\nFollow the steps below to set up forward authentication with the OIDC Forward-Auth app.\\n\\n## Set up OIDC\\n\\nYou can choose from many OIDC providers: KPN's Grip, Microsoft, Google, Keycloak, etc. In order to deploy the OIDC Forward-Auth app, you need the following information from your OIDC provider:\\n\\n- The URL for your client at the OIDC provider\\n- The client ID\\n- The client secret\\n\\n## Configure the OIDC Forward-Auth app\\n\\nBefore deploying the OIDC Forward-Auth app, you need to create 2 secrets on the DSH:\\n\\n- A DSH secret with the client secret that you obtained from your OIDC provider\\n- A DSH secret with a random key to encrypt the cookie with\\n\\nYou can then click \\\"Configure\\\" to set up the app. Fill out the fields below:\\n\\n- **App name**: Fill out a name for your app:\\n  - Only use lowercase letters (a–z), numbers (0–9), or hyphens (`-`).\\n  - The name can’t be longer than 45 characters.\\n  - The name can’t end in a hyphen (`-`) and should start with a lowercase letter.\\n- **Client ID**: The ID of the client at your OIDC provider\\n- **Client secret**: The name of the DSH secret that contains the secret for the client at your OIDC provider\\n- **Cookie secret**: The name of the DSH secret that contains the random key to encrypt the cookie with\\n- **Log level**: Select \\\"Info\\\" for informational logging, or \\\"Debug\\\" for detailed logging for debugging purposes\\n- **OIDC URL**: The URL for the client at your OIDC provider\\n- **UMA authorization**: Whether you want to use User-Managed Access (UMA) authorization\\n\\nClick \\\"Deploy\\\" to deploy the app.\\n\\n## Configure your custom service\\n\\nOnce the OIDC Forward-Auth app is up and running, you can configure one or more of your custom services to actually use it. You can manage this in the service definition of you custom service, see [Vhost authentication](https://docs.kpn-dsh.com/kpn-dsh-user-doc-console/custom-services-management/the-service-definition/#vhost-authentication) in the DSH's manual for more information.\\n\\nIn the service definition, you need to enter the following key and value if you use the OIDC Forward-Auth app:\\n\\n```JSON\\n\\\"auth\\\": \\\"fwd-auth@\\u003capp-name\\u003e@\\u003cheaders\\u003e\\\"`\\n```\\n\\n- `\\u003capp-name\\u003e`: Enter the name that you chose for the OIDC Forward-Auth app.\\n- `\\u003cheaders\\u003e`: A comma separated list of headers to forward to the custom service. The service can then use it to grant permissions based on the user's token.\\n  - `X-Forwarded-Id-Token`: Forward the ID token\\n  - `X-Forwarded-Access-Token`: Forward the access token\",\"contact\":\"sre@kpn-dsh.com\",\"configuration\":{\"$schema\":\"https://json-schema.org/draft/2019-09/schema\",\"type\":\"object\",\"properties\":{\"CLIENT_ID\":{\"description\":\"Enter the ID of the client at your OIDC provider.\",\"type\":\"string\"},\"CLIENT_SECRET\":{\"description\":\"OIDC client secret (name of the DSH secret that contains the value). Enter the name of the DSH secret that contains the client secret at your OIDC provider.\",\"type\":\"string\"},\"COOKIE_SECRET\":{\"description\":\"Random key to encrypt cookie with (name of the DSH secret that contains the value). Enter the name of the DSH secret that contains the random key to encrypt the cookie with.\",\"type\":\"string\"},\"LOG_LEVEL\":{\"description\":\"Choose between informational logging, or detailed logging for debugging purposes.\",\"type\":\"string\",\"enum\":[\"debug\",\"info\"],\"default\":\"info\"},\"OIDC_ISSUER\":{\"description\":\"OIDC issuer url. Enter the the URL for the client at your OIDC provider.\",\"type\":\"string\"},\"UMA_AUTHORIZATION\":{\"description\":\"Enable UMA authorization (fine grained authorization). Indicate whether you want to activate User-Managed Access (UMA) authorization.\",\"type\":\"string\",\"enum\":[\"true\",\"false\"],\"default\":\"false\"}}},\"resources\":{\"allocation/${@tenant}/application/${@name}\":{\"cpus\":0.1,\"env\":{\"CLIENT_ID\":\"${CLIENT_ID}\",\"COOKIE_NAME\":\"__Secure-${@tenant}_${@name}\",\"COOKIE_SECURE\":\"true\",\"CSRF_COOKIE_NAME\":\"__Secure-${@tenant}_${@name}_csrf\",\"INFO_COOKIE_NAME\":\"__Secure-${@tenant}_${@name}_info\",\"LOG_LEVEL\":\"${LOG_LEVEL}\",\"OIDC_ISSUER\":\"${OIDC_ISSUER}\",\"SECURE\":\"true\",\"UMA_AUTHORIZATION\":\"${UMA_AUTHORIZATION}\"},\"image\":\"${@appcatalog}/release/kpn/traefik-fwd-auth:3.1.0\",\"instances\":1,\"mem\":64,\"name\":\"${@name}\",\"needsToken\":false,\"secrets\":[{\"injections\":[{\"env\":\"CLIENT_SECRET\"}],\"name\":\"${CLIENT_SECRET}\"},{\"injections\":[{\"env\":\"SECRET\"}],\"name\":\"${COOKIE_SECRET}\"}],\"singleInstance\":false,\"user\":\"${@uid}:${@gid}\"}}}"
  },
  {
    "draft": false,
    "lastModified": 1697188398890.0,
    "payload": "{\"id\":\"kpn/cmdline\",\"name\":\"Cmd Line\",\"version\":\"1.1.3\",\"vendor\":\"KPN\",\"kind\":\"manifest\",\"apiVersion\":\"v0-alpha\",\"description\":\"A browser based terminal with kafka command line utilities\",\"moreInfo\":\"Access a command line terminal through Vhost. Includes the following tools:\\n  - openssl\\n  - curl\\n  - jq\\n  - kcl\\n  - dshkcl\\n  - nc\\n\\nkcl is an open source, 'one stop shop' to do anything with kafka. - https://github.com/twmb/kcl\\ndshkcl is a wrapper for kcl to deserialize consumed messages from dsh's custom envelope into json.\\njq is available for json data manipulation - https://stedolan.github.io/jq/\\nnetcat-openbsd \\\"the tcp/ip swiss army knife\\\" - https://man.openbsd.org/nc.1\",\"contact\":\"dsh-appcatalog@kpn.com\",\"configuration\":{\"$schema\":\"https://json-schema.org/draft/2019-09/schema\",\"type\":\"object\",\"properties\":{}},\"resources\":{\"allocation/${@tenant}/application/${@name}\":{\"cpus\":0.25,\"env\":{},\"exposedPorts\":{\"8080\":{\"auth\":\"system-fwd-auth@view,manage\",\"tls\":\"auto\",\"vhost\":\"{ vhost('${@name}.${@tenant}', 'public') }\"}},\"image\":\"${@appcatalog}/release/kpn/cmdline:1.1.3\",\"instances\":1,\"mem\":256,\"name\":\"${@name}\",\"needsToken\":true,\"singleInstance\":false,\"user\":\"${@uid}:${@gid}\"},\"allocation/${@tenant}/vhost/${@name}.${@tenant}@public\":\"${@name}.${@tenant}@public\"}}"
  },
  {
    "draft": false,
    "lastModified": 1697814445876.0,
    "payload": "{\"id\":\"kpn/eavesdropper\",\"name\":\"eavesdropper\",\"version\":\"0.8.0\",\"vendor\":\"KPN\",\"kind\":\"manifest\",\"apiVersion\":\"v0-alpha\",\"description\":\"Web application to visualise messages on Kafka streams in realtime.\",\"moreInfo\":\"## View data in realtime on Data Streams  \\nVia a visual interface you can easily subscribe to a data stream to follow the data in realtime. You can even filter on keys or values, to only see a selection of the data that you're interested in.  \\n## Built-in Web Authentication  \\nShares a SSO login with the Console, which allows you to login with the same credentials. Also, if you're logged in to the Console you'll also have access to the Eavesdropper.  \\nOnce deployed, the Eavesdropper will become available on a vhost of your choosing.  \\n## Built-in Kafka Authentication  \\nRuns in your tenant environment and automatically connects to Kafka as your tenant, meaning it has the same access rights as your other Kafka applications.\",\"contact\":\"wilbert.schelvis@kpn.com\",\"configuration\":{\"$schema\":\"https://json-schema.org/draft/2019-09/schema\",\"type\":\"object\",\"properties\":{\"LOG_LEVEL\":{\"description\":\"application log level\",\"type\":\"string\",\"enum\":[\"error\",\"warn\",\"info\"],\"default\":\"info\"}}},\"resources\":{\"allocation/${@tenant}/application/${@name}\":{\"cpus\":0.1,\"env\":{\"GROUP_ID_PREFIX\":\"${@tenant}_\",\"INSTANCE_IDENTIFIER\":\"${@tenant}_${@name}\",\"LOG_LEVEL\":\"${LOG_LEVEL}\",\"LOG_LEVEL_ENTRYPOINT\":\"${LOG_LEVEL}\",\"LOG_LEVEL_GREENBOX\":\"error\",\"LOG_LEVEL_KAFKA_CLIENT\":\"error\",\"LOG_LEVEL_MONITOR\":\"${LOG_LEVEL}\",\"LOG_LEVEL_RECDES\":\"info\",\"LOG_LEVEL_SERVICE\":\"${LOG_LEVEL}\"},\"exposedPorts\":{\"8081\":{\"auth\":\"system-fwd-auth@view,manage\",\"tls\":\"auto\",\"vhost\":\"{ vhost('${@name}.${@tenant}', 'public') }\"}},\"image\":\"${@appstore}/release/kpn/eavesdropper:0.8.0\",\"instances\":1,\"mem\":384,\"name\":\"${@name}\",\"needsToken\":true,\"singleInstance\":false,\"user\":\"${@uid}:${@gid}\"},\"allocation/${@tenant}/vhost/${@name}.${@tenant}@public\":\"${@name}.${@tenant}@public\"}}"
  },
  {
    "draft": false,
    "lastModified": 1701270200239.0,
    "payload": "{\"id\":\"kpn/aep-sink-connect\",\"name\":\"Aep-Sink-Connect\",\"version\":\"0.1.1\",\"vendor\":\"KPN\",\"kind\":\"manifest\",\"apiVersion\":\"v0-alpha\",\"description\":\"The application will read events from a stream and push it to the Adobe Experience Platform (AEP) to update customer profiles\",\"moreInfo\":\"The application needs a simple configuration file to set up the proper topic and data endpoint for AEP.\\n### More context\\n The Adobe Experience Platform contains customer profiles and can be used to personalize customer experience. The customer profiles can be updated real time by streaming customer events to the platform. The container we developed will take events from a stream in the DSH and push it to AEP.\\n### Use case\\n The Advanced Analytics department has built models which scores customer profiles and/or actions. An action of a customer can trigger a model to score that action and publish it on a kafka stream. The container will take the score from the stream and send it to Adobe. In the future more models will be developed. To prevent that analists need to rebuild this step to get data from a stream to AEP we want to turn it into a re-useable component.\",\"contact\":\"dsh-appcatalog@kpn.com\",\"configuration\":{\"$schema\":\"https://json-schema.org/draft/2019-09/schema\",\"type\":\"object\",\"properties\":{\"AEP_CLIENT_ID\":{\"description\":\"aep-client-id\",\"type\":\"string\",\"default\":\"\"},\"AEP_CLIENT_SECRET\":{\"description\":\"aep-client-secret\",\"type\":\"string\",\"default\":\"\"},\"AEP_PRIVATE_KEY\":{\"description\":\"aep-private-key\",\"type\":\"string\",\"default\":\"\"},\"AEP_TECHNICAL_ACCOUNT_ID\":{\"description\":\"aep-technical-account-id\",\"type\":\"string\",\"default\":\"\"},\"CONNECT_CONFIG_STORAGE_TOPIC\":{\"description\":\"scratch.kafka-connect-aep-config-storage.\\u003ctenant_name\\u003e\",\"type\":\"string\",\"default\":\"\"},\"CONNECT_OFFSET_STORAGE_TOPIC\":{\"description\":\"scratch.kafka-connect-aep-offset-storage.\\u003ctenant_name\\u003e\",\"type\":\"string\",\"default\":\"\"},\"CONNECT_REST_ADVERTISED_HOST_NAME\":{\"description\":\"\\u003cservice_name\\u003e.\\u003ctenant_name\\u003e.marathon.mesos\",\"type\":\"string\",\"default\":\"\"},\"CONNECT_STATUS_STORAGE_TOPIC\":{\"description\":\"scratch.kafka-connect-aep-status-storage.\\u003ctenant_name\\u003e\",\"type\":\"string\",\"default\":\"\"},\"ENV\":{\"description\":\"platform\",\"type\":\"string\",\"enum\":[\"np\",\"prod\"],\"default\":\"prod\"},\"VOL_KAFKA_CONNECT_JARS\":{\"description\":\"kafka-connect-jars\",\"type\":\"string\",\"default\":\"kafka-connect-jars\"},\"VOL_KAFKA_CONNECT_SECRETS\":{\"description\":\"kafka-connect-secrets\",\"type\":\"string\",\"default\":\"kafka-connect-secrets\"}}},\"resources\":{\"allocation/${@tenant}/application/${@name}\":{\"cpus\":1,\"env\":{\"AEP_IMS_ORG\":\"BCC6148954F6271F0A4C98BC@AdobeOrg\",\"CONNECT_CONFIG_STORAGE_TOPIC\":\"${CONNECT_CONFIG_STORAGE_TOPIC}\",\"CONNECT_KEY_CONVERTER\":\"org.apache.kafka.connect.storage.StringConverter\",\"CONNECT_KEY_IGNORE\":\"true\",\"CONNECT_LOG4J_LOGGERS\":\"org.reflections=ERROR,org.apache.zookeeper=ERROR,org.I0Itec.zkclient=ERROR,com.adobe.platform.streaming.sink=DEBUG\",\"CONNECT_OFFSET_STORAGE_TOPIC\":\"${CONNECT_OFFSET_STORAGE_TOPIC}\",\"CONNECT_REST_ADVERTISED_HOST_NAME\":\"${CONNECT_REST_ADVERTISED_HOST_NAME}\",\"CONNECT_REST_PORT\":\"8080\",\"CONNECT_SECURITY_PROTOCOL\":\"SSL\",\"CONNECT_STATUS_STORAGE_TOPIC\":\"${CONNECT_STATUS_STORAGE_TOPIC}\",\"CONNECT_VALUE_CONVERTER\":\"org.apache.kafka.connect.json.JsonConverter\",\"CONNECT_VALUE_CONVERTER.SCHEMAS.ENABLE\":\"false\",\"ENV\":\"${ENV}\",\"LOGGING_ENABLED\":\"false\",\"MANAGE_JOB_INTERVAL\":\"60\",\"PKI_CONFIG_DIR\":\"/home/appuser/config\"},\"image\":\"${@appcatalog}/release/kpn/aep-sink-connect:0.1.0\",\"instances\":1,\"mem\":2048,\"name\":\"${@name}\",\"needsToken\":true,\"secrets\":[{\"injections\":[{\"env\":\"AEP_CLIENT_ID\"}],\"name\":\"${AEP_CLIENT_ID}\"},{\"injections\":[{\"env\":\"AEP_CLIENT_SECRET\"}],\"name\":\"${AEP_CLIENT_SECRET}\"},{\"injections\":[{\"env\":\"AEP_PRIVATE_KEY\"}],\"name\":\"${AEP_PRIVATE_KEY}\"},{\"injections\":[{\"env\":\"AEP_TECHNICAL_ACCOUNT_ID\"}],\"name\":\"${AEP_TECHNICAL_ACCOUNT_ID}\"}],\"singleInstance\":false,\"user\":\"${@uid}:${@gid}\",\"volumes\":{\"/etc/kafka-connect/jars\":{\"name\":\"{ volume('${VOL_KAFKA_CONNECT_JARS}') }\"},\"/etc/kafka-connect/secrets\":{\"name\":\"{ volume('${VOL_KAFKA_CONNECT_SECRETS}') }\"}}},\"allocation/${@tenant}/vhost/${@name}.${@tenant}@public\":\"${@name}.${@tenant}@public\"}}"
  },
  {
    "draft": false,
    "lastModified": 1701269169594.0,
    "payload": "{\"id\":\"kpn/eavesdropper\",\"name\":\"Eavesdropper\",\"version\":\"0.8.1\",\"vendor\":\"KPN\",\"kind\":\"manifest\",\"apiVersion\":\"v0-alpha\",\"description\":\"Web application to visualise messages on Kafka streams in realtime\",\"moreInfo\":\"## View data in realtime on Data Streams  \\nVia a visual interface, you can easily subscribe to a data stream to follow the data in real time. You can even filter on keys or values only to see a selection of the data that you're interested in.  \\n## Built-in Web Authentication  \\nShares a SSO login with the Console, which allows you to log in with the same credentials. Also, if you're logged in to the Console, you'll also have access to the Eavesdropper.  \\nOnce deployed, the Eavesdropper will become available on a vhost of your choosing.  \\n## Built-in Kafka Authentication  \\nRuns in your tenant environment and automatically connects to Kafka as your tenant, meaning it has the same access rights as your other Kafka applications.\",\"contact\":\"wilbert.schelvis@kpn.com\",\"configuration\":{\"$schema\":\"https://json-schema.org/draft/2019-09/schema\",\"type\":\"object\",\"properties\":{\"LOG_LEVEL\":{\"description\":\"application log level\",\"type\":\"string\",\"enum\":[\"error\",\"warn\",\"info\"],\"default\":\"info\"}}},\"resources\":{\"allocation/${@tenant}/application/${@name}\":{\"cpus\":0.1,\"env\":{\"GROUP_ID_PREFIX\":\"${@tenant}_\",\"INSTANCE_IDENTIFIER\":\"${@tenant}_${@name}\",\"LOG_LEVEL\":\"${LOG_LEVEL}\",\"LOG_LEVEL_ENTRYPOINT\":\"${LOG_LEVEL}\",\"LOG_LEVEL_GREENBOX\":\"error\",\"LOG_LEVEL_KAFKA_CLIENT\":\"error\",\"LOG_LEVEL_MONITOR\":\"${LOG_LEVEL}\",\"LOG_LEVEL_RECDES\":\"info\",\"LOG_LEVEL_SERVICE\":\"${LOG_LEVEL}\"},\"exposedPorts\":{\"8081\":{\"auth\":\"system-fwd-auth@view,manage\",\"tls\":\"auto\",\"vhost\":\"{ vhost('${@name}.${@tenant}', 'public') }\"}},\"image\":\"${@appstore}/release/kpn/eavesdropper:0.8.0\",\"instances\":1,\"mem\":384,\"name\":\"${@name}\",\"needsToken\":true,\"singleInstance\":false,\"user\":\"${@uid}:${@gid}\"},\"allocation/${@tenant}/vhost/${@name}.${@tenant}@public\":\"${@name}.${@tenant}@public\"}}"
  },
  {
    "draft": false,
    "lastModified": 1697191128930.0,
    "payload": "{\"id\":\"kpn/metrics-proxy\",\"name\":\"metrics-proxy\",\"version\":\"0.1.1\",\"vendor\":\"KPN\",\"kind\":\"manifest\",\"apiVersion\":\"v0-alpha\",\"description\":\"DSH Metrics Proxy\",\"moreInfo\":\"# metrics-proxy\\nThe **metrics-proxy** can combine the metrics of multiple Prometheus exporters, possibly running in different services in your tenant, and expose the result via one single endpoint.\\n* The combined metrics will be available for the DSH monitoring-as-a-service.\\n* The **metrics-proxy** has the capability to add prefixes to the combined metric names.\\n* The **metrics-proxy** will also expose the combined metrics via a vhost, making them available for scraping by your own Prometheus instance, or via your browser for testing purposes.\\n* The **metrics-proxy** also adds metrics for the status of the configured exporters.\\n\\nSee the [project's git repo](https://git.kpn.org/projects/GB/repos/metrics-proxy/browse) for more information.\\n\",\"contact\":\"wilbert.schelvis@kpn.com\",\"configuration\":{\"$schema\":\"https://json-schema.org/draft/2019-09/schema\",\"type\":\"object\",\"properties\":{\"METRICS_PREFIX\":{\"description\":\"global metrics prefix\",\"type\":\"string\",\"default\":\"proxy\"},\"METRICS_SCRAPER_1\":{\"description\":\"configuration pattern for scraper 1 ([prefix:]service-name:port[/path])\",\"type\":\"string\",\"default\":\"\"},\"METRICS_SCRAPER_2\":{\"description\":\"configuration pattern for scraper 2 ([prefix:]service-name:port[/path])\",\"type\":\"string\",\"default\":\"\"},\"METRICS_SCRAPER_3\":{\"description\":\"configuration pattern for scraper 3 ([prefix:]service-name:port[/path])\",\"type\":\"string\",\"default\":\"\"},\"METRICS_SCRAPER_4\":{\"description\":\"configuration pattern for scraper 4 ([prefix:]service-name:port[/path])\",\"type\":\"string\",\"default\":\"\"},\"METRICS_SCRAPER_5\":{\"description\":\"configuration pattern for scraper 5 ([prefix:]service-name:port[/path])\",\"type\":\"string\",\"default\":\"\"},\"METRICS_SCRAPER_6\":{\"description\":\"configuration pattern for scraper 6 ([prefix:]service-name:port[/path])\",\"type\":\"string\",\"default\":\"\"}}},\"resources\":{\"allocation/${@tenant}/application/${@name}\":{\"cpus\":0.1,\"env\":{\"METRICS_PREFIX\":\"${METRICS_PREFIX}\",\"METRICS_SCRAPER_1\":\"${METRICS_SCRAPER_1}\",\"METRICS_SCRAPER_2\":\"${METRICS_SCRAPER_2}\",\"METRICS_SCRAPER_3\":\"${METRICS_SCRAPER_3}\",\"METRICS_SCRAPER_4\":\"${METRICS_SCRAPER_4}\",\"METRICS_SCRAPER_5\":\"${METRICS_SCRAPER_5}\",\"METRICS_SCRAPER_6\":\"${METRICS_SCRAPER_6}\"},\"exposedPorts\":{\"9292\":{\"auth\":\"system-fwd-auth@view,manage\",\"tls\":\"auto\",\"vhost\":\"{ vhost('${@name}.${@tenant}', 'public') }\"}},\"image\":\"${@appstore}/release/kpn/metrics-proxy:0.1.1\",\"instances\":1,\"mem\":32,\"metrics\":{\"path\":\"/metrics\",\"port\":9292},\"name\":\"${@name}\",\"needsToken\":false,\"singleInstance\":true,\"user\":\"${@uid}:${@gid}\"},\"allocation/${@tenant}/vhost/${@name}.${@tenant}@public\":\"${@name}.${@tenant}@public\"}}"
  },
  {
    "draft": false,
    "lastModified": 1709294553717.0,
    "payload": "{\"id\":\"kpn/keyring-kafka-database-extractor\",\"name\":\"DSH Database Extractor\",\"version\":\"0.4.2\",\"vendor\":\"KPN\",\"kind\":\"manifest\",\"apiVersion\":\"v0-alpha\",\"description\":\"Application to bulk load/sync records from a relational database to Kafka in DSH\",\"moreInfo\":\"## Database Extractor for Apache Kafka in DSH \\nAn application to import data from any relational database with a JDBC driver into an Apache Kafka topic by extending the [Kafka Connect JDBC Source connector](https://docs.confluent.io/current/connect/kafka-connect-jdbc/source-connector/index.html) project. \\n\\n**1. Data Source:** Currently, we support Teradata, Postgres (includes Yugabyte), MySQL, Oracle, SQLite, SAP Sybase, Microsoft SQL Server and IBM DB2.\\n\\n**2. Kafka JDBC Database Extractor Application:** The application can be deployed with single or multiple workers. Cluster mode ensures fault tolerancy and HA. The data extraction mode can be configured by setting the `MODE` environment variable. These can be bulk, timestamp, incrementing or timestamp+incrementing. Furthermore, the `POLL_INTERVAL` environment variable can be used to determine the degree of realtime sync or periodicity of sync. \\n\\n**3. Data Sink:** The data is stored in JSON format in the kafka topic `DATABASE_OUTPUT_TOPIC`.\\n ### More Information\\nThe _Greenbox Team_ can be reached at **greenbox@kpn.com** for more information.\",\"contact\":\"greenbox@kpn.com\",\"configuration\":{\"$schema\":\"https://json-schema.org/draft/2019-09/schema\",\"type\":\"object\",\"properties\":{\"CPU\":{\"description\":\"Amount of CPU cores per instance.\",\"type\":\"string\",\"default\":\"0.1\"},\"DATABASE\":{\"description\":\"The database to connect to. Currently, we support Teradata, Postgres (includes Yugabyte), MySQL, Oracle, SQLite, SAP Sybase, Microsoft SQL Server and IBM DB2.\",\"type\":\"string\",\"enum\":[\"teradata\",\"postgresql\",\"mysql\",\"oracle\",\"sqlite\",\"sybase\",\"sqlserver\",\"db2\"],\"default\":\"teradata\"},\"DATABASE_JDBC_URL\":{\"description\":\"The database host to connect to. Ensure that the host is accessible from the application. Example: jdbc:teradata://tdp.kpn.org\",\"type\":\"string\",\"default\":\"jdbc:teradata://tdp.kpn.org\"},\"DATABASE_OUTPUT_TOPIC\":{\"description\":\"Name of the Kafka topic to write the records to.\",\"type\":\"string\",\"default\":\"scratch.database-extractor-output.greenbox\"},\"DATABASE_PASSWORD_FIELD_IN_SECRET_STORE\":{\"description\":\"Name of the field in the DSH secrets store that contains the Database password.\",\"type\":\"string\"},\"DATABASE_QUERY\":{\"description\":\"The SQL query to execute. The query should be a valid SQL query for the database specified in the `DATABASE` environment variable.\",\"type\":\"string\",\"default\":\"SELECT * FROM \\u003ctable_name\\u003e\"},\"DATABASE_TIMEZONE\":{\"description\":\"Used to recognize the timezone of timestamp values. The default is UTC. KPN Teradata uses Europe/Amsterdam.\",\"type\":\"string\",\"default\":\"UTC\"},\"DATABASE_USER\":{\"description\":\"Database user name. Make sure this user has the right permissions to execute the query.\",\"type\":\"string\"},\"INCREMENTING_COLUMN_NAME\":{\"description\":\"This is used in the `incrementing` and `timestamp+incrementing` extraction modes. Only relevant for these two modes. The column name should be a valid autoincrementing column in the database table.\",\"type\":\"string\",\"default\":\"\"},\"INSTANCES\":{\"description\":\"The number of instances to deploy in distributed mode (cluster). The default is 1.\",\"type\":\"string\",\"enum\":[\"1\",\"2\",\"3\",\"4\",\"5\"],\"default\":\"1\"},\"LOG_LEVEL\":{\"description\":\"The log level for the application. The default is INFO.\",\"type\":\"string\",\"enum\":[\"TRACE\",\"DEBUG\",\"INFO\",\"WARN\",\"ERROR\"],\"default\":\"INFO\"},\"MEMORY\":{\"description\":\"Amount of memory per instance.\",\"type\":\"string\",\"default\":\"1024\"},\"MODE\":{\"description\":\"The data extraction mode can be `bulk`, `timestamp`, `incrementing` and `timestamp+incrementing`. Bulk is used for batch extraction whereas the other modes are used for realtime extraction.\",\"type\":\"string\",\"enum\":[\"bulk\",\"timestamp\",\"incrementing\",\"timestamp+incrementing\"],\"default\":\"timestamp\"},\"POLL_INTERVAL\":{\"description\":\"Interval in milliseconds for polling (24 hours is 86400000 milliseconds).\",\"type\":\"string\",\"default\":\"60000\"},\"TIMESTAMP_COLUMN_NAME\":{\"description\":\"This is used in the `timestamp` and `timestamp+incrementing` extraction modes. Only relevant for these two modes. The column name should be a valid date/timestamp column in the database table.\",\"type\":\"string\",\"default\":\"\"}}},\"resources\":{\"allocation/${@tenant}/application/${@name}\":{\"cpus\":\"${CPU | number}\",\"env\":{\"DATABASE\":\"${DATABASE}\",\"DATABASE_JDBC_URL\":\"${DATABASE_JDBC_URL}\",\"DATABASE_OUTPUT_TOPIC\":\"${DATABASE_OUTPUT_TOPIC}\",\"DATABASE_QUERY\":\"${DATABASE_QUERY}\",\"DATABASE_TIMEZONE\":\"${DATABASE_TIMEZONE}\",\"DATABASE_USER\":\"${DATABASE_USER}\",\"DEPLOYMENT\":\"distributed\",\"DISTRIBUTED_MODE_CONFIG_TOPIC\":\"scratch.${@name}-config.${@tenant}\",\"DISTRIBUTED_MODE_OFFSETS_TOPIC\":\"scratch.${@name}-offset.${@tenant}\",\"DISTRIBUTED_MODE_STATUS_TOPIC\":\"scratch.${@name}-status.${@tenant}\",\"INCREMENTING_COLUMN_NAME\":\"${INCREMENTING_COLUMN_NAME}\",\"KAFKA_OPTS\":\"-Xms256M\",\"LOG_LEVEL\":\"${LOG_LEVEL}\",\"MODE\":\"${MODE}\",\"POLL_INTERVAL\":\"${POLL_INTERVAL}\",\"TIMESTAMP_COLUMN_NAME\":\"${TIMESTAMP_COLUMN_NAME}\"},\"image\":\"${@appcatalog}/release/kpn/keyring-kafka-database-extractor:0.4.2\",\"imageConsole\":\"registry.cp.kpn-dsh.com/greenbox/keyring-kafka-database-extractor:0.4.2\",\"instances\":\"${INSTANCES | number}\",\"mem\":\"${MEMORY | number}\",\"metrics\":{\"path\":\"/\",\"port\":9009},\"name\":\"${@name}\",\"needsToken\":true,\"secrets\":[{\"injections\":[{\"env\":\"DATABASE_PASSWORD\"}],\"name\":\"${DATABASE_PASSWORD_FIELD_IN_SECRET_STORE}\"}],\"singleInstance\":false,\"user\":\"${@uid}:${@gid}\"},\"allocation/${@tenant}/application/${@name}-ui\":{\"cpus\":0.1,\"env\":{\"CONNECT_URL\":\"${@name}:8083\",\"PORT\":\"8000\"},\"exposedPorts\":{\"8000\":{\"auth\":\"system-fwd-auth@view,manage\",\"tls\":\"auto\",\"vhost\":\"{ vhost('${@name}.${@tenant}', 'public') }\"}},\"image\":\"${@appcatalog}/release/kpn/keyring-kafka-database-extractor-ui:0.4.2\",\"imageConsole\":\"registry.cp.kpn-dsh.com/greenbox/keyring-kafka-database-extractor-ui:0.4.2\",\"instances\":1,\"mem\":8,\"name\":\"${@name}\",\"needsToken\":false,\"singleInstance\":false,\"user\":\"${@uid}:${@gid}\"},\"allocation/${@tenant}/topic/${@name}-config\":{\"kafkaProperties\":{\"cleanup.policy\":\"compact\"},\"name\":\"${@name}-config\",\"partitions\":1,\"replicationFactor\":3},\"allocation/${@tenant}/topic/${@name}-offset\":{\"kafkaProperties\":{\"cleanup.policy\":\"compact\"},\"name\":\"${@name}-offset\",\"partitions\":1,\"replicationFactor\":3},\"allocation/${@tenant}/topic/${@name}-status\":{\"kafkaProperties\":{\"cleanup.policy\":\"compact\"},\"name\":\"${@name}-status\",\"partitions\":1,\"replicationFactor\":3},\"allocation/${@tenant}/vhost/${@name}.${@tenant}@public\":\"${@name}.${@tenant}@public\"}}"
  },
  {
    "draft": false,
    "lastModified": 1751023973951.0,
    "payload": "{\"id\":\"kpn/kafdrop\",\"name\":\"Kafdrop\",\"version\":\"4.1.0\",\"vendor\":\"KPN\",\"kind\":\"manifest\",\"apiVersion\":\"v0-alpha\",\"description\":\"Web UI for viewing kafka topics and consumer groups\",\"moreInfo\":\"## Info  \\n Kafdrop is a web UI for viewing Kafka topics and browsing consumer groups. The tool displays information such as brokers, topics, partitions, consumers, and lets you view messages. \\n## Github \\n https://github.com/obsidiandynamics/kafdrop/ \\n\\n## Release Notes:  \\n* v3.30 newest release\",\"contact\":\"dsh-appcatalog@kpn.com\",\"configuration\":{\"$schema\":\"https://json-schema.org/draft/2019-09/schema\",\"type\":\"object\",\"properties\":{\"CMD_ARGS\":{\"description\":\"Look at the obsidiandynamics/kafdrop repository on GitHub for available options\",\"type\":\"string\",\"default\":\" \"},\"CPU\":{\"description\":\"Amount of CPU cores per instance.\",\"type\":\"string\",\"default\":\"0.3\"},\"DNS_ZONE\":{\"description\":\"DNS zone name for the vhost\",\"type\":\"dns-zone\"},\"JVM_RAM_PERCENTAGE\":{\"description\":\"JVM Memory allocation % of container memory.\",\"type\":\"string\",\"default\":\"75.0\"},\"MEMORY\":{\"description\":\"Amount of memory per instance.\",\"type\":\"string\",\"default\":\"700\"}}},\"resources\":{\"allocation/${@tenant}/application/${@name}\":{\"cpus\":\"${CPU | number}\",\"env\":{\"CMD_ARGS\":\"--server.port=8473 ${CMD_ARGS}\",\"HOST\":\"localhost\",\"JVM_OPTS\":\"-XX:InitialRAMPercentage=${JVM_RAM_PERCENTAGE} -XX:MaxRAMPercentage=${JVM_RAM_PERCENTAGE}\",\"KAFKA_KEYSTORE_FILE\":\"/tmp/keystore.jks\",\"KAFKA_PROPERTIES_FILE\":\"/tmp/datastreams.properties\",\"KAFKA_TRUSTSTORE_FILE\":\"/tmp/truststore.jks\",\"PORT0\":\"10922\",\"SERVER_PORT\":\"8473\",\"SERVER_SERVLET_CONTEXTPATH\":\"/\"},\"exposedPorts\":{\"8473\":{\"auth\":\"system-fwd-auth@view,manage\",\"vhost\":\"{ vhost('${@name}.${@tenant}', '${DNS_ZONE}') }\"}},\"image\":\"${@appcatalog}/release/kpn/kafdrop:4.1.0\",\"instances\":1,\"mem\":\"${MEMORY | number}\",\"name\":\"${@name}\",\"needsToken\":true,\"singleInstance\":false,\"user\":\"${@uid}:${@gid}\"},\"allocation/${@tenant}/vhost/${@name}.${@tenant}@${DNS_ZONE}\":\"${@name}.${@tenant}@${DNS_ZONE}\"}}"
  },
  {
    "draft": true,
    "lastModified": 1758060644690.0,
    "payload": "{\"id\":\"kpn/explorer\",\"name\":\"Explorer\",\"version\":\"0.2.3\",\"vendor\":\"KPN\",\"kind\":\"manifest\",\"apiVersion\":\"v0-alpha\",\"description\":\"An application that helps you explore data in your DSH tenant across various locations, such as Kafka topics or S3 buckets.\",\"moreInfo\":\"## Explorer \\nAn application to explore Apache Kafka topics and S3 buckets. \\n\\n### Features \\n- Explore Apache Kafka topics \\n- Explore S3 buckets\",\"contact\":\"unibox@kpn.com\",\"configuration\":{\"$schema\":\"https://json-schema.org/draft/2019-09/schema\",\"type\":\"object\",\"properties\":{\"CPU\":{\"description\":\"Amount of CPU cores per instance.\",\"type\":\"string\",\"default\":\"1\"},\"LOG_LEVEL\":{\"description\":\"The log level for the application. The default is INFO.\",\"type\":\"string\",\"enum\":[\"trace\",\"debug\",\"info\",\"warn\",\"error\"],\"default\":\"info\"},\"MEMORY\":{\"description\":\"Amount of memory per instance.\",\"type\":\"string\",\"default\":\"2048\"},\"VHOST_DNS_ZONE\":{\"description\":\"The DNS zone for the application: kpn.com (public) and kpn.org (private)\",\"type\":\"string\",\"enum\":[\"public\",\"private\"],\"default\":\"private\"}}},\"resources\":{\"allocation/${@tenant}/application/${@name}\":{\"cpus\":\"${CPU | number}\",\"env\":{\"DSH_ENVIRONMENT\":\"{ variables('DSH_ENVIRONMENT') }\",\"DSH_GID\":\"${@gid}\",\"DSH_PLATFORM_REGION\":\"{ variables('DSH_PLATFORM_REGION') }\",\"DSH_TENANT\":\"{ variables('DSH_TENANT') }\",\"DSH_UID\":\"${@uid}\",\"MAXIMUM_FILE_SIZE_BYTE\":\"1024000\",\"RUST_LOG\":\"${LOG_LEVEL}\",\"SERVER_IP_ADDRESS\":\"0.0.0.0:4000\"},\"exposedPorts\":{\"4000\":{\"auth\":\"system-fwd-auth@view,manage\",\"tls\":\"auto\",\"vhost\":\"{ vhost('${@name}.${@tenant}', '${VHOST_DNS_ZONE}') }\"}},\"image\":\"${@appcatalog}/draft/kpn/greenbox-compact:0.2.3\",\"instances\":1,\"mem\":\"${MEMORY | number}\",\"metrics\":{\"path\":\"/\",\"port\":9009},\"name\":\"${@name}\",\"needsToken\":true,\"secrets\":[{\"injections\":[{\"env\":\"DSH_CLIENT_SECRET\"}],\"name\":\"system/rest-api-client\"},{\"injections\":[{\"env\":\"BUCKET_IDENTIFIER\"}],\"name\":\"system/objectstore/access_key_id\"},{\"injections\":[{\"env\":\"BUCKET_SECRET\"}],\"name\":\"system/objectstore/secret_access_key\"}],\"singleInstance\":true,\"user\":\"${@uid}:${@gid}\"},\"allocation/${@tenant}/bucket/${@name}\":{\"encrypted\":true,\"name\":\"${@name}\",\"versioned\":false},\"allocation/${@tenant}/vhost/${@name}.${@tenant}@${VHOST_DNS_ZONE}\":\"${@name}.${@tenant}@${VHOST_DNS_ZONE}\"}}"
  },
  {
    "draft": false,
    "lastModified": 1701270829460.0,
    "payload": "{\"id\":\"kpn/secor\",\"name\":\"Secor\",\"version\":\"0.30.3\",\"vendor\":\"KPN\",\"kind\":\"manifest\",\"apiVersion\":\"v0-alpha\",\"description\":\"Secor is a service persisting Kafka logs to Amazon S3, Google Cloud Storage, Microsoft Azure Blob Storage and Openstack Swift.\",\"moreInfo\":\"## Key features ##\\n  - **Strong consistency**: as long as [Kafka] is not dropping messages (e.g., due to aggressive cleanup policy) before Secor is able to read them, it is guaranteed that each message will be saved in exactly one [S3] file. This property is not compromised by the notorious temporal inconsistency of [S3] caused by the [eventual consistency] model.\\n  - **Fault tolerance**: any component of Secor is allowed to crash at any given point without compromising data integrity.\\n  - **Load distribution**: Secor may be distributed across multiple machines.\\n  - **Horizontal scalability**: scaling the system out to handle more load is as easy as starting extra Secor processes. Reducing the resource footprint can be achieved by killing any of the running Secor processes. Neither ramping up nor down has any impact on data consistency.\\n  - **Output partitioning**: Secor parses incoming messages and puts them under partitioned s3 paths to enable direct import into systems like [Hive]. day,hour,minute level partitions are supported by secor.\\n  - **Configurable upload policies**: commit points controlling when data is persisted in S3 are configured through size-based and time-based policies (e.g., upload data when local buffer reaches size of 100MB and at least once per hour).\\n  - **Monitoring**: metrics tracking various performance properties are exposed through [Ostrich], [Micrometer] and optionally exported to [OpenTSDB] / [statsD].\\n  - **Customizability**: external log message parser may be loaded by updating the configuration.\\n  - **Event transformation**: external message level transformation can be done by using customized class.\\n  - **Qubole interface**: Secor connects to [Qubole] to add finalized output partitions to Hive tables.\\n## Github \\n https://github.com/pinterest/secor \\n\\n## Release Notes:  \\n* First release\",\"contact\":\"dsh-appcatalog@kpn.com\",\"configuration\":{\"$schema\":\"https://json-schema.org/draft/2019-09/schema\",\"type\":\"object\",\"properties\":{\"AWS_ACCESS_KEY\":{\"description\":\"aws.access.key\",\"type\":\"string\",\"default\":\"system/objectstore/access_key_id\"},\"AWS_REGION\":{\"description\":\"aws.region\",\"type\":\"string\",\"default\":\"eu-central-1\"},\"AWS_SECOR_S3_BUCKET_REQUIRED\":{\"description\":\"secor.s3.bucket dev-dsh-dshtest-secor-demo\",\"type\":\"string\",\"default\":\"\"},\"AWS_SECRET_KEY\":{\"description\":\"aws.secret.key\",\"type\":\"string\",\"default\":\"system/objectstore/secret_access_key\"},\"CLOUD_SERVICE\":{\"description\":\"cloud.service\",\"type\":\"string\",\"enum\":[\"S3\",\"Azure\"],\"default\":\"S3\"},\"SECOR_AZURE_ACCOUNT_KEY\":{\"description\":\"secor.azure.account.key\",\"type\":\"string\",\"default\":\"system/objectstore/secret_access_key\"},\"SECOR_AZURE_ACCOUNT_NAME\":{\"description\":\"secor.azure.account.name\",\"type\":\"string\",\"default\":\"\"},\"SECOR_AZURE_CONTAINER_NAME\":{\"description\":\"secor.azure.container.name\",\"type\":\"string\",\"default\":\"\"},\"SECOR_AZURE_ENDPOINTS_PROTOCOL\":{\"description\":\"secor.azure.endpoints.protocol\",\"type\":\"string\",\"default\":\"https\"},\"SECOR_AZURE_PATH\":{\"description\":\"secor.azure.path\",\"type\":\"string\",\"default\":\"data\"},\"SECOR_FILE_READER_WRITER_FACTORY\":{\"description\":\"secor.file.reader.writer.factory\",\"type\":\"string\",\"enum\":[\"com.pinterest.secor.io.impl.AvroFileReaderWriterFactory\",\"com.pinterest.secor.io.impl.DelimitedTextFileReaderWriterFactory\",\"com.pinterest.secor.io.impl.SequenceFileReaderWriterFactory\",\"com.pinterest.secor.io.impl.JsonORCFileReaderWriterFactory\",\"com.pinterest.secor.io.impl.ProtobufParquetFileReaderWriterFactory\",\"com.pinterest.secor.io.impl.ThriftParquetFileReaderWriterFactory\",\"com.pinterest.secor.io.impl.AvroParquetFileReaderWriterFactory\"],\"default\":\"com.pinterest.secor.io.impl.DelimitedTextFileReaderWriterFactory\"},\"SECOR_KAFKA_TOPIC_FILTER\":{\"description\":\"secor.kafka.topic_filter scratch.secor-demo.dshtest\",\"type\":\"string\",\"default\":\"\"},\"SECOR_MAX_FILE_AGE_SECONDS\":{\"description\":\"secor.max.file.age.seconds\",\"type\":\"string\",\"default\":\"3600\"},\"SECOR_MAX_FILE_SIZE_BYTES\":{\"description\":\"secor.max.file.size.bytes\",\"type\":\"string\",\"default\":\"200000000\"},\"SECOR_MESSAGE_PARSER_CLASS\":{\"description\":\"secor.message.parser.class\",\"type\":\"string\",\"enum\":[\"com.pinterest.secor.parser.JsonMessageParser\",\"com.pinterest.secor.parser.AvroMessageParser\",\"com.pinterest.secor.parser.Iso8601MessageParser\",\"com.pinterest.secor.parser.MessagePackParser\",\"com.pinterest.secor.parser.ProtobufMessageParser\"],\"default\":\"com.pinterest.secor.parser.JsonMessageParser\"},\"SECOR_UPLOAD_MANAGER_CLASS\":{\"description\":\"secor.upload.manager.class\",\"type\":\"string\",\"enum\":[\"com.pinterest.secor.uploader.S3UploadManager\",\"com.pinterest.secor.uploader.AzureUploadManager\"],\"default\":\"com.pinterest.secor.uploader.S3UploadManager\"},\"ZOOKEEPER_QUORUM_REQUIRED\":{\"description\":\"zookeeper name\",\"type\":\"string\",\"default\":\"zk-{zookeeper-name}.{tenant}.marathon.mesos:2181\"}}},\"resources\":{\"allocation/${@tenant}/application/${@name}\":{\"cpus\":0.5,\"env\":{\"PKI_CONFIG_DIR\":\"/tmp\",\"aws.region\":\"${AWS_REGION}\",\"cloud.service\":\"${CLOUD_SERVICE}\",\"kafka.dual.commit.enabled\":\"false\",\"kafka.offsets.storage\":\"kafka\",\"secor.azure.account.name\":\"${SECOR_AZURE_ACCOUNT_NAME}\",\"secor.azure.container.name\":\"${SECOR_AZURE_CONTAINER_NAME}\",\"secor.azure.endpoints.protocol\":\"${SECOR_AZURE_ENDPOINTS_PROTOCOL}\",\"secor.azure.path\":\"${SECOR_AZURE_PATH}\",\"secor.file.reader.writer.factory\":\"${SECOR_FILE_READER_WRITER_FACTORY}\",\"secor.kafka.topic_filter\":\"${SECOR_KAFKA_TOPIC_FILTER}\",\"secor.local.path\":\"/tmp/secor_prod/message_logs/partition\",\"secor.max.file.age.seconds\":\"${SECOR_MAX_FILE_AGE_SECONDS}\",\"secor.max.file.size.bytes\":\"${SECOR_MAX_FILE_SIZE_BYTES}\",\"secor.message.parser.class\":\"${SECOR_MESSAGE_PARSER_CLASS}\",\"secor.s3.bucket\":\"${AWS_SECOR_S3_BUCKET_REQUIRED}\",\"secor.upload.manager.class\":\"${SECOR_UPLOAD_MANAGER_CLASS}\",\"zookeeper.quorum\":\"${ZOOKEEPER_QUORUM_REQUIRED}\"},\"image\":\"${@appcatalog}/release/kpn/secor:0.30.2\",\"instances\":1,\"mem\":2300,\"name\":\"${@name}\",\"needsToken\":true,\"secrets\":[{\"injections\":[{\"env\":\"aws.access.key\"}],\"name\":\"${AWS_ACCESS_KEY}\"},{\"injections\":[{\"env\":\"aws.secret.key\"}],\"name\":\"${AWS_SECRET_KEY}\"},{\"injections\":[{\"env\":\"secor.azure.account.key\"}],\"name\":\"${SECOR_AZURE_ACCOUNT_KEY}\"}],\"singleInstance\":false,\"user\":\"${@uid}:${@gid}\"}}}"
  },
  {
    "draft": false,
    "lastModified": 1758727791231.0,
    "payload": "{\"id\":\"kpn/dsh-labs\",\"name\":\"DSH Labs\",\"version\":\"0.0.3\",\"vendor\":\"KPN\",\"kind\":\"manifest\",\"apiVersion\":\"v0-alpha\",\"description\":\"Run (Jupyter) notebooks on DSH\",\"moreInfo\":\"# DSH Labs\\n\\n**NOTE:** This is an alpha relase, it might contain bugs and is still under development.\\n\\n\\nA fully integrated development environment for data science and machine learning by using Notebooks, built on the DSH platform.\\n\\nThis application is based on Jupyter Notebook. Jupyter Notebook is an open-source web application that allows you to create and share documents that contain live code, equations, visualizations, and narrative text. It is widely used for data analysis, machine learning, and scientific computing.\\n\\nThis application provides a user-friendly interface for running Jupyter Notebooks on the DSH platform, allowing users to easily create, edit, and run their notebooks in a secure and scalable environment.\",\"contact\":\"unibox@kpn.com\",\"configuration\":{\"$schema\":\"https://json-schema.org/draft/2019-09/schema\",\"type\":\"object\",\"properties\":{}},\"resources\":{\"allocation/${@tenant}/application/${@name}\":{\"cpus\":0.1,\"env\":{\"DSH_ENVIRONMENT\":\"{ variables('DSH_ENVIRONMENT') }\",\"DSH_PLATFORM_REGION\":\"{ variables('DSH_PLATFORM_REGION')}\",\"RUST_LOG\":\"info\",\"USER\":\"${@uid}:${@gid}\"},\"exposedPorts\":{\"5317\":{\"auth\":\"system-fwd-auth@view,manage\",\"paths\":[{\"prefix\":\"/\"}],\"tls\":\"auto\",\"vhost\":\"{ vhost('${@name}.${@tenant}', 'private') }\"}},\"image\":\"${@appcatalog}/release/kpn/dsh-labs-app:0.0.3\",\"imageConsole\":\"registry.cp.kpn-dsh.com/dsh-studio/dsh-labs-app:0.0.3\",\"instances\":1,\"mem\":128,\"name\":\"${@name}\",\"needsToken\":true,\"secrets\":[{\"injections\":[{\"env\":\"REST_API_CLIENT_KEY\"}],\"name\":\"system/rest-api-client\"}],\"singleInstance\":true,\"user\":\"${@uid}:${@gid}\"},\"allocation/${@tenant}/vhost/${@name}.${@tenant}@private\":\"${@name}.${@tenant}@private\"}}"
  },
  {
    "draft": false,
    "lastModified": 1715346933928.0,
    "payload": "{\"id\":\"kpn/schema-store-ui\",\"name\":\"Schema Store UI (beta)\",\"version\":\"0.0.14\",\"vendor\":\"KPN\",\"kind\":\"manifest\",\"apiVersion\":\"v0-alpha\",\"description\":\"Swagger UI documentation of the Schema Store API\",\"moreInfo\":\"\",\"contact\":\"dsh-appcatalog@kpn.com\",\"configuration\":{\"$schema\":\"https://json-schema.org/draft/2019-09/schema\",\"type\":\"object\",\"properties\":{\"DNS_ZONE\":{\"description\":\"DNS zone for the vhost\",\"type\":\"string\",\"enum\":[\"public\",\"private\"],\"default\":\"public\"}}},\"resources\":{\"allocation/${@tenant}/application/${@name}\":{\"cpus\":0.1,\"env\":{\"SR_DOMAIN\":\"api.schema-store.dsh.marathon.mesos\",\"SR_PATH\":\"/\",\"SR_PORT\":\"8443\"},\"exposedPorts\":{\"8080\":{\"auth\":\"system-fwd-auth@view,manage\",\"vhost\":\"{ vhost('${@name}.${@tenant}','${DNS_ZONE}') }\"}},\"image\":\"${@appcatalog}/release/kpn/schema-store-ui:0.0.14\",\"instances\":1,\"mem\":256,\"name\":\"${@name}\",\"needsToken\":true,\"singleInstance\":false,\"user\":\"${@uid}:${@gid}\"},\"allocation/${@tenant}/vhost/${@name}.${@tenant}@public\":\"${@name}.${@tenant}@${DNS_ZONE}\"}}"
  },
  {
    "draft": false,
    "lastModified": 1725952483575.0,
    "payload": "{\"id\":\"kpn/kafka-data-archiver\",\"name\":\"Kafka Data Archiver\",\"version\":\"1.0.0\",\"vendor\":\"KPN\",\"kind\":\"manifest\",\"apiVersion\":\"v0-alpha\",\"description\":\"Service to store kafka messages in an object storage like s3 and adls gen2\",\"moreInfo\":\"## Kafka Data Archiver (KDA)\\nA dsh service to store kafka messages in an object storage like S3 or ADLS Gen2. It also allows to convert kafka message into another format using schemas.\\n\\nTo run a KDA, an application configuration should be provided (stored in dsh secrets).\\nHere is an example application configuration file for storing json messages from kafka into AWS S3 bucket as apache parquet format;\\n```\\ninclude \\\"base_application.conf\\\" # Contains application configuration with their default values\\ninclude \\\"base_kafka.conf\\\" # Contains kafka configuration with default values\\n\\nkafka.consumerBootstrapServers = [\\n  \\\"broker-0.tt.kafka.mesos:9091\\\"\\n  \\\"broker-1.tt.kafka.mesos:9091\\\"\\n  \\\"broker-2.tt.kafka.mesos:9091\\\"\\n]\\n\\nkafka.producerBootstrapServers = [\\n  \\\"broker-0.tt.kafka.mesos:9091\\\"\\n  \\\"broker-1.tt.kafka.mesos:9091\\\"\\n  \\\"broker-2.tt.kafka.mesos:9091\\\"\\n]\\nkafka.readTopics = [\\n  \\\"input_topic1\\\"\\n]\\nkafka.groupId = \\\"example_groupid_1\\\"\\n\\napp.maxMessageCountToUpload = 1000 # This is the threshold for number of messages written in a local file before uploading to object storage.\\napp.timeIntervalToUpload = 60.seconds # This is timeout threshold to upload file to object storage.\\npartitionerType = \\\"timebased\\\" # If you want KDA to create timebased partition folders in object storage.\\npartitionerFormat = \\\"yyyy-MM-dd\\\" # Timebased Partition format. (Reference: https://docs.oracle.com/en/java/javase/17/docs/api/java.base/java/time/format/DateTimeFormatter.html#patterns))\\nparquet.compression = \\\"snappy\\\"\\n\\n# Topic input format mapping. You can define multiple topic which contains different message formats.\\ntopic.inputFormat { \\n  \\\"input_topic1\\\": json\\n}\\n\\n# Topic ouput format mapping. You can define multiple topics with is stored in different formats.\\ntopic.outputFormat {\\n  \\\"input_topic1\\\": parquet\\n}\\n\\n# Topic schema id mapping.\\ntopic.schemaIds {\\n  \\\"input_topic1\\\": \\\"schema-v1.avsc\\\"\\n}\\n\\n# Topic s3 bucket mapping. It's possible to map each topic to different s3 bucket.\\ntopic.s3Buckets {\\n  \\\"input_topic1\\\": \\\"bucket1\\\"\\n}\\n\\nstorage.type = S3 # Valid values S3 or ADLS_GEN_2\\nstorage.s3.prefix = \\\"archived\\\" # Prefix in s3 bucket before topic name\\n\\n# Schema Registry Url is needed if message conversion if needed. \\n# Schemas can be stored in s3, adls_gen2 or http service\\nschemaRegistry.url = \\\"s3://bucket1/schemas\\\"\\n\\n# Define aws_access_key_id for each bucket\\nstorage.s3.accessKeyIds {\\n  \\\"bucket1\\\": \\\"value of aws_access_key_id\\\"\\n}\\n\\n# Define aws_secret_key for each bucket\\nstorage.s3.secretKeys {\\n  \\\"bucket1\\\": \\\"value of aws_secret_key\\\"\\n}\\n\\nerror.tolerance = \\\"ignore\\\"\\nerror.routing {\\n  \\\"input_topic1\\\": \\\"dead-letter-queue1\\\"\\n}```\\n\",\"contact\":\"dsh-lfm@kpn.com\",\"configuration\":{\"$schema\":\"https://json-schema.org/draft/2019-09/schema\",\"type\":\"object\",\"properties\":{\"APP_CONFIG_SECRET_NAME\":{\"description\":\"Name of the dsh secret which contains kda application configuration.\",\"type\":\"string\"},\"CPU\":{\"description\":\"Amount of CPU cores per instance.\",\"type\":\"string\",\"default\":\"0.1\"},\"INSTANCES\":{\"description\":\"The number of instances to deploy in distributed mode (cluster). The default is 1.\",\"type\":\"string\",\"default\":\"1\"},\"JVM_MAX_RAM_PERCENTAGE\":{\"description\":\"Sets -XX:MaxRAMPercentage JVM argument\",\"type\":\"string\",\"default\":\"80\"},\"LOG_LEVEL\":{\"description\":\"\",\"type\":\"string\",\"enum\":[\"TRACE\",\"DEBUG\",\"INFO\",\"WARN\",\"ERROR\"],\"default\":\"INFO\"},\"MEMORY\":{\"description\":\"Amount of memory per instance.\",\"type\":\"string\",\"default\":\"512\"}}},\"resources\":{\"allocation/${@tenant}/application/${@name}\":{\"cpus\":\"${CPU | number}\",\"env\":{\"ARCHIVER_LOG_LEVEL\":\"${LOG_LEVEL}\",\"DSH_LOG_LEVEL\":\"${LOG_LEVEL}\",\"MAIN_LOG_LEVEL\":\"${LOG_LEVEL}\",\"MAX_RAM_PERCENTAGE\":\"${JVM_MAX_RAM_PERCENTAGE}\"},\"image\":\"${@appcatalog}/release/kpn/kafka-data-archiver:20240206.1\",\"imageConsole\":\"registry.cp.kpn-dsh.com/kpn-lfm-01/kafka-data-archiver_prod:20240206.1\",\"instances\":\"${INSTANCES | number}\",\"mem\":\"${MEMORY | number}\",\"metrics\":{\"path\":\"/metrics\",\"port\":9090},\"name\":\"${@name}\",\"needsToken\":true,\"secrets\":[{\"injections\":[{\"env\":\"APP_CONF\"}],\"name\":\"${APP_CONFIG_SECRET_NAME}\"}],\"singleInstance\":false,\"user\":\"${@uid}:${@gid}\"}}}"
  },
  {
    "draft": false,
    "lastModified": 1709275199262.0,
    "payload": "{\"id\":\"kpn/kafdrop\",\"name\":\"Kafdrop\",\"version\":\"4.0.1\",\"vendor\":\"KPN\",\"kind\":\"manifest\",\"apiVersion\":\"v0-alpha\",\"description\":\"Web UI for viewing kafka topics and consumer groups\",\"moreInfo\":\"## Info  \\n Kafdrop is a web UI for viewing Kafka topics and browsing consumer groups. The tool displays information such as brokers, topics, partitions, consumers, and lets you view messages. \\n## Github \\n https://github.com/obsidiandynamics/kafdrop/ \\n\\n## Release Notes:  \\n* v4.0.1 newest release\",\"contact\":\"dsh-appcatalog@kpn.com\",\"configuration\":{\"$schema\":\"https://json-schema.org/draft/2019-09/schema\",\"type\":\"object\",\"properties\":{\"CMD_ARGS\":{\"description\":\"Look at the obsidiandynamics/kafdrop repository on GitHub for available options\",\"type\":\"string\",\"default\":\" \"},\"CPU\":{\"description\":\"Amount of CPU cores per instance.\",\"type\":\"string\",\"default\":\"0.3\"},\"DNS_ZONE\":{\"description\":\"DNS zone name for the vhost\",\"type\":\"dns-zone\"},\"JVM_RAM_PERCENTAGE\":{\"description\":\"JVM Memory allocation % of container memory.\",\"type\":\"string\",\"default\":\"75.0\"},\"MEMORY\":{\"description\":\"Amount of memory per instance.\",\"type\":\"string\",\"default\":\"700\"}}},\"resources\":{\"allocation/${@tenant}/application/${@name}\":{\"cpus\":\"${CPU | number}\",\"env\":{\"CMD_ARGS\":\"--server.port=8473 ${CMD_ARGS}\",\"HOST\":\"localhost\",\"JVM_OPTS\":\"-XX:InitialRAMPercentage=${JVM_RAM_PERCENTAGE} -XX:MaxRAMPercentage=${JVM_RAM_PERCENTAGE}\",\"KAFKA_KEYSTORE_FILE\":\"/tmp/keystore.jks\",\"KAFKA_PROPERTIES_FILE\":\"/tmp/datastreams.properties\",\"KAFKA_TRUSTSTORE_FILE\":\"/tmp/truststore.jks\",\"PORT0\":\"10922\",\"SERVER_PORT\":\"8473\",\"SERVER_SERVLET_CONTEXTPATH\":\"/\"},\"exposedPorts\":{\"8473\":{\"auth\":\"system-fwd-auth@view,manage\",\"vhost\":\"{ vhost('${@name}.${@tenant}', '${DNS_ZONE}') }\"}},\"image\":\"${@appcatalog}/release/kpn/kafdrop:4.0.1\",\"instances\":1,\"mem\":\"${MEMORY | number}\",\"name\":\"${@name}\",\"needsToken\":true,\"singleInstance\":false,\"user\":\"${@uid}:${@gid}\"},\"allocation/${@tenant}/vhost/${@name}.${@tenant}@${DNS_ZONE}\":\"${@name}.${@tenant}@${DNS_ZONE}\"}}"
  },
  {
    "draft": true,
    "lastModified": 1740158176466.0,
    "payload": "{\"id\":\"kpn/greenbox\",\"name\":\"Greenbox\",\"version\":\"0.0.7\",\"vendor\":\"KPN\",\"kind\":\"manifest\",\"apiVersion\":\"v0-alpha\",\"description\":\"An application that helps you explore data in your DSH tenant across various locations, such as Kafka topics or S3 buckets, and manage the compliance of your data. Since this application is in an early stage, please don't use it in a production environment—only in development for now.\",\"moreInfo\":\"## Greenbox \\nAn application to explore Apache Kafka topics and S3 buckets. \\n\\n### Features \\n- Explore Apache Kafka topics \\n- Explore S3 buckets \\n- Manage data compliance \\n- Integrated analytics tools\",\"contact\":\"For more information, please contact unibox@kpn.com\",\"configuration\":{\"$schema\":\"https://json-schema.org/draft/2019-09/schema\",\"type\":\"object\",\"properties\":{\"CPU\":{\"description\":\"Amount of CPU cores per instance.\",\"type\":\"string\",\"default\":\"0.1\"},\"GREENBOX_BUCKET_NAME\":{\"description\":\"Greenbox needs an S3 bucket to store its data, including backups, files, and logs. Please create a bucket and put the name here. e.g. my-s3-bucket\",\"type\":\"string\"},\"GREENBOX_POSTGRESQL_PASSWORD_SECRET_NAME\":{\"description\":\"Create a strong password and store it in DSH Secrets. Then, enter the name of the secret here.\",\"type\":\"string\"},\"LOG_LEVEL\":{\"description\":\"The log level for the application. The default is INFO.\",\"type\":\"string\",\"enum\":[\"trace\",\"debug\",\"info\",\"warn\",\"error\"],\"default\":\"info\"},\"MEMORY\":{\"description\":\"Amount of memory per instance.\",\"type\":\"string\",\"default\":\"256\"},\"SEARCH_SERVICE_INSTANCE\":{\"description\":\"The number of search service instances. If you run a lot of searches, increase the number of instances. By default, it is 2.\",\"type\":\"string\",\"default\":\"2\"},\"VHOST_DNS_ZONE\":{\"description\":\"The DNS zone for the application: kpn.com (public) and kpn.org (private)\",\"type\":\"string\",\"enum\":[\"public\",\"private\"],\"default\":\"private\"},\"VOLUME\":{\"description\":\"Size of the volume in GB. This is used by postgres.\",\"type\":\"string\",\"default\":\"5\"}}},\"resources\":{\"allocation/${@tenant}/application/${@name}\":{\"cpus\":\"${CPU | number}\",\"env\":{\"CHECK_SERVICE_NAME\":\"${@name}-check-service\",\"CHECK_SERVICE_PORT\":\"4001\",\"DATABASE_HOST\":\"${@name}-postgres\",\"DATABASE_NAME\":\"${@name}\",\"DATABASE_PORT\":\"5432\",\"DATABASE_USER\":\"${@name}\",\"DB_BACKUP_ENABLED\":\"false\",\"DB_BACKUP_INTERVAL_MS\":\"60000\",\"DB_BACKUP_S3_BUCKET_NAME\":\"{bucket_name('${GREENBOX_BUCKET_NAME}')}\",\"DSH_GID\":\"${@gid}\",\"DSH_REALM\":\"{ variables('DSH_ENVIRONMENT') }\",\"DSH_TENANT\":\"{ variables('DSH_TENANT') }\",\"DSH_UID\":\"${@uid}\",\"RUST_LOG\":\"${LOG_LEVEL}\",\"SERVER_IP_ADDRESS\":\"127.0.0.1:4000\"},\"exposedPorts\":{\"3000\":{\"auth\":\"system-fwd-auth@view,manage\",\"paths\":[{\"prefix\":\"/\"}],\"tls\":\"auto\",\"vhost\":\"{ vhost('${@name}.${@tenant}', '${VHOST_DNS_ZONE}') }\"}},\"image\":\"${@appcatalog}/release/kpn/greenbox:0.0.16\",\"imageConsole\":\"registry.cp.kpn-dsh.com/greenbox-dev/greenbox:0.0.16\",\"instances\":1,\"mem\":\"${MEMORY | number}\",\"metrics\":{\"path\":\"/\",\"port\":9009},\"name\":\"${@name}\",\"needsToken\":true,\"secrets\":[{\"injections\":[{\"env\":\"DSH_CLIENT_SECRET\"}],\"name\":\"system/rest-api-client\"},{\"injections\":[{\"env\":\"AWS_ACCESS_KEY_ID\"}],\"name\":\"system/objectstore/access_key_id\"},{\"injections\":[{\"env\":\"AWS_SECRET_ACCESS_KEY\"}],\"name\":\"system/objectstore/secret_access_key\"},{\"injections\":[{\"env\":\"POSTGRESQL_PASSWORD\"}],\"name\":\"${GREENBOX_POSTGRESQL_PASSWORD_SECRET_NAME}\"}],\"singleInstance\":true,\"user\":\"${@uid}:${@gid}\"},\"allocation/${@tenant}/application/${@name}-check-service\":{\"cpus\":0.1,\"env\":{\"DATABASE_HOST\":\"${@name}-postgres\",\"DATABASE_NAME\":\"${@name}\",\"DATABASE_PORT\":\"5432\",\"DATABASE_USER\":\"${@name}\",\"DSH_REALM\":\"{ variables('DSH_ENVIRONMENT') }\",\"DSH_TENANT_NAME\":\"{ variables('DSH_TENANT') }\",\"MAXIMUM_FILE_SIZE_BYTE\":\"1024000\",\"OUTPUT_BUCKET_NAME\":\"{bucket_name('${GREENBOX_BUCKET_NAME}')}\",\"RUST_LOG\":\"${LOG_LEVEL}\",\"SERVER_IP_ADDRESS\":\"0.0.0.0:4001\"},\"image\":\"${@appcatalog}/release/kpn/greenbox-check-service:0.0.8\",\"imageConsole\":\"registry.cp.kpn-dsh.com/greenbox-dev/greenbox-check-service:0.0.8\",\"instances\":\"${SEARCH_SERVICE_INSTANCE | number}\",\"mem\":512,\"name\":\"${@name}-check-service\",\"needsToken\":false,\"secrets\":[{\"injections\":[{\"env\":\"POSTGRESQL_PASSWORD\"}],\"name\":\"${GREENBOX_POSTGRESQL_PASSWORD_SECRET_NAME}\"}],\"singleInstance\":false,\"user\":\"${@uid}:${@gid}\"},\"allocation/${@tenant}/application/${@name}-postgres\":{\"cpus\":0.3,\"env\":{\"POSTGRESQL_DATABASE\":\"${@name}\",\"POSTGRESQL_USERNAME\":\"${@name}\"},\"image\":\"${@appcatalog}/release/kpn/greenbox-database:0.0.1-dev\",\"imageConsole\":\"registry.cp.kpn-dsh.com/greenbox-dev/greenbox-database:0.0.1-dev\",\"instances\":1,\"mem\":1024,\"name\":\"${@name}-postgres\",\"needsToken\":false,\"secrets\":[{\"injections\":[{\"env\":\"POSTGRESQL_PASSWORD\"}],\"name\":\"${GREENBOX_POSTGRESQL_PASSWORD_SECRET_NAME}\"}],\"singleInstance\":true,\"user\":\"${@uid}:${@gid}\",\"volumes\":{\"/bitnami/postgresql/\":{\"name\":\"{ volume('${@name}-postgres') }\"}}},\"allocation/${@tenant}/vhost/${@name}.${@tenant}@${VHOST_DNS_ZONE}\":\"${@name}.${@tenant}@${VHOST_DNS_ZONE}\",\"allocation/${@tenant}/volume/${@name}-postgres\":{\"name\":\"${@name}-postgres\",\"size\":\"${VOLUME | number}\"}}}"
  },
  {
    "draft": false,
    "lastModified": 1697188154779.0,
    "payload": "{\"id\":\"kpn/aep-sink-connect\",\"name\":\"aep-sink-connect\",\"version\":\"0.1.0\",\"vendor\":\"KPN\",\"kind\":\"manifest\",\"apiVersion\":\"v0-alpha\",\"description\":\"The application will read events from a stream and push it to the Adobe Experience Platform (AEP) to update customer profiles\",\"moreInfo\":\"The application needs a simple configuration file to set up the proper topic and data endpoint for AEP.\\n### More context\\n The Adobe Experience Platform contains customer profiles and can be used to personalize customer experience. The customer profiles can be updated real time by streaming customer events to the platform. The container we developed will take events from a stream in the DSH and push it to AEP.\\n### Use case\\n The Advanced Analytics department has built models which scores customer profiles and/or actions. An action of a customer can trigger a model to score that action and publish it on a kafka stream. The container will take the score from the stream and send it to Adobe. In the future more models will be developed. To prevent that analists need to rebuild this step to get data from a stream to AEP we want to turn it into a re-useable component.\",\"contact\":\"dsh-appcatalog@kpn.com\",\"configuration\":{\"$schema\":\"https://json-schema.org/draft/2019-09/schema\",\"type\":\"object\",\"properties\":{\"AEP_CLIENT_ID\":{\"description\":\"aep-client-id\",\"type\":\"string\",\"default\":\"\"},\"AEP_CLIENT_SECRET\":{\"description\":\"aep-client-secret\",\"type\":\"string\",\"default\":\"\"},\"AEP_PRIVATE_KEY\":{\"description\":\"aep-private-key\",\"type\":\"string\",\"default\":\"\"},\"AEP_TECHNICAL_ACCOUNT_ID\":{\"description\":\"aep-technical-account-id\",\"type\":\"string\",\"default\":\"\"},\"CONNECT_CONFIG_STORAGE_TOPIC\":{\"description\":\"scratch.kafka-connect-aep-config-storage.\\u003ctenant_name\\u003e\",\"type\":\"string\",\"default\":\"\"},\"CONNECT_OFFSET_STORAGE_TOPIC\":{\"description\":\"scratch.kafka-connect-aep-offset-storage.\\u003ctenant_name\\u003e\",\"type\":\"string\",\"default\":\"\"},\"CONNECT_REST_ADVERTISED_HOST_NAME\":{\"description\":\"\\u003cservice_name\\u003e.\\u003ctenant_name\\u003e.marathon.mesos\",\"type\":\"string\",\"default\":\"\"},\"CONNECT_STATUS_STORAGE_TOPIC\":{\"description\":\"scratch.kafka-connect-aep-status-storage.\\u003ctenant_name\\u003e\",\"type\":\"string\",\"default\":\"\"},\"ENV\":{\"description\":\"platform\",\"type\":\"string\",\"enum\":[\"np\",\"prod\"],\"default\":\"prod\"},\"VOL_KAFKA_CONNECT_JARS\":{\"description\":\"kafka-connect-jars\",\"type\":\"string\",\"default\":\"kafka-connect-jars\"},\"VOL_KAFKA_CONNECT_SECRETS\":{\"description\":\"kafka-connect-secrets\",\"type\":\"string\",\"default\":\"kafka-connect-secrets\"}}},\"resources\":{\"allocation/${@tenant}/application/${@name}\":{\"cpus\":1,\"env\":{\"AEP_IMS_ORG\":\"BCC6148954F6271F0A4C98BC@AdobeOrg\",\"CONNECT_CONFIG_STORAGE_TOPIC\":\"${CONNECT_CONFIG_STORAGE_TOPIC}\",\"CONNECT_KEY_CONVERTER\":\"org.apache.kafka.connect.storage.StringConverter\",\"CONNECT_KEY_IGNORE\":\"true\",\"CONNECT_LOG4J_LOGGERS\":\"org.reflections=ERROR,org.apache.zookeeper=ERROR,org.I0Itec.zkclient=ERROR,com.adobe.platform.streaming.sink=DEBUG\",\"CONNECT_OFFSET_STORAGE_TOPIC\":\"${CONNECT_OFFSET_STORAGE_TOPIC}\",\"CONNECT_REST_ADVERTISED_HOST_NAME\":\"${CONNECT_REST_ADVERTISED_HOST_NAME}\",\"CONNECT_REST_PORT\":\"8080\",\"CONNECT_SECURITY_PROTOCOL\":\"SSL\",\"CONNECT_STATUS_STORAGE_TOPIC\":\"${CONNECT_STATUS_STORAGE_TOPIC}\",\"CONNECT_VALUE_CONVERTER\":\"org.apache.kafka.connect.json.JsonConverter\",\"CONNECT_VALUE_CONVERTER.SCHEMAS.ENABLE\":\"false\",\"ENV\":\"${ENV}\",\"LOGGING_ENABLED\":\"false\",\"MANAGE_JOB_INTERVAL\":\"60\",\"PKI_CONFIG_DIR\":\"/home/appuser/config\"},\"image\":\"${@appcatalog}/release/kpn/aep-sink-connect:0.1.0\",\"instances\":1,\"mem\":2048,\"name\":\"${@name}\",\"needsToken\":true,\"secrets\":[{\"injections\":[{\"env\":\"AEP_CLIENT_ID\"}],\"name\":\"${AEP_CLIENT_ID}\"},{\"injections\":[{\"env\":\"AEP_CLIENT_SECRET\"}],\"name\":\"${AEP_CLIENT_SECRET}\"},{\"injections\":[{\"env\":\"AEP_PRIVATE_KEY\"}],\"name\":\"${AEP_PRIVATE_KEY}\"},{\"injections\":[{\"env\":\"AEP_TECHNICAL_ACCOUNT_ID\"}],\"name\":\"${AEP_TECHNICAL_ACCOUNT_ID}\"}],\"singleInstance\":false,\"user\":\"${@uid}:${@gid}\",\"volumes\":{\"/etc/kafka-connect/jars\":{\"name\":\"{ volume('${VOL_KAFKA_CONNECT_JARS}') }\"},\"/etc/kafka-connect/secrets\":{\"name\":\"{ volume('${VOL_KAFKA_CONNECT_SECRETS}') }\"}}},\"allocation/${@tenant}/vhost/${@name}.${@tenant}@public\":\"${@name}.${@tenant}@public\"}}"
  },
  {
    "draft": false,
    "lastModified": 1726481575691.0,
    "payload": "{\"id\":\"kpn/dsh-database-ingester\",\"name\":\"DSH Database Ingester\",\"version\":\"0.4.4\",\"vendor\":\"KPN\",\"kind\":\"manifest\",\"apiVersion\":\"v0-alpha\",\"description\":\"Application to stream events from a Kafka topic to a relational database.\",\"moreInfo\":\"## Database Ingester for DSH \\nAn application to stream structured data from a Kafka topic in DSH to any relational database with a JDBC driver by extending the [Kafka Connect JDBC Sink Connector](https://docs.confluent.io/current/connect/kafka-connect-jdbc/sink-connector/index.html). \\n\\n**1. Data Source:** The source topic is configured via `DATABASE_SOURCE_TOPIC` and the `SCHEMA_TYPE` can be Avro, JSON or Protobuf and the schema should be registered in DSH Schema Store. A flat schema is recommended for the source topic so that the data can be easily mapped to the database table. If not, it will be flattened. Dynamic arrays are not supported. \\n\\n**2. Kafka JDBC Database Ingester Application:** The application can be deployed with single or multiple workers. Cluster mode ensures fault tolerancy and HA. Furthermore, vertical scalability can also be achieved by increasing the number of tasks (`TASK_COUNT`). \\n\\n**3. Data Sink:** Currently, we support SQL Server, Postgres, MySQL, Oracle, SQLite, Sybase and DB2 for `DATABASE_SINK_TABLE`. The application expects the table to be created beforehand with matching columns to that of the schema. Table auto creation and evolution is disabled by default but can be enabled via the UI provided. A `DEAD_LETTER_QUEUE_TOPIC` can also be configured. \\n ### More Information\\nThe _Unibox Team_ can be reached at **unibox@kpn.com** for more information.\",\"contact\":\"unibox@kpn.com\",\"configuration\":{\"$schema\":\"https://json-schema.org/draft/2019-09/schema\",\"type\":\"object\",\"properties\":{\"CPU\":{\"description\":\"Amount of CPU cores per instance.\",\"type\":\"string\",\"default\":\"0.1\"},\"DATABASE\":{\"description\":\"The database to connect to. Currently, we support Postgres (includes Yugabyte), MySQL, Oracle, SQLite, SAP Sybase, Microsoft SQL Server and IBM DB2.\",\"type\":\"string\",\"enum\":[\"postgresql\",\"mysql\",\"oracle\",\"sqlite\",\"sybase\",\"sqlserver\",\"db2\"],\"default\":\"sqlserver\"},\"DATABASE_JDBC_URL\":{\"description\":\"The database host to connect to. Ensure that the host is accessible from the application. Example: jdbc:teradata://tdp.kpn.org\",\"type\":\"string\",\"default\":\"jdbc:teradata://tdp.kpn.org\"},\"DATABASE_PASSWORD_FIELD_IN_SECRET_STORE\":{\"description\":\"Name of the field in the DSH secrets store that contains the Database password.\",\"type\":\"string\"},\"DATABASE_SINK_TABLE\":{\"description\":\"Name of the table in the database to stream the records to.\",\"type\":\"string\",\"default\":\"TABLE_NAME\"},\"DATABASE_SOURCE_TOPIC\":{\"description\":\"Name of the Kafka topic to stream the records from.\",\"type\":\"string\",\"default\":\"scratch.database-ingester-source.greenbox\"},\"DATABASE_TIMEZONE\":{\"description\":\"Used to recognize the timezone of timestamp values. The default is UTC. KPN Teradata uses Europe/Amsterdam.\",\"type\":\"string\",\"default\":\"UTC\"},\"DATABASE_USER\":{\"description\":\"Database user name. Make sure this user has the right permissions to execute the query.\",\"type\":\"string\"},\"DEAD_LETTER_QUEUE_TOPIC\":{\"description\":\"Name of the Kafka topic to stream the dead letter records to in case of errors.\",\"type\":\"string\",\"default\":\"scratch.dead-letter-queue.greenbox\"},\"INSTANCES\":{\"description\":\"The number of instances to deploy in distributed mode (cluster). The default is 1.\",\"type\":\"string\",\"enum\":[\"1\",\"2\",\"3\",\"4\",\"5\"],\"default\":\"1\"},\"LOG_LEVEL\":{\"description\":\"The log level for the application. The default is INFO.\",\"type\":\"string\",\"enum\":[\"TRACE\",\"DEBUG\",\"INFO\",\"WARN\",\"ERROR\"],\"default\":\"INFO\"},\"MEMORY\":{\"description\":\"Amount of memory per instance.\",\"type\":\"string\",\"default\":\"1024\"},\"SCHEMA_TYPE\":{\"description\":\"Schema type of the serialized data in the topic.\",\"type\":\"string\",\"enum\":[\"avro\",\"json\",\"protobuf\"],\"default\":\"avro\"},\"TASK_COUNT\":{\"description\":\"The number of tasks to deploy for better parallelism\",\"type\":\"string\",\"enum\":[\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\"],\"default\":\"3\"}}},\"resources\":{\"allocation/${@tenant}/application/${@name}\":{\"cpus\":\"${CPU | number}\",\"env\":{\"DATABASE\":\"${DATABASE}\",\"DATABASE_JDBC_URL\":\"${DATABASE_JDBC_URL}\",\"DATABASE_SINK_TABLE\":\"${DATABASE_SINK_TABLE}\",\"DATABASE_SOURCE_TOPIC\":\"${DATABASE_SOURCE_TOPIC}\",\"DATABASE_TIMEZONE\":\"${DATABASE_TIMEZONE}\",\"DATABASE_USER\":\"${DATABASE_USER}\",\"DEAD_LETTER_QUEUE_TOPIC\":\"${DEAD_LETTER_QUEUE_TOPIC}\",\"DEPLOYMENT\":\"distributed\",\"DISTRIBUTED_MODE_CONFIG_TOPIC\":\"scratch.${@name}-config.${@tenant}\",\"DISTRIBUTED_MODE_OFFSETS_TOPIC\":\"scratch.${@name}-offset.${@tenant}\",\"DISTRIBUTED_MODE_STATUS_TOPIC\":\"scratch.${@name}-status.${@tenant}\",\"KAFKA_OPTS\":\"-Xms256M -Xmx1000M\",\"LOG_LEVEL\":\"${LOG_LEVEL}\",\"SCHEMA_REGISTRY_URL\":\"https://api.schema-store.dsh.marathon.mesos:8443\",\"SCHEMA_TYPE\":\"${SCHEMA_TYPE}\",\"TASK_COUNT\":\"${TASK_COUNT}\"},\"image\":\"${@appcatalog}/release/kpn/database-ingester-app:0.4.4\",\"imageConsole\":\"registry.cp.kpn-dsh.com/greenbox/database-ingester-app:0.4.4\",\"instances\":\"${INSTANCES | number}\",\"mem\":\"${MEMORY | number}\",\"metrics\":{\"path\":\"/\",\"port\":9009},\"name\":\"${@name}\",\"needsToken\":true,\"secrets\":[{\"injections\":[{\"env\":\"DATABASE_PASSWORD\"}],\"name\":\"${DATABASE_PASSWORD_FIELD_IN_SECRET_STORE}\"}],\"singleInstance\":false,\"user\":\"${@uid}:${@gid}\"},\"allocation/${@tenant}/application/${@name}-ui\":{\"cpus\":0.1,\"env\":{\"CONNECT_URL\":\"${@name}:8083\",\"PORT\":\"8000\"},\"exposedPorts\":{\"8000\":{\"auth\":\"system-fwd-auth@view,manage\",\"tls\":\"auto\",\"vhost\":\"{ vhost('${@name}.${@tenant}', 'public') }\"}},\"image\":\"${@appcatalog}/release/kpn/database-ingester-app-ui:0.4.4\",\"imageConsole\":\"registry.cp.kpn-dsh.com/greenbox/database-ingester-app-ui:0.4.4\",\"instances\":1,\"mem\":16,\"name\":\"${@name}\",\"needsToken\":false,\"singleInstance\":false,\"user\":\"${@uid}:${@gid}\"},\"allocation/${@tenant}/topic/${@name}-config\":{\"kafkaProperties\":{\"cleanup.policy\":\"compact\"},\"name\":\"${@name}-config\",\"partitions\":1,\"replicationFactor\":3},\"allocation/${@tenant}/topic/${@name}-offset\":{\"kafkaProperties\":{\"cleanup.policy\":\"compact\"},\"name\":\"${@name}-offset\",\"partitions\":1,\"replicationFactor\":3},\"allocation/${@tenant}/topic/${@name}-status\":{\"kafkaProperties\":{\"cleanup.policy\":\"compact\"},\"name\":\"${@name}-status\",\"partitions\":1,\"replicationFactor\":3},\"allocation/${@tenant}/vhost/${@name}.${@tenant}@public\":\"${@name}.${@tenant}@public\"}}"
  },
  {
    "draft": false,
    "lastModified": 1731932140571.0,
    "payload": "{\"id\":\"kpn/kafka-data-archiver\",\"name\":\"Kafka Data Archiver\",\"version\":\"1.5.0\",\"vendor\":\"KPN\",\"kind\":\"manifest\",\"apiVersion\":\"v0-alpha\",\"description\":\"Service to store kafka messages in an object storage like s3 and adls gen2\",\"moreInfo\":\"## Kafka Data Archiver (KDA)\\nA dsh service to store kafka messages in an object storage like S3 or ADLS Gen2. It also allows to convert kafka message into another format using schemas.\\n\\nTo run a KDA, an application configuration should be provided (stored in dsh secrets).\\nHere is an example application configuration file for storing json messages from kafka into AWS S3 bucket as apache parquet format;\\n```\\ninclude \\\"base_application.conf\\\" # Contains application configuration with their default values\\ninclude \\\"base_kafka.conf\\\" # Contains kafka configuration with default values\\n\\nkafka.consumerBootstrapServersOverride = false\\n\\nkafka.consumerBootstrapServers = []\\n\\nkafka.consumerBootstrapServersOverride = false\\n\\nkafka.producerBootstrapServers = []\\nkafka.readTopics = [\\n  \\\"input_topic1\\\"\\n]\\nkafka.groupId = \\\"example_groupid_1\\\"\\n\\napp.maxMessageCountToUpload = 1000 # This is the threshold for number of messages written in a local file before uploading to object storage.\\napp.timeIntervalToUpload = 60.seconds # This is timeout threshold to upload file to object storage.\\npartitionerType = \\\"timebased\\\" # If you want KDA to create timebased partition folders in object storage.\\npartitionerFormat = \\\"yyyy-MM-dd\\\" # Timebased Partition format. (Reference: https://docs.oracle.com/en/java/javase/17/docs/api/java.base/java/time/format/DateTimeFormatter.html#patterns))\\nparquet.compression = \\\"snappy\\\"\\n\\n# Topic input format mapping. You can define multiple topic which contains different message formats.\\ntopic.inputFormat { \\n  \\\"input_topic1\\\": json\\n}\\n\\n# Topic ouput format mapping. You can define multiple topics with is stored in different formats.\\ntopic.outputFormat {\\n  \\\"input_topic1\\\": parquet\\n}\\n\\n# Topic schema id mapping.\\ntopic.schemaIds {\\n  \\\"input_topic1\\\": \\\"schema-v1.avsc\\\"\\n}\\n\\n# Topic s3 bucket mapping. It's possible to map each topic to different s3 bucket and/or prefix.\\ntopic.s3Buckets {\\n  \\\"input_topic1\\\" {\\n    bucket: \\\"bucket1\\\"\\n    prefix: \\\"archived\\\"\\n  }\\n}\\n\\nstorage.type = S3 # Valid values S3 or ADLS_GEN2\\n\\n# Schema Registry Url is needed if message conversion if needed. \\n# Schemas can be stored in s3, adls_gen2 or http service\\nschemaRegistry.url = \\\"s3://bucket1/schemas\\\"\\n\\n# Define aws_access_key_id for each bucket\\nstorage.s3.accessKeyIds {\\n  \\\"bucket1\\\": \\\"value of aws_access_key_id\\\"\\n}\\n\\n# Define aws_secret_key for each bucket\\nstorage.s3.secretKeys {\\n  \\\"bucket1\\\": \\\"value of aws_secret_key\\\"\\n}\\n\\n# Alternatively (do not use both!) if storage.type = adls_gen2\\nschemaRegistry.url = \\\"abfss://testcontainer@testaccount.dfs.core.windows.net/schemas\\\"\\n\\ntopic.adlsAccountNames {\\n  \\\"input_topic2\\\": {\\n    accountName: \\\"testaccount\\\"\\n    container: \\\"testcontainer\\\"\\n    prefix: \\\"archived\\\"\\n  }\\n}\\n\\n# Each input_topic needs to have associated credentials, either on account level or SAS keys\\nstorage.adls.accountKeys {\\n  \\\"testaccount\\\": \\\"\\u003cACCOUNT KEY\\u003e\\\"\\n}\\n\\nstorage.adls.sasKeys {\\n  \\\"testaccount\\\": {\\n    \\\"test\\\": {\\n      \\\"schemas\\\": \\\"\\u003cSAS TOKEN\\u003e\\\"\\n    }\\n    \\\"test\\\": {\\n      \\\"archived\\\": \\\"\\u003cSAS TOKEN\\u003e\\\"\\n    }\\n  }\\n}\\n\\n# What to do with messages that cannot be processed (parsing fails with schema)\\nerror.tolerance = \\\"ignore\\\"\\nerror.routing {\\n  \\\"input_topic1\\\": \\\"dead-letter-queue1\\\"\\n}\\n```\\n\",\"contact\":\"dsh-lfm@kpn.com\",\"configuration\":{\"$schema\":\"https://json-schema.org/draft/2019-09/schema\",\"type\":\"object\",\"properties\":{\"APP_CONFIG_SECRET_NAME\":{\"description\":\"Name of the dsh secret which contains kda application configuration.\",\"type\":\"string\"},\"CPU\":{\"description\":\"Amount of CPU cores per instance.\",\"type\":\"string\",\"default\":\"0.1\"},\"INSTANCES\":{\"description\":\"The number of instances to deploy in distributed mode (cluster). The default is 1.\",\"type\":\"string\",\"default\":\"1\"},\"JVM_MAX_RAM_PERCENTAGE\":{\"description\":\"Sets -XX:MaxRAMPercentage JVM argument\",\"type\":\"string\",\"default\":\"80\"},\"LOG_LEVEL\":{\"description\":\"\",\"type\":\"string\",\"enum\":[\"TRACE\",\"DEBUG\",\"INFO\",\"WARN\",\"ERROR\"],\"default\":\"INFO\"},\"MEMORY\":{\"description\":\"Amount of memory per instance.\",\"type\":\"string\",\"default\":\"512\"}}},\"resources\":{\"allocation/${@tenant}/application/${@name}\":{\"cpus\":\"${CPU | number}\",\"env\":{\"ARCHIVER_LOG_LEVEL\":\"${LOG_LEVEL}\",\"DSH_LOG_LEVEL\":\"${LOG_LEVEL}\",\"MAIN_LOG_LEVEL\":\"${LOG_LEVEL}\",\"MAX_RAM_PERCENTAGE\":\"${JVM_MAX_RAM_PERCENTAGE}\"},\"image\":\"${@appcatalog}/release/kpn/kafka-data-archiver:20241118.1\",\"imageConsole\":\"registry.cp.kpn-dsh.com/kpn-lfm-01/kafka-data-archiver_prod:20241118.1\",\"instances\":\"${INSTANCES | number}\",\"mem\":\"${MEMORY | number}\",\"metrics\":{\"path\":\"/metrics\",\"port\":9090},\"name\":\"${@name}\",\"needsToken\":true,\"secrets\":[{\"injections\":[{\"env\":\"APP_CONF\"}],\"name\":\"${APP_CONFIG_SECRET_NAME}\"}],\"singleInstance\":false,\"user\":\"${@uid}:${@gid}\"}}}"
  },
  {
    "draft": false,
    "lastModified": 1708010696082.0,
    "payload": "{\"id\":\"kpn/metrics-proxy\",\"name\":\"Metrics-Proxy\",\"version\":\"0.1.2\",\"vendor\":\"KPN\",\"kind\":\"manifest\",\"apiVersion\":\"v0-alpha\",\"description\":\"DSH Metrics Proxy\",\"moreInfo\":\"# metrics-proxy\\nThe **metrics-proxy** can combine the metrics of multiple Prometheus exporters, possibly running in different services in your tenant, and expose the result via one single endpoint.\\n* The combined metrics will be available for the DSH monitoring-as-a-service.\\n* The **metrics-proxy** has the capability to add prefixes to the combined metric names.\\n* The **metrics-proxy** will also expose the combined metrics via a vhost, making them available for scraping by your own Prometheus instance, or via your browser for testing purposes.\\n* The **metrics-proxy** also adds metrics for the status of the configured exporters.\\n\\nSee the [project's git repo](https://git.kpn.org/projects/GB/repos/metrics-proxy/browse) for more information.\\n\",\"contact\":\"wilbert.schelvis@kpn.com\",\"configuration\":{\"$schema\":\"https://json-schema.org/draft/2019-09/schema\",\"type\":\"object\",\"properties\":{\"METRICS_PREFIX\":{\"description\":\"Global metrics prefix\",\"type\":\"string\",\"default\":\"proxy\"},\"METRICS_SCRAPER_1\":{\"description\":\"Configuration pattern for scraper 1 ([prefix:]service-name:port[/path])\",\"type\":\"string\",\"default\":\"\"},\"METRICS_SCRAPER_2\":{\"description\":\"Configuration pattern for scraper 2 ([prefix:]service-name:port[/path])\",\"type\":\"string\",\"default\":\"\"},\"METRICS_SCRAPER_3\":{\"description\":\"Configuration pattern for scraper 3 ([prefix:]service-name:port[/path])\",\"type\":\"string\",\"default\":\"\"},\"METRICS_SCRAPER_4\":{\"description\":\"Configuration pattern for scraper 4 ([prefix:]service-name:port[/path])\",\"type\":\"string\",\"default\":\"\"},\"METRICS_SCRAPER_5\":{\"description\":\"Configuration pattern for scraper 5 ([prefix:]service-name:port[/path])\",\"type\":\"string\",\"default\":\"\"},\"METRICS_SCRAPER_6\":{\"description\":\"Configuration pattern for scraper 6 ([prefix:]service-name:port[/path])\",\"type\":\"string\",\"default\":\"\"}}},\"resources\":{\"allocation/${@tenant}/application/${@name}\":{\"cpus\":0.1,\"env\":{\"METRICS_PREFIX\":\"${METRICS_PREFIX}\",\"METRICS_SCRAPER_1\":\"${METRICS_SCRAPER_1}\",\"METRICS_SCRAPER_2\":\"${METRICS_SCRAPER_2}\",\"METRICS_SCRAPER_3\":\"${METRICS_SCRAPER_3}\",\"METRICS_SCRAPER_4\":\"${METRICS_SCRAPER_4}\",\"METRICS_SCRAPER_5\":\"${METRICS_SCRAPER_5}\",\"METRICS_SCRAPER_6\":\"${METRICS_SCRAPER_6}\"},\"exposedPorts\":{\"9292\":{\"auth\":\"system-fwd-auth@view,manage\",\"tls\":\"auto\",\"vhost\":\"{ vhost('${@name}.${@tenant}', 'public') }\"}},\"image\":\"${@appstore}/release/kpn/metrics-proxy:0.1.1\",\"instances\":1,\"mem\":32,\"metrics\":{\"path\":\"/metrics\",\"port\":9292},\"name\":\"${@name}\",\"needsToken\":false,\"singleInstance\":true,\"user\":\"${@uid}:${@gid}\"},\"allocation/${@tenant}/vhost/${@name}.${@tenant}@public\":\"${@name}.${@tenant}@public\"}}"
  },
  {
    "draft": false,
    "lastModified": 1749732252043.0,
    "payload": "{\"id\":\"kpn/explorer\",\"name\":\"Explorer\",\"version\":\"0.0.7\",\"vendor\":\"KPN\",\"kind\":\"manifest\",\"apiVersion\":\"v0-alpha\",\"description\":\"An application that helps you explore data in your DSH tenant across various locations, such as Kafka topics or S3 buckets, and manage the compliance of your data.\",\"moreInfo\":\"## Explorer \\nAn application to explore Apache Kafka topics and S3 buckets. \\n\\n### Features \\n- Explore Apache Kafka topics \\n- Explore S3 buckets\",\"contact\":\"unibox@kpn.com\",\"configuration\":{\"$schema\":\"https://json-schema.org/draft/2019-09/schema\",\"type\":\"object\",\"properties\":{\"CPU\":{\"description\":\"Amount of CPU cores per instance.\",\"type\":\"string\",\"default\":\"0.1\"},\"EXPLORER_POSTGRESQL_PASSWORD_SECRET_NAME\":{\"description\":\"Create a strong password and store it in DSH Secrets. Then, enter the name of the secret here.\",\"type\":\"string\"},\"LOG_LEVEL\":{\"description\":\"The log level for the application. The default is INFO.\",\"type\":\"string\",\"enum\":[\"trace\",\"debug\",\"info\",\"warn\",\"error\"],\"default\":\"info\"},\"MEMORY\":{\"description\":\"Amount of memory per instance.\",\"type\":\"string\",\"default\":\"256\"},\"SEARCH_SERVICE_INSTANCE\":{\"description\":\"The number of search service instances. If you run a lot of searches, increase the number of instances. By default, it is 2.\",\"type\":\"string\",\"default\":\"2\"},\"VHOST_DNS_ZONE\":{\"description\":\"The DNS zone for the application: kpn.com (public) and kpn.org (private)\",\"type\":\"string\",\"enum\":[\"public\",\"private\"],\"default\":\"private\"},\"VOLUME\":{\"description\":\"Size of the volume in GB. This is used by postgres.\",\"type\":\"string\",\"default\":\"5\"}}},\"resources\":{\"allocation/${@tenant}/application/${@name}\":{\"cpus\":\"${CPU | number}\",\"env\":{\"CHECK_SERVER_IP_ADDRESS\":\"0.0.0.0:4001\",\"DATABASE_HOST\":\"greenbox-database\",\"DATABASE_NAME\":\"greenbox\",\"DATABASE_PORT\":\"5432\",\"DATABASE_USER\":\"greenbox\",\"DB_BACKUP_S3_BUCKET_NAME\":\"{bucket_name('${@name}')}\",\"DSH_GID\":\"${@gid}\",\"DSH_PLATFORM_REGION\":\"{ variables('DSH_PLATFORM_REGION') }\",\"DSH_REALM\":\"{ variables('DSH_ENVIRONMENT') }\",\"DSH_TENANT\":\"{ variables('DSH_TENANT') }\",\"DSH_UID\":\"${@uid}\",\"MAXIMUM_FILE_SIZE_BYTE\":\"1024000\",\"POSTGRESQL_DATABASE\":\"greenbox\",\"POSTGRESQL_USERNAME\":\"greenbox\",\"RUST_LOG\":\"${LOG_LEVEL}\",\"SERVER_IP_ADDRESS\":\"0.0.0.0:4000\"},\"exposedPorts\":{\"3000\":{\"auth\":\"system-fwd-auth@view,manage\",\"tls\":\"auto\",\"vhost\":\"{ vhost('${@name}.${@tenant}', '${VHOST_DNS_ZONE}') }\"}},\"image\":\"${@appcatalog}/release/kpn/explorer:0.0.33\",\"imageConsole\":\"registry.cp.kpn-dsh.com/greenbox-dev/greenbox-compact:0.0.33\",\"instances\":1,\"mem\":\"${MEMORY | number}\",\"metrics\":{\"path\":\"/\",\"port\":9009},\"name\":\"${@name}\",\"needsToken\":true,\"secrets\":[{\"injections\":[{\"env\":\"DSH_CLIENT_SECRET\"}],\"name\":\"system/rest-api-client\"},{\"injections\":[{\"env\":\"AWS_ACCESS_KEY_ID\"}],\"name\":\"system/objectstore/access_key_id\"},{\"injections\":[{\"env\":\"AWS_SECRET_ACCESS_KEY\"}],\"name\":\"system/objectstore/secret_access_key\"},{\"injections\":[{\"env\":\"DATABASE_PASSWORD\"}],\"name\":\"${EXPLORER_POSTGRESQL_PASSWORD_SECRET_NAME}\"},{\"injections\":[{\"env\":\"POSTGRESQL_PASSWORD\"}],\"name\":\"${EXPLORER_POSTGRESQL_PASSWORD_SECRET_NAME}\"}],\"singleInstance\":true,\"user\":\"${@uid}:${@gid}\",\"volumes\":{\"/bitnami/postgresql/\":{\"name\":\"{ volume('${@name}-postgres') }\"}}},\"allocation/${@tenant}/bucket/${@name}\":{\"encrypted\":true,\"name\":\"${@name}\",\"versioned\":false},\"allocation/${@tenant}/vhost/${@name}.${@tenant}@${VHOST_DNS_ZONE}\":\"${@name}.${@tenant}@${VHOST_DNS_ZONE}\",\"allocation/${@tenant}/volume/${@name}-postgres\":{\"name\":\"${@name}-postgres\",\"size\":\"${VOLUME | number}\"}}}"
  },
  {
    "draft": false,
    "lastModified": 1747750233236.0,
    "payload": "{\"id\":\"kpn/prometheus-scraper\",\"name\":\"Prometheus Scraper\",\"version\":\"0.1.4\",\"vendor\":\"KPN\",\"kind\":\"manifest\",\"apiVersion\":\"v0-alpha\",\"description\":\"Application to scrape Prometheus metrics to Kafka in DSH\",\"moreInfo\":\"## Prometheus Scraper to Kafka \\n\\n  \\n\\n **1. Prometheus URL**: Prometheus endpoint to scrape metrics from. If you do not change the default value `tenant-prometheus-server`, it will be automatically assigned to your tenant's Prometheus server. \\n\\n **2. Kafka Topic**: Kafka topic name for producing metrics to. Can be configured using `KAFKA_TOPIC`. \\n\\n **3. Scraper Interval Seconds**: Interval in seconds at which metrics are scraped. The default interval is 15 secs. Too frequent intervals might cause issues on Prometheus server. So, less than 15 seconds is not allowed and will be reaasigned to 15 secs as minimal interval seconds. Set the interval using `SCRAPE_INTERVAL_SECS`. \\n\\n **4. Prometheus Queries**: Comma-separated Prometheus queries. Default value: 'up,node_cpu_seconds_total'. Can be configured using `PROMETHEUS_QUERIES` variable. \\n\\n **5. Max Retries**: Maxiumum number of retries in the case of failure of scraping metrics. Set to 0 if you do not want to retry. \\n\\n **6. Scaling**: Use `CPU` \\u0026 `MEMORY` values for vertical scaling. \\n\\n ## More Information \\n The Unibox Team can be reached at unibox@kpn.com for more information.\",\"contact\":\"unibox@kpn.com\",\"configuration\":{\"$schema\":\"https://json-schema.org/draft/2019-09/schema\",\"type\":\"object\",\"properties\":{\"CPU\":{\"description\":\"Amount of CPU cores per instance.\",\"type\":\"string\",\"default\":\"0.1\"},\"KAFKA_TOPIC\":{\"description\":\"Kafka Topic name for producing metrics to.\",\"type\":\"string\",\"default\":\"scratch.scraper-metrics.connectors\"},\"LOG_LEVEL\":{\"description\":\"The log level for the application. The default is INFO.\",\"type\":\"string\",\"enum\":[\"TRACE\",\"DEBUG\",\"INFO\",\"WARN\",\"ERROR\"],\"default\":\"INFO\"},\"MAX_RETRIES\":{\"description\":\"Maximum number of retries in the case of failure of scraping metrics. Set to 0 if you do not want to retry.\",\"type\":\"string\",\"enum\":[\"0\",\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\"],\"default\":\"1\"},\"MEMORY\":{\"description\":\"Amount of memory per instance.\",\"type\":\"string\",\"default\":\"64\"},\"PROMETHEUS_ENDPOINT\":{\"description\":\"Prometheus endpoint URL. If you do not change the default value, it will be automatically assigned to your tenant Prometheus server.\",\"type\":\"string\",\"default\":\"tenant-prometheus-server\"},\"PROMETHEUS_QUERIES\":{\"description\":\"Comma-separated Prometheus queries.\",\"type\":\"string\",\"default\":\"up,node_cpu_seconds_total\"},\"SCRAPE_INTERVAL_SECS\":{\"description\":\"Interval in seconds at which metrics are scraped. The default interval is 15 secs. Too frequent intervals might cause issues on Prometheus server. So, less than 15 seconds is not allowed and will be reaasigned to 15 secs (min interval).\",\"type\":\"string\",\"default\":\"15\"}}},\"resources\":{\"allocation/${@tenant}/application/${@name}\":{\"cpus\":\"${CPU | number}\",\"env\":{\"KAFKA_TOPIC\":\"${KAFKA_TOPIC}\",\"MAX_RETRIES\":\"${MAX_RETRIES}\",\"PROMETHEUS_ENDPOINT\":\"${PROMETHEUS_ENDPOINT}\",\"PROMETHEUS_QUERIES\":\" ${PROMETHEUS_QUERIES}\",\"RUST_LOG\":\"${LOG_LEVEL}\",\"SCRAPE_INTERVAL_SECS\":\"${SCRAPE_INTERVAL_SECS}\"},\"image\":\"${@appcatalog}/release/kpn/prometheus-scraper:0.1.4\",\"imageConsole\":\"registry.cp.kpn-dsh.com/connectors/prometheus-scraper:0.1.4\",\"instances\":1,\"mem\":\"${MEMORY | number}\",\"metrics\":{\"path\":\"/metrics\",\"port\":8080},\"name\":\"${@name}\",\"needsToken\":true,\"singleInstance\":false,\"user\":\"${@uid}:${@gid}\"}}}"
  },
  {
    "draft": false,
    "lastModified": 1716790733306.0,
    "payload": "{\"id\":\"kpn/dsh-database-ingester\",\"name\":\"DSH Database Ingester\",\"version\":\"0.4.2\",\"vendor\":\"KPN\",\"kind\":\"manifest\",\"apiVersion\":\"v0-alpha\",\"description\":\"Application to stream events from a Kafka topic to a relational database.\",\"moreInfo\":\"## Database Ingester for DSH \\nAn application to stream structured data from a Kafka topic in DSH to any relational database with a JDBC driver by extending the [Kafka Connect JDBC Sink Connector](https://docs.confluent.io/current/connect/kafka-connect-jdbc/sink-connector/index.html). \\n\\n**1. Data Source:** The source topic is configured via `DATABASE_SOURCE_TOPIC` and the `SCHEMA_TYPE` can be Avro, JSON or Protobuf and the schema should be registered in DSH Schema Store. A flat schema is recommended for the source topic so that the data can be easily mapped to the database table. If not, it will be flattened. Dynamic arrays are not supported. \\n\\n**2. Kafka JDBC Database Ingester Application:** The application can be deployed with single or multiple workers. Cluster mode ensures fault tolerancy and HA. Furthermore, vertical scalability can also be achieved by increasing the number of tasks (`TASK_COUNT`). \\n\\n**3. Data Sink:** Currently, we support SQL Server, Postgres, MySQL, Oracle, SQLite, Sybase and DB2 for `DATABASE_SINK_TABLE`. The application expects the table to be created beforehand with matching columns to that of the schema. Table auto creation and evolution is disabled by default but can be enabled via the UI provided. A `DEAD_LETTER_QUEUE_TOPIC` can also be configured. \\n ### More Information\\nThe _Unibox Team_ can be reached at **unibox@kpn.com** for more information.\",\"contact\":\"unibox@kpn.com\",\"configuration\":{\"$schema\":\"https://json-schema.org/draft/2019-09/schema\",\"type\":\"object\",\"properties\":{\"CPU\":{\"description\":\"Amount of CPU cores per instance.\",\"type\":\"string\",\"default\":\"0.1\"},\"DATABASE\":{\"description\":\"The database to connect to. Currently, we support Postgres (includes Yugabyte), MySQL, Oracle, SQLite, SAP Sybase, Microsoft SQL Server and IBM DB2.\",\"type\":\"string\",\"enum\":[\"postgresql\",\"mysql\",\"oracle\",\"sqlite\",\"sybase\",\"sqlserver\",\"db2\"],\"default\":\"sqlserver\"},\"DATABASE_JDBC_URL\":{\"description\":\"The database host to connect to. Ensure that the host is accessible from the application. Example: jdbc:teradata://tdp.kpn.org\",\"type\":\"string\",\"default\":\"jdbc:teradata://tdp.kpn.org\"},\"DATABASE_PASSWORD_FIELD_IN_SECRET_STORE\":{\"description\":\"Name of the field in the DSH secrets store that contains the Database password.\",\"type\":\"string\"},\"DATABASE_SINK_TABLE\":{\"description\":\"Name of the table in the database to stream the records to.\",\"type\":\"string\",\"default\":\"TABLE_NAME\"},\"DATABASE_SOURCE_TOPIC\":{\"description\":\"Name of the Kafka topic to stream the records from.\",\"type\":\"string\",\"default\":\"scratch.database-ingester-source.greenbox\"},\"DATABASE_TIMEZONE\":{\"description\":\"Used to recognize the timezone of timestamp values. The default is UTC. KPN Teradata uses Europe/Amsterdam.\",\"type\":\"string\",\"default\":\"UTC\"},\"DATABASE_USER\":{\"description\":\"Database user name. Make sure this user has the right permissions to execute the query.\",\"type\":\"string\"},\"DEAD_LETTER_QUEUE_TOPIC\":{\"description\":\"Name of the Kafka topic to stream the dead letter records to in case of errors.\",\"type\":\"string\",\"default\":\"scratch.dead-letter-queue.greenbox\"},\"INSTANCES\":{\"description\":\"The number of instances to deploy in distributed mode (cluster). The default is 1.\",\"type\":\"string\",\"enum\":[\"1\",\"2\",\"3\",\"4\",\"5\"],\"default\":\"1\"},\"LOG_LEVEL\":{\"description\":\"The log level for the application. The default is INFO.\",\"type\":\"string\",\"enum\":[\"TRACE\",\"DEBUG\",\"INFO\",\"WARN\",\"ERROR\"],\"default\":\"INFO\"},\"MEMORY\":{\"description\":\"Amount of memory per instance.\",\"type\":\"string\",\"default\":\"1024\"},\"SCHEMA_TYPE\":{\"description\":\"Schema type of the serialized data in the topic.\",\"type\":\"string\",\"enum\":[\"avro\",\"json\",\"protobuf\"],\"default\":\"avro\"},\"TASK_COUNT\":{\"description\":\"The number of tasks to deploy for better parallelism\",\"type\":\"string\",\"enum\":[\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\"],\"default\":\"3\"}}},\"resources\":{\"allocation/${@tenant}/application/${@name}\":{\"cpus\":\"${CPU | number}\",\"env\":{\"DATABASE\":\"${DATABASE}\",\"DATABASE_JDBC_URL\":\"${DATABASE_JDBC_URL}\",\"DATABASE_SINK_TABLE\":\"${DATABASE_SINK_TABLE}\",\"DATABASE_SOURCE_TOPIC\":\"${DATABASE_SOURCE_TOPIC}\",\"DATABASE_TIMEZONE\":\"${DATABASE_TIMEZONE}\",\"DATABASE_USER\":\"${DATABASE_USER}\",\"DEAD_LETTER_QUEUE_TOPIC\":\"${DEAD_LETTER_QUEUE_TOPIC}\",\"DEPLOYMENT\":\"distributed\",\"DISTRIBUTED_MODE_CONFIG_TOPIC\":\"scratch.${@name}-config.${@tenant}\",\"DISTRIBUTED_MODE_OFFSETS_TOPIC\":\"scratch.${@name}-offset.${@tenant}\",\"DISTRIBUTED_MODE_STATUS_TOPIC\":\"scratch.${@name}-status.${@tenant}\",\"KAFKA_OPTS\":\"-Xms256M -Xmx1000M\",\"LOG_LEVEL\":\"${LOG_LEVEL}\",\"SCHEMA_REGISTRY_URL\":\"https://api.schema-store.dsh.marathon.mesos:8443\",\"SCHEMA_TYPE\":\"${SCHEMA_TYPE}\",\"TASK_COUNT\":\"${TASK_COUNT}\"},\"image\":\"${@appcatalog}/release/kpn/database-ingester-app:0.4.2\",\"imageConsole\":\"registry.cp.kpn-dsh.com/greenbox/database-ingester-app:0.4.2\",\"instances\":\"${INSTANCES | number}\",\"mem\":\"${MEMORY | number}\",\"metrics\":{\"path\":\"/\",\"port\":9009},\"name\":\"${@name}\",\"needsToken\":true,\"secrets\":[{\"injections\":[{\"env\":\"DATABASE_PASSWORD\"}],\"name\":\"${DATABASE_PASSWORD_FIELD_IN_SECRET_STORE}\"}],\"singleInstance\":false,\"user\":\"${@uid}:${@gid}\"},\"allocation/${@tenant}/application/${@name}-ui\":{\"cpus\":0.1,\"env\":{\"CONNECT_URL\":\"${@name}:8083\",\"PORT\":\"8000\"},\"exposedPorts\":{\"8000\":{\"auth\":\"system-fwd-auth@view,manage\",\"tls\":\"auto\",\"vhost\":\"{ vhost('${@name}.${@tenant}', 'public') }\"}},\"image\":\"${@appcatalog}/release/kpn/database-ingester-app-ui:0.4.2\",\"imageConsole\":\"registry.cp.kpn-dsh.com/greenbox/database-ingester-app-ui:0.4.2\",\"instances\":1,\"mem\":16,\"name\":\"${@name}\",\"needsToken\":false,\"singleInstance\":false,\"user\":\"${@uid}:${@gid}\"},\"allocation/${@tenant}/topic/${@name}-config\":{\"kafkaProperties\":{\"cleanup.policy\":\"compact\"},\"name\":\"${@name}-config\",\"partitions\":1,\"replicationFactor\":3},\"allocation/${@tenant}/topic/${@name}-offset\":{\"kafkaProperties\":{\"cleanup.policy\":\"compact\"},\"name\":\"${@name}-offset\",\"partitions\":1,\"replicationFactor\":3},\"allocation/${@tenant}/topic/${@name}-status\":{\"kafkaProperties\":{\"cleanup.policy\":\"compact\"},\"name\":\"${@name}-status\",\"partitions\":1,\"replicationFactor\":3},\"allocation/${@tenant}/vhost/${@name}.${@tenant}@public\":\"${@name}.${@tenant}@public\"}}"
  },
  {
    "draft": false,
    "lastModified": 1696866666007.0,
    "payload": "{\"id\":\"kpn/keyring-service\",\"name\":\"keyring-service\",\"version\":\"0.4.1\",\"vendor\":\"KPN\",\"kind\":\"manifest\",\"apiVersion\":\"v0-alpha\",\"description\":\"KPN keyring service.\",\"moreInfo\":\"# keyring service\\n\\nThe **keyring service** is a service that exposes a REST-service that maps identifiers from a *domain* to a *codomain*. E.g. it can map Boss identifiers to KRN identifiers. The **keyring service** runs in your own tenant environment, under your own control. It optionally provides an OpenAPI specification of the REST-service and a Swagger UI to explore and test the REST-service and it can expose metrics for a Prometheus scraper.\\n\\nBe sure to check the [keyring documentation](https://keyring.dsh-dev.dsh.np.aws.kpn.org/using/app-catalog.html) for more and important information.\\n\",\"contact\":\"greenbox@kpn.com\",\"configuration\":{\"$schema\":\"https://json-schema.org/draft/2019-09/schema\",\"type\":\"object\",\"properties\":{\"CODOMAINS\":{\"description\":\"select the codomain(s) for the relation separated by commas (allowed codomain names: BOSS, ECID, ETKN, KIDH, KRN, SBCM)\",\"type\":\"string\",\"enum\":[\"BOSS\",\"KRN\",\"SBCM\",\"BOSS,KRN,SBCM\",\"ECID\",\"ETKN\",\"KIDH\",\"ECID,ETKN,KIDH\"]},\"DOMAINS\":{\"description\":\"select the domain(s) for the relation separated by commas (allowed domain names: BOSS, ECID, ETKN, KIDH, KRN, SBCM)\",\"type\":\"string\",\"enum\":[\"BOSS\",\"KRN\",\"SBCM\",\"BOSS,KRN,SBCM\",\"ECID\",\"ETKN\",\"KIDH\",\"ECID,ETKN,KIDH\"]},\"ENABLE_METRICS_EXPORTER\":{\"description\":\"select whether to enable exporting metrics\",\"type\":\"string\",\"enum\":[\"no\",\"yes\"],\"default\":\"yes\"},\"ENABLE_OPEN_API_SPECIFICATION\":{\"description\":\"select whether to enable the openapi specification\",\"type\":\"string\",\"enum\":[\"no\",\"yes\"],\"default\":\"yes\"},\"ENABLE_SWAGGER_UI\":{\"description\":\"select whether to enable the swagger gui\",\"type\":\"string\",\"enum\":[\"no\",\"yes\"],\"default\":\"yes\"}}},\"resources\":{\"allocation/${@tenant}/application/${@name}\":{\"cpus\":1,\"env\":{\"CACHE_TYPE\":\"collections\",\"CODOMAINS\":\"${CODOMAINS}\",\"DOMAINS\":\"${DOMAINS}\",\"ENABLE_DERIVED_RELATIONS\":\"true\",\"ENABLE_METRICS_EXPORTER\":\"${ENABLE_METRICS_EXPORTER}\",\"ENABLE_OPEN_API_SPECIFICATION\":\"${ENABLE_OPEN_API_SPECIFICATION}\",\"ENABLE_SWAGGER_UI\":\"${ENABLE_SWAGGER_UI}\",\"GROUP_ID_PREFIX\":\"${@tenant}_\",\"HEAP_MEMORY\":\"4500\",\"KEYRING_SERVICE_PORT\":\"8088\",\"LOG_LEVEL\":\"info\",\"LOG_LEVEL_AKKA\":\"error\",\"LOG_LEVEL_CACHE\":\"info\",\"LOG_LEVEL_CODOMAIN_VALUES\":\"info\",\"LOG_LEVEL_ENGINE\":\"info\",\"LOG_LEVEL_ENTRYPOINT\":\"info\",\"LOG_LEVEL_MASTER_RELATION\":\"info\",\"LOG_LEVEL_REDIS\":\"info\",\"LOG_LEVEL_SERVICE\":\"info\",\"MASTER_RELATIONS_CONFIG_FILE\":\"master-relations-all.conf\",\"METRICS_EXPORTER_PREFIX\":\"\",\"REDIS_EMBEDDED\":\"true\",\"SWAGGER_UI_CONTACT_NAME\":\"Greenbox Team\",\"SWAGGER_UI_CONTACT_URL\":\"https://confluence.kpn.org/display/KEYR/Keyring\",\"SWAGGER_UI_DOCUMENTATION_URL\":\"https://keyring.dsh-dev.dsh.np.aws.kpn.org/using/app-catalog.html\"},\"exposedPorts\":{\"8088\":{\"auth\":\"system-fwd-auth@view,manage\",\"tls\":\"auto\",\"vhost\":\"{ vhost('${@name}.${@tenant}', 'public') }\"}},\"image\":\"${@appstore}/release/kpn/keyring-service:0.4.1\",\"imageConsole\":\"registry.cp.kpn-dsh.com/greenbox-dev/keyring-service:0.4.1\",\"instances\":1,\"mem\":5500,\"metrics\":{\"path\":\"/\",\"port\":9585},\"name\":\"${@name}\",\"needsToken\":true,\"singleInstance\":false,\"user\":\"${@uid}:${@gid}\"},\"allocation/${@tenant}/vhost/${@name}.${@tenant}@public\":\"${@name}.${@tenant}@public\"}}"
  },
  {
    "draft": false,
    "lastModified": 1725953614770.0,
    "payload": "{\"id\":\"kpn/kafka-data-archiver\",\"name\":\"Kafka Data Archiver\",\"version\":\"1.3.0\",\"vendor\":\"KPN\",\"kind\":\"manifest\",\"apiVersion\":\"v0-alpha\",\"description\":\"Service to store kafka messages in an object storage like s3 and adls gen2\",\"moreInfo\":\"## Kafka Data Archiver (KDA)\\nA dsh service to store kafka messages in an object storage like S3 or ADLS Gen2. It also allows to convert kafka message into another format using schemas.\\n\\nTo run a KDA, an application configuration should be provided (stored in dsh secrets).\\nHere is an example application configuration file for storing json messages from kafka into AWS S3 bucket as apache parquet format;\\n```\\ninclude \\\"base_application.conf\\\" # Contains application configuration with their default values\\ninclude \\\"base_kafka.conf\\\" # Contains kafka configuration with default values\\n\\nkafka.consumerBootstrapServers = [\\n  \\\"broker-0.tt.kafka.mesos:9091\\\"\\n  \\\"broker-1.tt.kafka.mesos:9091\\\"\\n  \\\"broker-2.tt.kafka.mesos:9091\\\"\\n]\\n\\nkafka.producerBootstrapServers = [\\n  \\\"broker-0.tt.kafka.mesos:9091\\\"\\n  \\\"broker-1.tt.kafka.mesos:9091\\\"\\n  \\\"broker-2.tt.kafka.mesos:9091\\\"\\n]\\nkafka.readTopics = [\\n  \\\"input_topic1\\\"\\n]\\nkafka.groupId = \\\"example_groupid_1\\\"\\n\\napp.maxMessageCountToUpload = 1000 # This is the threshold for number of messages written in a local file before uploading to object storage.\\napp.timeIntervalToUpload = 60.seconds # This is timeout threshold to upload file to object storage.\\npartitionerType = \\\"timebased\\\" # If you want KDA to create timebased partition folders in object storage.\\npartitionerFormat = \\\"yyyy-MM-dd\\\" # Timebased Partition format. (Reference: https://docs.oracle.com/en/java/javase/17/docs/api/java.base/java/time/format/DateTimeFormatter.html#patterns))\\nparquet.compression = \\\"snappy\\\"\\n\\n# Topic input format mapping. You can define multiple topic which contains different message formats.\\ntopic.inputFormat { \\n  \\\"input_topic1\\\": json\\n}\\n\\n# Topic ouput format mapping. You can define multiple topics with is stored in different formats.\\ntopic.outputFormat {\\n  \\\"input_topic1\\\": parquet\\n}\\n\\n# Topic schema id mapping.\\ntopic.schemaIds {\\n  \\\"input_topic1\\\": \\\"schema-v1.avsc\\\"\\n}\\n\\n# Topic s3 bucket mapping. It's possible to map each topic to different s3 bucket and/or prefix.\\ntopic.s3Buckets {\\n  \\\"input_topic1\\\" {\\n    bucket: \\\"bucket1\\\"\\n    prefix: \\\"archived\\\"\\n  }\\n}\\n\\nstorage.type = S3 # Valid values S3 or ADLS_GEN_2\\n\\n# Schema Registry Url is needed if message conversion if needed. \\n# Schemas can be stored in s3, adls_gen2 or http service\\nschemaRegistry.url = \\\"s3://bucket1/schemas\\\"\\n\\n# Define aws_access_key_id for each bucket\\nstorage.s3.accessKeyIds {\\n  \\\"bucket1\\\": \\\"value of aws_access_key_id\\\"\\n}\\n\\n# Define aws_secret_key for each bucket\\nstorage.s3.secretKeys {\\n  \\\"bucket1\\\": \\\"value of aws_secret_key\\\"\\n}\\n\\n# Alternatively (do not use both!) if storage.type = adls_gen2\\nschemaRegistry.url = \\\"abfss://testcontainer@testaccount.dfs.core.windows.net/schemas\\\"\\n\\ntopic.adlsAccountNames {\\n  \\\"input_topic2\\\": {\\n    accountName: \\\"testaccount\\\"\\n    container: \\\"testcontainer\\\"\\n    prefix: \\\"archived\\\"\\n  }\\n}\\n\\n# Each input_topic needs to have associated credentials, either on account level or SAS keys\\nstorage.adls.accountKeys {\\n  \\\"testaccount\\\": \\\"\\u003cACCOUNT KEY\\u003e\\\"\\n}\\n\\nstorage.adls.sasKeys {\\n  \\\"testaccount\\\": {\\n    \\\"test\\\": {\\n      \\\"schemas\\\": \\\"\\u003cSAS TOKEN\\u003e\\\"\\n    }\\n    \\\"test\\\": {\\n      \\\"archived\\\": \\\"\\u003cSAS TOKEN\\u003e\\\"\\n    }\\n  }\\n}\\n\\n# What to do with messages that cannot be processed (parsing fails with schema)\\nerror.tolerance = \\\"ignore\\\"\\nerror.routing {\\n  \\\"input_topic1\\\": \\\"dead-letter-queue1\\\"\\n}\\n```\\n\",\"contact\":\"dsh-lfm@kpn.com\",\"configuration\":{\"$schema\":\"https://json-schema.org/draft/2019-09/schema\",\"type\":\"object\",\"properties\":{\"APP_CONFIG_SECRET_NAME\":{\"description\":\"Name of the dsh secret which contains kda application configuration.\",\"type\":\"string\"},\"CPU\":{\"description\":\"Amount of CPU cores per instance.\",\"type\":\"string\",\"default\":\"0.1\"},\"INSTANCES\":{\"description\":\"The number of instances to deploy in distributed mode (cluster). The default is 1.\",\"type\":\"string\",\"default\":\"1\"},\"JVM_MAX_RAM_PERCENTAGE\":{\"description\":\"Sets -XX:MaxRAMPercentage JVM argument\",\"type\":\"string\",\"default\":\"80\"},\"LOG_LEVEL\":{\"description\":\"\",\"type\":\"string\",\"enum\":[\"TRACE\",\"DEBUG\",\"INFO\",\"WARN\",\"ERROR\"],\"default\":\"INFO\"},\"MEMORY\":{\"description\":\"Amount of memory per instance.\",\"type\":\"string\",\"default\":\"512\"}}},\"resources\":{\"allocation/${@tenant}/application/${@name}\":{\"cpus\":\"${CPU | number}\",\"env\":{\"ARCHIVER_LOG_LEVEL\":\"${LOG_LEVEL}\",\"DSH_LOG_LEVEL\":\"${LOG_LEVEL}\",\"MAIN_LOG_LEVEL\":\"${LOG_LEVEL}\",\"MAX_RAM_PERCENTAGE\":\"${JVM_MAX_RAM_PERCENTAGE}\"},\"image\":\"${@appcatalog}/release/kpn/kafka-data-archiver:20240430.6\",\"imageConsole\":\"registry.cp.kpn-dsh.com/kpn-lfm-01/kafka-data-archiver_dev:20240425.10\",\"instances\":\"${INSTANCES | number}\",\"mem\":\"${MEMORY | number}\",\"metrics\":{\"path\":\"/metrics\",\"port\":9090},\"name\":\"${@name}\",\"needsToken\":true,\"secrets\":[{\"injections\":[{\"env\":\"APP_CONF\"}],\"name\":\"${APP_CONFIG_SECRET_NAME}\"}],\"singleInstance\":false,\"user\":\"${@uid}:${@gid}\"}}}"
  },
  {
    "draft": false,
    "lastModified": 1707508016944.0,
    "payload": "{\"id\":\"kpn/eavesdropper\",\"name\":\"eavesdropper\",\"version\":\"0.9.1\",\"vendor\":\"KPN\",\"kind\":\"manifest\",\"apiVersion\":\"v0-alpha\",\"description\":\"Web application to visualize messages on Kafka topics in realtime\",\"moreInfo\":\"## Realtime record visualization\\n\\nThis app enables you to view records on your DSH Kafka topics in realtime. It can show the record's keys, values and headers in json, text or binary format, and also recognizes some custom formats for some special topics. Furthermore, it allows you to:\\n\\n* unwrap a record from the envelope that the DSH platform enforces on stream topics, showing the envelope metadata and the envelope payload separately,\\n* filter records based on regular expressions and/or throttling,\\n* show records individually or in list view,\\n* download/copy record values in json, text or binary format.\\n\\nWhen started from the App Catalog the Eavesdropper will be available with the same SSO authorization as the DSH console, and will expose all topics that your tenant is entitled to see.\",\"contact\":\"wilbert.schelvis@kpn.com\",\"configuration\":{\"$schema\":\"https://json-schema.org/draft/2019-09/schema\",\"type\":\"object\",\"properties\":{\"ALLOW_CUSTOM_GROUP_ID\":{\"description\":\"allow custom group id\",\"type\":\"string\",\"enum\":[\"true\",\"false\"],\"default\":\"true\"},\"ENVELOPE_DESERIALIZERS\":{\"description\":\"enable envelope deserializers\",\"type\":\"string\",\"enum\":[\"\",\"dsh-envelope\"],\"default\":\"\"},\"LOG_LEVEL\":{\"description\":\"application log level\",\"type\":\"string\",\"enum\":[\"error\",\"warn\",\"info\"],\"default\":\"info\"},\"REPRESENTATION_DESERIALIZERS\":{\"description\":\"enable representation deserializers\",\"type\":\"string\",\"enum\":[\"\",\"codomain-values-record\",\"greenbox-ri-avro,greenbox-ri-protobuf\",\"codomain-values-record,greenbox-ri-avro,greenbox-ri-protobuf\"],\"default\":\"\"}}},\"resources\":{\"allocation/${@tenant}/application/${@name}\":{\"cpus\":0.1,\"env\":{\"ALLOW_CUSTOM_GROUP_ID\":\"${ALLOW_CUSTOM_GROUP_ID}\",\"CONSUMER_BUILDER\":\"dsh-datastreams-properties\",\"DEFAULT_GROUP_IDS\":\"*\",\"ENVELOPE_DESERIALIZERS\":\"${ENVELOPE_DESERIALIZERS}\",\"EXCLUDED_TOPICS\":\"\",\"EXCLUDED_TOPICS_REGEX\":\"\",\"GROUP_ID_PREFIX\":\"${@tenant}_\",\"INCLUDED_TOPICS\":\"\",\"INCLUDED_TOPICS_REGEX\":\"\",\"INCLUDE___CONSUMER_OFFSETS_TOPIC\":\"false\",\"INSTANCE_IDENTIFIER\":\"${@tenant}_${@name}\",\"LOG_LEVEL\":\"${LOG_LEVEL}\",\"LOG_LEVEL_ENTRYPOINT\":\"${LOG_LEVEL}\",\"LOG_LEVEL_GREENBOX\":\"error\",\"LOG_LEVEL_KAFKA_CLIENT\":\"error\",\"LOG_LEVEL_MONITOR\":\"${LOG_LEVEL}\",\"LOG_LEVEL_RECDES\":\"info\",\"LOG_LEVEL_SERVICE\":\"${LOG_LEVEL}\",\"REPRESENTATION_DESERIALIZERS\":\"${REPRESENTATION_DESERIALIZERS}\"},\"exposedPorts\":{\"8081\":{\"auth\":\"system-fwd-auth@view,manage\",\"tls\":\"auto\",\"vhost\":\"{ vhost('${@name}.${@tenant}', 'public') }\"}},\"image\":\"${@appcatalog}/release/kpn/eavesdropper:0.9.1\",\"instances\":1,\"mem\":384,\"name\":\"${@name}\",\"needsToken\":true,\"singleInstance\":false,\"user\":\"${@uid}:${@gid}\"},\"allocation/${@tenant}/vhost/${@name}.${@tenant}@public\":\"${@name}.${@tenant}@public\"}}"
  },
  {
    "draft": false,
    "lastModified": 1719241959042.0,
    "payload": "{\"id\":\"kpn/keyring-kafka-database-extractor\",\"name\":\"DSH Database Extractor\",\"version\":\"0.4.4\",\"vendor\":\"KPN\",\"kind\":\"manifest\",\"apiVersion\":\"v0-alpha\",\"description\":\"Application to bulk load/sync records from a relational database to Kafka in DSH\",\"moreInfo\":\"## Database Extractor for Apache Kafka in DSH \\nAn application to import data from any relational database with a JDBC driver into an Apache Kafka topic by extending the [Kafka Connect JDBC Source connector](https://docs.confluent.io/current/connect/kafka-connect-jdbc/source-connector/index.html) project. \\n\\n**1. Data Source:** Currently, we support Teradata, Postgres (includes Yugabyte), MySQL, Oracle, SQLite, SAP Sybase, Microsoft SQL Server and IBM DB2.\\n\\n**2. Kafka JDBC Database Extractor Application:** The application can be deployed with single or multiple workers. Cluster mode ensures fault tolerancy and HA. The data extraction mode can be configured by setting the `MODE` environment variable. These can be bulk, timestamp, incrementing or timestamp+incrementing. Furthermore, the `POLL_INTERVAL` environment variable can be used to determine the degree of realtime sync or periodicity of sync. \\n\\n**3. Data Sink:** The data is stored in JSON format in the kafka topic `DATABASE_OUTPUT_TOPIC`.\\n ### More Information\\nThe _Greenbox Team_ can be reached at **greenbox@kpn.com** for more information.\",\"contact\":\"greenbox@kpn.com\",\"configuration\":{\"$schema\":\"https://json-schema.org/draft/2019-09/schema\",\"type\":\"object\",\"properties\":{\"CPU\":{\"description\":\"Amount of CPU cores per instance.\",\"type\":\"string\",\"default\":\"0.1\"},\"DATABASE\":{\"description\":\"The database to connect to. Currently, we support Teradata, Postgres (includes Yugabyte), MySQL, Oracle, SQLite, SAP Sybase, Microsoft SQL Server and IBM DB2.\",\"type\":\"string\",\"enum\":[\"teradata\",\"postgresql\",\"mysql\",\"oracle\",\"sqlite\",\"sybase\",\"sqlserver\",\"db2\"],\"default\":\"teradata\"},\"DATABASE_JDBC_URL\":{\"description\":\"The database host to connect to. Ensure that the host is accessible from the application. Example: jdbc:teradata://tdp.kpn.org\",\"type\":\"string\",\"default\":\"jdbc:teradata://tdp.kpn.org\"},\"DATABASE_OUTPUT_TOPIC\":{\"description\":\"Name of the Kafka topic to write the records to.\",\"type\":\"string\",\"default\":\"scratch.database-extractor-output.greenbox\"},\"DATABASE_PASSWORD_FIELD_IN_SECRET_STORE\":{\"description\":\"Name of the field in the DSH secrets store that contains the Database password.\",\"type\":\"string\"},\"DATABASE_QUERY\":{\"description\":\"The SQL query to execute. The query should be a valid SQL query for the database specified in the `DATABASE` environment variable.\",\"type\":\"string\",\"default\":\"SELECT * FROM \\u003ctable_name\\u003e\"},\"DATABASE_TIMEZONE\":{\"description\":\"Used to recognize the timezone of timestamp values. The default is UTC. KPN Teradata uses Europe/Amsterdam.\",\"type\":\"string\",\"default\":\"UTC\"},\"DATABASE_USER\":{\"description\":\"Database user name. Make sure this user has the right permissions to execute the query.\",\"type\":\"string\"},\"INCREMENTING_COLUMN_NAME\":{\"description\":\"This is used in the `incrementing` and `timestamp+incrementing` extraction modes. Only relevant for these two modes. The column name should be a valid autoincrementing column in the database table.\",\"type\":\"string\",\"default\":\"\"},\"INSTANCES\":{\"description\":\"The number of instances to deploy in distributed mode (cluster). The default is 1.\",\"type\":\"string\",\"enum\":[\"1\",\"2\",\"3\",\"4\",\"5\"],\"default\":\"1\"},\"LOG_LEVEL\":{\"description\":\"The log level for the application. The default is INFO.\",\"type\":\"string\",\"enum\":[\"TRACE\",\"DEBUG\",\"INFO\",\"WARN\",\"ERROR\"],\"default\":\"INFO\"},\"MEMORY\":{\"description\":\"Amount of memory per instance.\",\"type\":\"string\",\"default\":\"1024\"},\"MODE\":{\"description\":\"The data extraction mode can be `bulk`, `timestamp`, `incrementing` and `timestamp+incrementing`. Bulk is used for batch extraction whereas the other modes are used for realtime extraction.\",\"type\":\"string\",\"enum\":[\"bulk\",\"timestamp\",\"incrementing\",\"timestamp+incrementing\"],\"default\":\"timestamp\"},\"POLL_INTERVAL\":{\"description\":\"Interval in milliseconds for polling (24 hours is 86400000 milliseconds).\",\"type\":\"string\",\"default\":\"60000\"},\"TIMESTAMP_COLUMN_NAME\":{\"description\":\"This is used in the `timestamp` and `timestamp+incrementing` extraction modes. Only relevant for these two modes. The column name should be a valid date/timestamp column in the database table.\",\"type\":\"string\",\"default\":\"\"}}},\"resources\":{\"allocation/${@tenant}/application/${@name}\":{\"cpus\":\"${CPU | number}\",\"env\":{\"DATABASE\":\"${DATABASE}\",\"DATABASE_JDBC_URL\":\"${DATABASE_JDBC_URL}\",\"DATABASE_OUTPUT_TOPIC\":\"${DATABASE_OUTPUT_TOPIC}\",\"DATABASE_QUERY\":\"${DATABASE_QUERY}\",\"DATABASE_TIMEZONE\":\"${DATABASE_TIMEZONE}\",\"DATABASE_USER\":\"${DATABASE_USER}\",\"DEPLOYMENT\":\"distributed\",\"DISTRIBUTED_MODE_CONFIG_TOPIC\":\"scratch.${@name}-config.${@tenant}\",\"DISTRIBUTED_MODE_OFFSETS_TOPIC\":\"scratch.${@name}-offset.${@tenant}\",\"DISTRIBUTED_MODE_STATUS_TOPIC\":\"scratch.${@name}-status.${@tenant}\",\"INCREMENTING_COLUMN_NAME\":\"${INCREMENTING_COLUMN_NAME}\",\"KAFKA_OPTS\":\"-Xms256M\",\"LOG_LEVEL\":\"${LOG_LEVEL}\",\"MEMORY\":\"${MEMORY}\",\"MODE\":\"${MODE}\",\"POLL_INTERVAL\":\"${POLL_INTERVAL}\",\"TIMESTAMP_COLUMN_NAME\":\"${TIMESTAMP_COLUMN_NAME}\"},\"image\":\"${@appcatalog}/release/kpn/keyring-kafka-database-extractor:0.4.4\",\"imageConsole\":\"registry.cp.kpn-dsh.com/greenbox/keyring-kafka-database-extractor:0.4.4\",\"instances\":\"${INSTANCES | number}\",\"mem\":\"${MEMORY | number}\",\"metrics\":{\"path\":\"/\",\"port\":9009},\"name\":\"${@name}\",\"needsToken\":true,\"secrets\":[{\"injections\":[{\"env\":\"DATABASE_PASSWORD\"}],\"name\":\"${DATABASE_PASSWORD_FIELD_IN_SECRET_STORE}\"}],\"singleInstance\":false,\"user\":\"${@uid}:${@gid}\"},\"allocation/${@tenant}/application/${@name}-ui\":{\"cpus\":0.1,\"env\":{\"CONNECT_URL\":\"${@name}:8083\",\"PORT\":\"8000\"},\"exposedPorts\":{\"8000\":{\"auth\":\"system-fwd-auth@view,manage\",\"tls\":\"auto\",\"vhost\":\"{ vhost('${@name}.${@tenant}', 'public') }\"}},\"image\":\"${@appcatalog}/release/kpn/keyring-kafka-database-extractor-ui:0.4.4\",\"imageConsole\":\"registry.cp.kpn-dsh.com/greenbox/keyring-kafka-database-extractor-ui:0.4.4\",\"instances\":1,\"mem\":8,\"name\":\"${@name}\",\"needsToken\":false,\"singleInstance\":false,\"user\":\"${@uid}:${@gid}\"},\"allocation/${@tenant}/topic/${@name}-config\":{\"kafkaProperties\":{\"cleanup.policy\":\"compact\"},\"name\":\"${@name}-config\",\"partitions\":1,\"replicationFactor\":3},\"allocation/${@tenant}/topic/${@name}-offset\":{\"kafkaProperties\":{\"cleanup.policy\":\"compact\"},\"name\":\"${@name}-offset\",\"partitions\":1,\"replicationFactor\":3},\"allocation/${@tenant}/topic/${@name}-status\":{\"kafkaProperties\":{\"cleanup.policy\":\"compact\"},\"name\":\"${@name}-status\",\"partitions\":1,\"replicationFactor\":3},\"allocation/${@tenant}/vhost/${@name}.${@tenant}@public\":\"${@name}.${@tenant}@public\"}}"
  },
  {
    "draft": false,
    "lastModified": 1753785086711.0,
    "payload": "{\"id\":\"kpn/airflow-ephemeral\",\"name\":\"Airflow ephemeral\",\"version\":\"3.0.3\",\"vendor\":\"KPN\",\"kind\":\"manifest\",\"apiVersion\":\"v0-alpha\",\"description\":\"Airflow ephemeral instance to test airflow dags. When the instance is down, all state, such as logs and dags will dissappear\",\"moreInfo\":\"## Airflow ephemeral\\n\\nThe airflow ephemeral instance provides a sandbox environment to experiment with Airflow. The python code (DAGs) is pulled in using a gitsync.\\n\\nVersion: 3.0.3\\n\\n- Airflow:  3.0.3\\n- Python:   3.12\\n- Gitsync:  4.4.2\\n- Statsd exporter: 0.28.0\\n\\n### Before you start\\n\\nAdd the following secrets to the DSH secret store:\\n\\n1. airflow admin user password (create a password that you will use for the airflow ui)\\n2. git sync password (can also be a personal access token)\\n\\nThese secrets will be used for the container configuration.\\n\\n### Airflow\\n\\nThe airflow app is an ephemeral app. When it stops or restarts, all state and logs are deleted.\\n\\nTo login to the airflow instance, the default user is `admin` the password is provided by the tenant using the `TENANT_AIRFLOW_ADMIN_USER_PASSWORD_FIELD_IN_SECRET_STORE` environment variable. Which takes the secret you have created earlier from the DSH secrets store and injects it into the container.\\n\\n#### DAGs\\n\\nThe core concept of airflow is a DAG (Directed Acyclic Graph), collecting Tasks together, organized with dependencies and relationships to say how they should run. [airflow dags](https://airflow.apache.org/docs/apache-airflow/stable/core-concepts/dags.html)\\n\\nAll of the code specified in airflow is being wrapped around in tasks and orchestrated by the DAGs. To understand how they work, have a look at the following [Fundamental concepts](https://airflow.apache.org/docs/apache-airflow/stable/tutorial/fundamentals.html). The page also provides a turorial with example DAGs that you can try out.\\n\\n#### Importing packages\\n\\nAt the moment the airflow setup from the app catalog is limited to the pip dependencies that came pre-installed with it. In the future we will provide the option for tenants to install packages to the airflow app. For now you can make use of the [PythonVirtualenvOperator](https://airflow.apache.org/docs/apache-airflow/stable/howto/operator/python.html#pythonvirtualenvoperator)\\n\\nWith the PythonVirtualenvOperator you can pull-in any public package.\\n\\n**Caution: Tenants are responsible for the python packages they import into their airflow container. Please follow the KPN Security Policy ([KSP](https://ciso-ksp.kpnnet.org/welcome/KSP)) as a guideline.**\\n\\n### Gitsync\\n\\nThe gitsync will pull in a git branch and put the code in the specified directory. For the gitsync configuration see all variables with the prefix `GIT_SYNC`. You can put all of your DAGs and python code in this git repository, here the python project structure applies.\\n\\nFor the configuration, make sure to set the following variables to the right values:\\n\\n- GIT_SYNC_DEST: the destination on the container where you would like to have the dags pushed to.  e.g. `/opt/airflow/dags/dex`\\n- GITSYNC_REPO: url of the git repo to sync.\\n- GITSYNC_REF: git branch to pull in.\\n- GITSYNC_PERIOD: number of seconds between syncs.\\n- GITSYNC_ROOT: where on the root should you do the git operations. the default is sufficient.\\n- GITSYNC_ONE_TIME: specifies if the gitsync should continuously pull or exit after the first pull. Default is false.\\n- GITSYNC_USERNAME: username of the user to pull in the git repo.\\n- GITSYNC_PASSWORD_FIELD_IN_SECRET_STORE: the field in the secret store that contains the password of your git account.\\n\\nFor some more background have a look at the [gitsync](https://github.com/kubernetes/git-sync/tree/v3.6.5?tab=readme-ov-file#primary-flags) repo.\\n\\n### More Information\\n\\nThe _DEX Team_ can be reached at **\\u003cdex@kpn.com\\u003e** for more information.\",\"contact\":\"dex@kpn.com\",\"configuration\":{\"$schema\":\"https://json-schema.org/draft/2019-09/schema\",\"type\":\"object\",\"properties\":{\"AIRFLOW_CPU\":{\"description\":\"Amount of CPU cores per instance.\",\"type\":\"string\",\"default\":\"1\"},\"AIRFLOW_MEMORY\":{\"description\":\"Amount of memory per instance.\",\"type\":\"string\",\"default\":\"4096\"},\"GITSYNC_LINK\":{\"description\":\"Folder destination on the host machine where to pull git repo towards.\",\"type\":\"string\",\"default\":\"/opt/airflow/dags/repo_name\"},\"GITSYNC_ONE_TIME\":{\"description\":\"specify to only pull on startup or continuously, if true the gitsync exits after the first sync.\",\"type\":\"string\",\"enum\":[\"true\",\"false\"],\"default\":\"false\"},\"GITSYNC_PASSWORD_FIELD_IN_SECRET_STORE\":{\"description\":\"Name of the field in the DSH secrets store that contains the Git Sync password.\",\"type\":\"string\"},\"GITSYNC_PERIOD\":{\"description\":\"the number of seconds between syncs.\",\"type\":\"string\",\"default\":\"60s\"},\"GITSYNC_REF\":{\"description\":\"git branch to pull in.\",\"type\":\"string\",\"default\":\"main\"},\"GITSYNC_REPO\":{\"description\":\"url of the git repo to sync.\",\"type\":\"string\",\"default\":\"\"},\"GITSYNC_USERNAME\":{\"description\":\"username of the user to pull in the git repo. In github the PAT indicates the username, so username will be oauth2.\",\"type\":\"string\",\"default\":\"oauth2\"},\"TENANT_AIRFLOW_ADMIN_USER_PASSWORD_FIELD_IN_SECRET_STORE\":{\"description\":\"Name of the field in the DSH secrets store that contains the airflow admin user password.\",\"type\":\"string\"}}},\"resources\":{\"allocation/${@tenant}/application/${@name}\":{\"cpus\":\"${AIRFLOW_CPU | number}\",\"env\":{\"AIRFLOW__CORE__LOAD_EXAMPLES\":\"false\",\"AIRFLOW__SCHEDULER__SCHEDULER_ZOMBIE_TASK_THRESHOLD\":\"600\",\"APPLICATION_NAME\":\"${@name}\",\"DSH_GIT_SYNC_ENABLED\":\"true\",\"GITSYNC_LINK\":\"${GITSYNC_LINK}\",\"GITSYNC_ONE_TIME\":\"${GITSYNC_ONE_TIME}\",\"GITSYNC_PASSWORD_FILE\":\"/home/dsh/git-pat-sdp\",\"GITSYNC_PERIOD\":\"${GITSYNC_PERIOD}\",\"GITSYNC_REF\":\"${GITSYNC_REF}\",\"GITSYNC_REPO\":\"${GITSYNC_REPO}\",\"GITSYNC_ROOT\":\"/tmp/git\",\"GITSYNC_USERNAME\":\"${GITSYNC_USERNAME}\"},\"exposedPorts\":{\"8080\":{\"auth\":\"system-fwd-auth@view,manage\",\"vhost\":\"{ vhost('${@name}.${@tenant}','private') }\"}},\"image\":\"${@appcatalog}/release/kpn/airflow-ephemeral:3.0.3\",\"imageConsole\":\"registry.cp.kpn-dsh.com/dex-dev/airflow-ephemeral:3.0.3\",\"instances\":1,\"mem\":\"${AIRFLOW_MEMORY | number}\",\"name\":\"${@name}\",\"needsToken\":true,\"secrets\":[{\"injections\":[{\"env\":\"GH_PAT_TOKEN\"}],\"name\":\"${GITSYNC_PASSWORD_FIELD_IN_SECRET_STORE}\"},{\"injections\":[{\"env\":\"TENANT_AIRFLOW_ADMIN_USER_PASSWORD\"}],\"name\":\"${TENANT_AIRFLOW_ADMIN_USER_PASSWORD_FIELD_IN_SECRET_STORE}\"}],\"singleInstance\":true,\"user\":\"${@uid}:${@gid}\"},\"allocation/${@tenant}/vhost/${@name}.${@tenant}@private\":\"${@name}.${@tenant}@private\"}}"
  },
  {
    "draft": false,
    "lastModified": 1748355221937.0,
    "payload": "{\"id\":\"kpn/topic-metrics-exporter\",\"name\":\"Topic Metrics Exporter\",\"version\":\"0.1.5\",\"vendor\":\"KPN\",\"kind\":\"manifest\",\"apiVersion\":\"v0-alpha\",\"description\":\"Application to export Kafka Topic metrics like earliest_offset, latest_offset, size (delta) and consumer lag per topic/partition per partitition to Prometheus. Metric names are as following: `kafka_topic_earliest_offset`, `kafka_topic_latest_offset`, `kafka_topic_offset_delta`, `kafka_consumer_group_lag`. \",\"moreInfo\":\"## Topic Metrics Exporter \\n\\n  \\n\\n **1. Export Interval Seconds**: Interval in seconds at which metrics are exported. The default interval and min value for the interval is 60 secs. Set the interval using `EXPORT_INTERVAL_SECS`. \\n\\n **2. Scaling**: Use `CPU` \\u0026 `MEMORY` values for vertical scaling. \\n\\n ## More Information \\n The Unibox Team can be reached at unibox@kpn.com for more information.\",\"contact\":\"unibox@kpn.com\",\"configuration\":{\"$schema\":\"https://json-schema.org/draft/2019-09/schema\",\"type\":\"object\",\"properties\":{\"CPU\":{\"description\":\"Amount of CPU cores per instance.\",\"type\":\"string\",\"default\":\"0.1\"},\"EXPORT_INTERVAL_SECS\":{\"description\":\"Interval in seconds at which metrics are exported. The default interval and min value for the interval is 60 secs.\",\"type\":\"string\",\"default\":\"60\"},\"LOG_LEVEL\":{\"description\":\"The log level for the application. The default is INFO.\",\"type\":\"string\",\"enum\":[\"TRACE\",\"DEBUG\",\"INFO\",\"WARN\",\"ERROR\"],\"default\":\"INFO\"},\"MEMORY\":{\"description\":\"Amount of memory per instance.\",\"type\":\"string\",\"default\":\"12\"}}},\"resources\":{\"allocation/${@tenant}/application/${@name}\":{\"cpus\":\"${CPU | number}\",\"env\":{\"EXPORT_INTERVAL_SECS\":\"${EXPORT_INTERVAL_SECS}\",\"RUST_LOG\":\"${LOG_LEVEL}\"},\"image\":\"${@appcatalog}/release/kpn/topic-metrics-exporter:0.1.5\",\"imageConsole\":\"registry.cp.kpn-dsh.com/connectors/topic-metrics-exporter:0.1.5\",\"instances\":1,\"mem\":\"${MEMORY | number}\",\"metrics\":{\"path\":\"/metrics\",\"port\":8080},\"name\":\"${@name}\",\"needsToken\":true,\"singleInstance\":false,\"user\":\"${@uid}:${@gid}\"}}}"
  },
  {
    "draft": false,
    "lastModified": 1722855777075.0,
    "payload": "{\"id\":\"kpn/http-source-connector\",\"name\":\"HTTP Source Connector\",\"version\":\"0.6.0\",\"vendor\":\"KPN\",\"kind\":\"manifest\",\"apiVersion\":\"v0-alpha\",\"description\":\"Application to produce data to Kafka via HTTP in DSH\",\"moreInfo\":\"## HTTP Source Kafka Connector with Oauth2 \\n\\n HTTP Source Kafka Connector with OAuth2 enables streaming data to DSH Kafka via HTTP with Digital Engine's AuthZ OAuth2 authentication. \\n\\n **1. HTTP Endpoint (Vhost)**: This will be automatically created during deployment based on the application name. It can be made `public` (kpn.com) or `private` (kpn.org) using the `VHOST_DNS_ZONE` environment variable. \\n\\n **2. OAuth2 Authentication**: You can choose the authz environment via `AUTHZ_ENVIRONMENT`. If you choose ACC (acceptance), the base url will be  https://api.acc.kpn.com and for PROD (production) it will be https://api.kpn.com . The fetching token endpoint for these urls is _oauth2/v1/token_ and validating token endpoint is _authz/v1/validate_. These will be arranged inside application according to `AUTHZ_ENVIRONMENT`. Authz Client ID, and Authz Client Secret should be stored in Secrets. These secret names must be specified in the deployment page. `AUTHZ_CLIENT_ID_IN_SECRET_STORE` and `AUTHZ_CLIENT_SECRET_IN_SECRET_STORE` are the secret names for the client ID and client secret, respectively. Please make sure you are using correct client-id and client-secret for your Authz environment. `AUTHZ_SCOPE` is for scope name for your application. Authorization is ensured via scopes which validates if the authenticated user has permission to produce data. Multiple scopes can be entered comma seperated. \\n\\n **3. URL Endpoint - Destination Topic - Data Format mapping string**: This establishes multiple endpoints for sending data and maps destination topics and data formats using a colon-separated format. You need to decide what to name your endpoint which you will use in your application to send data. The topic needs to be created in Topics section. Multiple mappings should be separated by commas. This could be set using the `OUTPUT_TOPIC_FORMATS` environment variable and it should be in the format of `\\u003cendpoint1\\u003e:\\u003ctopic1\\u003e:\\u003cformat1\\u003e`. The format can be `json`, `avro`, `protobuf`, `byte`, or `legacy`. JSON, Avro, and Protobuf correspond to the respective data formats (We support avro-byte encoding and we use Confluent standard for Avro \\u0026 Protobuf serialization). Byte is for raw byte data. Legacy format is for the old data format which adds a metadata wrapper with client ID around JSON data. Example - 'publish-json:scratch.json-topic.tenant_name:json'. \\n\\n **4. Dead Letter Queue**: Topic for storing failed messages during serialization/deserialization and can be configured using `DEAD_LETTER_TOPIC`. \\n\\n **5. Scaling**: Use `INSTANCES` count for horizontal scaling and `CPU` \\u0026 `MEMORY` values for vertical scaling. \\n\\n ## More Information \\n The Unibox Team can be reached at unibox@kpn.com for more information.\",\"contact\":\"unibox@kpn.com\",\"configuration\":{\"$schema\":\"https://json-schema.org/draft/2019-09/schema\",\"type\":\"object\",\"properties\":{\"AUTHZ_CLIENT_ID_IN_SECRET_STORE\":{\"description\":\"Name of the field in the DSH secrets store that contains the Authz Client ID.\",\"type\":\"string\"},\"AUTHZ_CLIENT_SECRET_IN_SECRET_STORE\":{\"description\":\"Name of the field in the DSH secrets store that contains the Authz Client Secret.\",\"type\":\"string\"},\"AUTHZ_ENVIRONMENT\":{\"description\":\"The environment value for Authz Digital Engine.\",\"type\":\"string\",\"enum\":[\"ACC\",\"PROD\"],\"default\":\"ACC\"},\"AUTHZ_SCOPE\":{\"description\":\"DE application scopes for authorization purposes\",\"type\":\"string\",\"default\":\"scope1,scope2\"},\"CPU\":{\"description\":\"Amount of CPU cores per instance.\",\"type\":\"string\",\"default\":\"0.1\"},\"DEAD_LETTER_TOPIC\":{\"description\":\"Topic for failed messages during serialization/deseralization\",\"type\":\"string\",\"default\":\"scratch.dlq.tenant\"},\"INSTANCES\":{\"description\":\"The number of instances to deploy for scaling. The default is 1.\",\"type\":\"string\",\"enum\":[\"1\",\"2\",\"3\",\"4\",\"5\"],\"default\":\"1\"},\"LOG_LEVEL\":{\"description\":\"The log level for the application. The default is INFO.\",\"type\":\"string\",\"enum\":[\"TRACE\",\"DEBUG\",\"INFO\",\"WARN\",\"ERROR\"],\"default\":\"INFO\"},\"MEMORY\":{\"description\":\"Amount of memory per instance.\",\"type\":\"string\",\"default\":\"64\"},\"OUTPUT_TOPIC_FORMATS\":{\"description\":\"Define endpoint, topic and data format mapping to declare where to publish data to. Mapping format is like following, endpoint:topic_name:data_format can be added more sepearting with comma.\",\"type\":\"string\",\"default\":\"endpoint1:scratch.topic1.tenant:byte\"},\"VHOST_DNS_ZONE\":{\"description\":\"DNS Zone selection for Vhost (kpn.com or kpn.org)\",\"type\":\"string\",\"enum\":[\"public\",\"private\"],\"default\":\"private\"}}},\"resources\":{\"allocation/${@tenant}/application/${@name}\":{\"cpus\":\"${CPU | number}\",\"env\":{\"AUTHZ_ENVIRONMENT\":\"${AUTHZ_ENVIRONMENT}\",\"AUTHZ_SCOPE\":\"${AUTHZ_SCOPE}\",\"DEAD_LETTER_TOPIC\":\"${DEAD_LETTER_TOPIC}\",\"ENABLE_OAUTH2\":\"true\",\"LOG_LEVEL\":\"${LOG_LEVEL}\",\"OUTPUT_TOPIC_FORMATS\":\"${OUTPUT_TOPIC_FORMATS}\"},\"exposedPorts\":{\"3000\":{\"vhost\":\"{ vhost('${@name}.${@tenant}', '${VHOST_DNS_ZONE}') }\"}},\"image\":\"${@appcatalog}/release/kpn/http-kafka-connector:0.6.0\",\"imageConsole\":\"registry.cp.kpn-dsh.com/greenbox-training/http-kafka-connector:0.6.0\",\"instances\":\"${INSTANCES | number}\",\"mem\":\"${MEMORY | number}\",\"metrics\":{\"path\":\"/metrics\",\"port\":9090},\"name\":\"${@name}\",\"needsToken\":true,\"secrets\":[{\"injections\":[{\"env\":\"AUTHZ_CLIENT_ID\"}],\"name\":\"${AUTHZ_CLIENT_ID_IN_SECRET_STORE}\"},{\"injections\":[{\"env\":\"AUTHZ_CLIENT_SECRET\"}],\"name\":\"${AUTHZ_CLIENT_SECRET_IN_SECRET_STORE}\"}],\"singleInstance\":false,\"user\":\"${@uid}:${@gid}\"},\"allocation/${@tenant}/vhost/${@name}.${@tenant}@${VHOST_DNS_ZONE}\":\"${@name}.${@tenant}@${VHOST_DNS_ZONE}\"}}"
  },
  {
    "draft": false,
    "lastModified": 1706273912736.0,
    "payload": "{\"id\":\"kpn/schema-store-ui\",\"name\":\"Schema Store UI (beta)\",\"version\":\"0.0.13\",\"vendor\":\"KPN\",\"kind\":\"manifest\",\"apiVersion\":\"v0-alpha\",\"description\":\"Swagger UI documentation of the Schema Store API\",\"moreInfo\":\"\",\"contact\":\"dsh-appcatalog@kpn.com\",\"configuration\":{\"$schema\":\"https://json-schema.org/draft/2019-09/schema\",\"type\":\"object\",\"properties\":{\"DNS_ZONE\":{\"description\":\"DNS zone for the vhost\",\"type\":\"string\",\"enum\":[\"public\",\"private\"],\"default\":\"public\"}}},\"resources\":{\"allocation/${@tenant}/application/${@name}\":{\"cpus\":0.1,\"env\":{\"SR_DOMAIN\":\"api.schema-store.dsh.marathon.mesos\",\"SR_PATH\":\"/\",\"SR_PORT\":\"8443\"},\"exposedPorts\":{\"8080\":{\"auth\":\"system-fwd-auth@view,manage\",\"vhost\":\"{ vhost('${@name}.${@tenant}','${DNS_ZONE}') }\"}},\"image\":\"${@appcatalog}/release/kpn/schema-store-ui:0.0.13\",\"instances\":1,\"mem\":256,\"name\":\"${@name}\",\"needsToken\":true,\"singleInstance\":false,\"user\":\"${@uid}:${@gid}\"},\"allocation/${@tenant}/vhost/${@name}.${@tenant}@public\":\"${@name}.${@tenant}@${DNS_ZONE}\"}}"
  },
  {
    "draft": false,
    "lastModified": 1724765556810.0,
    "payload": "{\"id\":\"kpn/schema-store-ui\",\"name\":\"Schema Store UI (beta)\",\"version\":\"0.0.15\",\"vendor\":\"KPN\",\"kind\":\"manifest\",\"apiVersion\":\"v0-alpha\",\"description\":\"Swagger UI documentation of the Schema Store API\",\"moreInfo\":\"\",\"contact\":\"dsh-appcatalog@kpn.com\",\"configuration\":{\"$schema\":\"https://json-schema.org/draft/2019-09/schema\",\"type\":\"object\",\"properties\":{\"DNS_ZONE\":{\"description\":\"DNS zone for the vhost\",\"type\":\"string\",\"enum\":[\"public\",\"private\"],\"default\":\"public\"}}},\"resources\":{\"allocation/${@tenant}/application/${@name}\":{\"cpus\":0.1,\"env\":{\"SR_DOMAIN\":\"api.schema-store.dsh.marathon.mesos\",\"SR_PATH\":\"/\",\"SR_PORT\":\"8443\"},\"exposedPorts\":{\"8080\":{\"auth\":\"system-fwd-auth@view,manage\",\"vhost\":\"{ vhost('${@name}.${@tenant}','${DNS_ZONE}') }\"}},\"image\":\"${@appcatalog}/release/kpn/schema-store-ui:0.0.15-RC.pgr.20240819.1\",\"instances\":1,\"mem\":256,\"name\":\"${@name}\",\"needsToken\":true,\"singleInstance\":false,\"user\":\"${@uid}:${@gid}\"},\"allocation/${@tenant}/vhost/${@name}.${@tenant}@public\":\"${@name}.${@tenant}@${DNS_ZONE}\"}}"
  },
  {
    "draft": false,
    "lastModified": 1725957527089.0,
    "payload": "{\"id\":\"kpn/kafka-data-archiver\",\"name\":\"Kafka Data Archiver\",\"version\":\"1.4.0\",\"vendor\":\"KPN\",\"kind\":\"manifest\",\"apiVersion\":\"v0-alpha\",\"description\":\"Service to store kafka messages in an object storage like s3 and adls gen2\",\"moreInfo\":\"## Kafka Data Archiver (KDA)\\nA dsh service to store kafka messages in an object storage like S3 or ADLS Gen2. It also allows to convert kafka message into another format using schemas.\\n\\nTo run a KDA, an application configuration should be provided (stored in dsh secrets).\\nHere is an example application configuration file for storing json messages from kafka into AWS S3 bucket as apache parquet format;\\n```\\ninclude \\\"base_application.conf\\\" # Contains application configuration with their default values\\ninclude \\\"base_kafka.conf\\\" # Contains kafka configuration with default values\\n\\nkafka.consumerBootstrapServersOverride = false\\n\\nkafka.consumerBootstrapServers = []\\n\\nkafka.consumerBootstrapServersOverride = false\\n\\nkafka.producerBootstrapServers = []\\nkafka.readTopics = [\\n  \\\"input_topic1\\\"\\n]\\nkafka.groupId = \\\"example_groupid_1\\\"\\n\\napp.maxMessageCountToUpload = 1000 # This is the threshold for number of messages written in a local file before uploading to object storage.\\napp.timeIntervalToUpload = 60.seconds # This is timeout threshold to upload file to object storage.\\npartitionerType = \\\"timebased\\\" # If you want KDA to create timebased partition folders in object storage.\\npartitionerFormat = \\\"yyyy-MM-dd\\\" # Timebased Partition format. (Reference: https://docs.oracle.com/en/java/javase/17/docs/api/java.base/java/time/format/DateTimeFormatter.html#patterns))\\nparquet.compression = \\\"snappy\\\"\\n\\n# Topic input format mapping. You can define multiple topic which contains different message formats.\\ntopic.inputFormat { \\n  \\\"input_topic1\\\": json\\n}\\n\\n# Topic ouput format mapping. You can define multiple topics with is stored in different formats.\\ntopic.outputFormat {\\n  \\\"input_topic1\\\": parquet\\n}\\n\\n# Topic schema id mapping.\\ntopic.schemaIds {\\n  \\\"input_topic1\\\": \\\"schema-v1.avsc\\\"\\n}\\n\\n# Topic s3 bucket mapping. It's possible to map each topic to different s3 bucket and/or prefix.\\ntopic.s3Buckets {\\n  \\\"input_topic1\\\" {\\n    bucket: \\\"bucket1\\\"\\n    prefix: \\\"archived\\\"\\n  }\\n}\\n\\nstorage.type = S3 # Valid values S3 or ADLS_GEN_2\\n\\n# Schema Registry Url is needed if message conversion if needed. \\n# Schemas can be stored in s3, adls_gen2 or http service\\nschemaRegistry.url = \\\"s3://bucket1/schemas\\\"\\n\\n# Define aws_access_key_id for each bucket\\nstorage.s3.accessKeyIds {\\n  \\\"bucket1\\\": \\\"value of aws_access_key_id\\\"\\n}\\n\\n# Define aws_secret_key for each bucket\\nstorage.s3.secretKeys {\\n  \\\"bucket1\\\": \\\"value of aws_secret_key\\\"\\n}\\n\\n# Alternatively (do not use both!) if storage.type = adls_gen2\\nschemaRegistry.url = \\\"abfss://testcontainer@testaccount.dfs.core.windows.net/schemas\\\"\\n\\ntopic.adlsAccountNames {\\n  \\\"input_topic2\\\": {\\n    accountName: \\\"testaccount\\\"\\n    container: \\\"testcontainer\\\"\\n    prefix: \\\"archived\\\"\\n  }\\n}\\n\\n# Each input_topic needs to have associated credentials, either on account level or SAS keys\\nstorage.adls.accountKeys {\\n  \\\"testaccount\\\": \\\"\\u003cACCOUNT KEY\\u003e\\\"\\n}\\n\\nstorage.adls.sasKeys {\\n  \\\"testaccount\\\": {\\n    \\\"test\\\": {\\n      \\\"schemas\\\": \\\"\\u003cSAS TOKEN\\u003e\\\"\\n    }\\n    \\\"test\\\": {\\n      \\\"archived\\\": \\\"\\u003cSAS TOKEN\\u003e\\\"\\n    }\\n  }\\n}\\n\\n# What to do with messages that cannot be processed (parsing fails with schema)\\nerror.tolerance = \\\"ignore\\\"\\nerror.routing {\\n  \\\"input_topic1\\\": \\\"dead-letter-queue1\\\"\\n}\\n```\\n\",\"contact\":\"dsh-lfm@kpn.com\",\"configuration\":{\"$schema\":\"https://json-schema.org/draft/2019-09/schema\",\"type\":\"object\",\"properties\":{\"APP_CONFIG_SECRET_NAME\":{\"description\":\"Name of the dsh secret which contains kda application configuration.\",\"type\":\"string\"},\"CPU\":{\"description\":\"Amount of CPU cores per instance.\",\"type\":\"string\",\"default\":\"0.1\"},\"INSTANCES\":{\"description\":\"The number of instances to deploy in distributed mode (cluster). The default is 1.\",\"type\":\"string\",\"default\":\"1\"},\"JVM_MAX_RAM_PERCENTAGE\":{\"description\":\"Sets -XX:MaxRAMPercentage JVM argument\",\"type\":\"string\",\"default\":\"80\"},\"LOG_LEVEL\":{\"description\":\"\",\"type\":\"string\",\"enum\":[\"TRACE\",\"DEBUG\",\"INFO\",\"WARN\",\"ERROR\"],\"default\":\"INFO\"},\"MEMORY\":{\"description\":\"Amount of memory per instance.\",\"type\":\"string\",\"default\":\"512\"}}},\"resources\":{\"allocation/${@tenant}/application/${@name}\":{\"cpus\":\"${CPU | number}\",\"env\":{\"ARCHIVER_LOG_LEVEL\":\"${LOG_LEVEL}\",\"DSH_LOG_LEVEL\":\"${LOG_LEVEL}\",\"MAIN_LOG_LEVEL\":\"${LOG_LEVEL}\",\"MAX_RAM_PERCENTAGE\":\"${JVM_MAX_RAM_PERCENTAGE}\"},\"image\":\"${@appcatalog}/release/kpn/kafka-data-archiver:20240910.1\",\"imageConsole\":\"registry.cp.kpn-dsh.com/kpn-lfm-01/kafka-data-archiver_prod:20240910.1\",\"instances\":\"${INSTANCES | number}\",\"mem\":\"${MEMORY | number}\",\"metrics\":{\"path\":\"/metrics\",\"port\":9090},\"name\":\"${@name}\",\"needsToken\":true,\"secrets\":[{\"injections\":[{\"env\":\"APP_CONF\"}],\"name\":\"${APP_CONFIG_SECRET_NAME}\"}],\"singleInstance\":false,\"user\":\"${@uid}:${@gid}\"}}}"
  },
  {
    "draft": false,
    "lastModified": 1708705798986.0,
    "payload": "{\"id\":\"kpn/keyring-kafka-database-extractor\",\"name\":\"DSH Database Extractor\",\"version\":\"0.4.1\",\"vendor\":\"KPN\",\"kind\":\"manifest\",\"apiVersion\":\"v0-alpha\",\"description\":\"Application to bulk load/sync records from a relational database to Kafka in DSH\",\"moreInfo\":\"## Database Extractor for Apache Kafka in DSH \\nAn application to import data from any relational database with a JDBC driver into an Apache Kafka topic by extending the [Kafka Connect JDBC Source connector](https://docs.confluent.io/current/connect/kafka-connect-jdbc/source-connector/index.html) project. \\n\\n**1. Data Source:** Currently, we support Teradata, Postgres (includes Yugabyte), MySQL, Oracle, SQLite, SAP Sybase, Microsoft SQL Server and IBM DB2.\\n\\n**2. Kafka JDBC Database Extractor Application:** The application can be deployed with single or multiple workers. Cluster mode ensures fault tolerancy and HA. The data extraction mode can be configured by setting the `MODE` environment variable. These can be bulk, timestamp, incrementing or timestamp+incrementing. Furthermore, the `POLL_INTERVAL` environment variable can be used to determine the degree of realtime sync or periodicity of sync. \\n\\n**3. Data Sink:** The data is stored in JSON format in the kafka topic `DATABASE_OUTPUT_TOPIC`.\\n ### More Information\\nThe _Greenbox Team_ can be reached at **greenbox@kpn.com** for more information.\",\"contact\":\"greenbox@kpn.com\",\"configuration\":{\"$schema\":\"https://json-schema.org/draft/2019-09/schema\",\"type\":\"object\",\"properties\":{\"CPU\":{\"description\":\"Amount of CPU cores per instance.\",\"type\":\"string\",\"default\":\"0.1\"},\"DATABASE\":{\"description\":\"The database to connect to. Currently, we support Teradata, Postgres (includes Yugabyte), MySQL, Oracle, SQLite, SAP Sybase, Microsoft SQL Server and IBM DB2.\",\"type\":\"string\",\"enum\":[\"teradata\",\"postgresql\",\"mysql\",\"oracle\",\"sqlite\",\"sybase\",\"sqlserver\",\"db2\"],\"default\":\"teradata\"},\"DATABASE_JDBC_URL\":{\"description\":\"The database host to connect to. Ensure that the host is accessible from the application. Example: jdbc:teradata://tdp.kpn.org\",\"type\":\"string\",\"default\":\"jdbc:teradata://tdp.kpn.org\"},\"DATABASE_OUTPUT_TOPIC\":{\"description\":\"Name of the Kafka topic to write the records to.\",\"type\":\"string\",\"default\":\"scratch.database-extractor-output.greenbox\"},\"DATABASE_PASSWORD_FIELD_IN_SECRET_STORE\":{\"description\":\"Name of the field in the DSH secrets store that contains the Database password.\",\"type\":\"string\"},\"DATABASE_QUERY\":{\"description\":\"The SQL query to execute. The query should be a valid SQL query for the database specified in the `DATABASE` environment variable.\",\"type\":\"string\",\"default\":\"SELECT * FROM \\u003ctable_name\\u003e\"},\"DATABASE_TIMEZONE\":{\"description\":\"Used to recognize the timezone of timestamp values. The default is UTC. KPN Teradata uses Europe/Amsterdam.\",\"type\":\"string\",\"default\":\"UTC\"},\"DATABASE_USER\":{\"description\":\"Database user name. Make sure this user has the right permissions to execute the query.\",\"type\":\"string\"},\"INCREMENTING_COLUMN_NAME\":{\"description\":\"This is used in the `incrementing` and `timestamp+incrementing` extraction modes. Only relevant for these two modes. The column name should be a valid autoincrementing column in the database table.\",\"type\":\"string\",\"default\":\"\"},\"INSTANCES\":{\"description\":\"The number of instances to deploy in distributed mode (cluster). The default is 1.\",\"type\":\"string\",\"enum\":[\"1\",\"2\",\"3\",\"4\",\"5\"],\"default\":\"1\"},\"LOG_LEVEL\":{\"description\":\"The log level for the application. The default is INFO.\",\"type\":\"string\",\"enum\":[\"TRACE\",\"DEBUG\",\"INFO\",\"WARN\",\"ERROR\"],\"default\":\"INFO\"},\"MEMORY\":{\"description\":\"Amount of memory per instance.\",\"type\":\"string\",\"default\":\"1024\"},\"MODE\":{\"description\":\"The data extraction mode can be `bulk`, `timestamp`, `incrementing` and `timestamp+incrementing`. Bulk is used for batch extraction whereas the other modes are used for realtime extraction.\",\"type\":\"string\",\"enum\":[\"bulk\",\"timestamp\",\"incrementing\",\"timestamp+incrementing\"],\"default\":\"timestamp\"},\"POLL_INTERVAL\":{\"description\":\"Interval in milliseconds for polling (24 hours is 86400000 milliseconds).\",\"type\":\"string\",\"default\":\"60000\"},\"TIMESTAMP_COLUMN_NAME\":{\"description\":\"This is used in the `timestamp` and `timestamp+incrementing` extraction modes. Only relevant for these two modes. The column name should be a valid date/timestamp column in the database table.\",\"type\":\"string\",\"default\":\"\"}}},\"resources\":{\"allocation/${@tenant}/application/${@name}\":{\"cpus\":\"${CPU | number}\",\"env\":{\"DATABASE\":\"${DATABASE}\",\"DATABASE_JDBC_URL\":\"${DATABASE_JDBC_URL}\",\"DATABASE_OUTPUT_TOPIC\":\"${DATABASE_OUTPUT_TOPIC}\",\"DATABASE_QUERY\":\"${DATABASE_QUERY}\",\"DATABASE_TIMEZONE\":\"${DATABASE_TIMEZONE}\",\"DATABASE_USER\":\"${DATABASE_USER}\",\"DEPLOYMENT\":\"distributed\",\"DISTRIBUTED_MODE_CONFIG_TOPIC\":\"scratch.${@name}-config.${@tenant}\",\"DISTRIBUTED_MODE_OFFSETS_TOPIC\":\"scratch.${@name}-offset.${@tenant}\",\"DISTRIBUTED_MODE_STATUS_TOPIC\":\"scratch.${@name}-status.${@tenant}\",\"INCREMENTING_COLUMN_NAME\":\"${INCREMENTING_COLUMN_NAME}\",\"KAFKA_OPTS\":\"-Xms256M\",\"LOG_LEVEL\":\"${LOG_LEVEL}\",\"MODE\":\"${MODE}\",\"POLL_INTERVAL\":\"${POLL_INTERVAL}\",\"TIMESTAMP_COLUMN_NAME\":\"${TIMESTAMP_COLUMN_NAME}\"},\"image\":\"${@appcatalog}/release/kpn/keyring-kafka-database-extractor:0.4.1\",\"imageConsole\":\"registry.cp.kpn-dsh.com/greenbox/keyring-kafka-database-extractor:0.4.1\",\"instances\":\"${INSTANCES | number}\",\"mem\":\"${MEMORY | number}\",\"metrics\":{\"path\":\"/\",\"port\":9009},\"name\":\"${@name}\",\"needsToken\":true,\"secrets\":[{\"injections\":[{\"env\":\"DATABASE_PASSWORD\"}],\"name\":\"${DATABASE_PASSWORD_FIELD_IN_SECRET_STORE}\"}],\"singleInstance\":false,\"user\":\"${@uid}:${@gid}\"},\"allocation/${@tenant}/application/${@name}-ui\":{\"cpus\":0.1,\"env\":{\"CONNECT_URL\":\"${@name}:8083\",\"PORT\":\"8000\"},\"exposedPorts\":{\"8000\":{\"auth\":\"system-fwd-auth@view,manage\",\"tls\":\"auto\",\"vhost\":\"{ vhost('${@name}.${@tenant}', 'public') }\"}},\"image\":\"${@appcatalog}/release/kpn/keyring-kafka-database-extractor-ui:0.4.1\",\"imageConsole\":\"registry.cp.kpn-dsh.com/greenbox/keyring-kafka-database-extractor-ui:0.4.1\",\"instances\":1,\"mem\":8,\"name\":\"${@name}\",\"needsToken\":false,\"singleInstance\":false,\"user\":\"${@uid}:${@gid}\"},\"allocation/${@tenant}/topic/${@name}-config\":{\"kafkaProperties\":{\"cleanup.policy\":\"compact\"},\"name\":\"${@name}-config\",\"partitions\":1,\"replicationFactor\":3},\"allocation/${@tenant}/topic/${@name}-offset\":{\"kafkaProperties\":{\"cleanup.policy\":\"compact\"},\"name\":\"${@name}-offset\",\"partitions\":1,\"replicationFactor\":3},\"allocation/${@tenant}/topic/${@name}-status\":{\"kafkaProperties\":{\"cleanup.policy\":\"compact\"},\"name\":\"${@name}-status\",\"partitions\":1,\"replicationFactor\":3},\"allocation/${@tenant}/vhost/${@name}.${@tenant}@public\":\"${@name}.${@tenant}@public\"}}"
  },
  {
    "draft": false,
    "lastModified": 1698161398954.0,
    "payload": "{\"id\":\"kpn/secor\",\"name\":\"Secor\",\"version\":\"0.30.2\",\"vendor\":\"KPN\",\"kind\":\"manifest\",\"apiVersion\":\"v0-alpha\",\"description\":\"Secor is a service persisting Kafka logs to Amazon S3, Google Cloud Storage, Microsoft Azure Blob Storage and Openstack Swift.\",\"moreInfo\":\"## Key features ##\\n  - **strong consistency**: as long as [Kafka] is not dropping messages (e.g., due to aggressive cleanup policy) before Secor is able to read them, it is guaranteed that each message will be saved in exactly one [S3] file. This property is not compromised by the notorious temporal inconsistency of [S3] caused by the [eventual consistency] model,\\n  - **fault tolerance**: any component of Secor is allowed to crash at any given point without compromising data integrity,\\n  - **load distribution**: Secor may be distributed across multiple machines,\\n  - **horizontal scalability**: scaling the system out to handle more load is as easy as starting extra Secor processes. Reducing the resource footprint can be achieved by killing any of the running Secor processes. Neither ramping up nor down has any impact on data consistency,\\n  - **output partitioning**: Secor parses incoming messages and puts them under partitioned s3 paths to enable direct import into systems like [Hive]. day,hour,minute level partitions are supported by secor\\n  - **configurable upload policies**: commit points controlling when data is persisted in S3 are configured through size-based and time-based policies (e.g., upload data when local buffer reaches size of 100MB and at least once per hour),\\n  - **monitoring**: metrics tracking various performance properties are exposed through [Ostrich], [Micrometer] and optionally exported to [OpenTSDB] / [statsD],\\n  - **customizability**: external log message parser may be loaded by updating the configuration,\\n  - **event transformation**: external message level transformation can be done by using customized class.\\n  - **Qubole interface**: Secor connects to [Qubole] to add finalized output partitions to Hive tables.\\n## Github \\n https://github.com/pinterest/secor \\n\\n## Release Notes:  \\n* first release\",\"contact\":\"dsh-appcatalog@kpn.com\",\"configuration\":{\"$schema\":\"https://json-schema.org/draft/2019-09/schema\",\"type\":\"object\",\"properties\":{\"AWS_ACCESS_KEY\":{\"description\":\"aws.access.key\",\"type\":\"string\",\"default\":\"system/objectstore/access_key_id\"},\"AWS_REGION\":{\"description\":\"aws.region\",\"type\":\"string\",\"default\":\"eu-central-1\"},\"AWS_SECOR_S3_BUCKET_REQUIRED\":{\"description\":\"secor.s3.bucket dev-dsh-dshtest-secor-demo\",\"type\":\"string\",\"default\":\"\"},\"AWS_SECRET_KEY\":{\"description\":\"aws.secret.key\",\"type\":\"string\",\"default\":\"system/objectstore/secret_access_key\"},\"CLOUD_SERVICE\":{\"description\":\"cloud.service\",\"type\":\"string\",\"enum\":[\"S3\",\"Azure\"],\"default\":\"S3\"},\"SECOR_AZURE_ACCOUNT_KEY\":{\"description\":\"secor.azure.account.key\",\"type\":\"string\",\"default\":\"system/objectstore/secret_access_key\"},\"SECOR_AZURE_ACCOUNT_NAME\":{\"description\":\"secor.azure.account.name\",\"type\":\"string\",\"default\":\"\"},\"SECOR_AZURE_CONTAINER_NAME\":{\"description\":\"secor.azure.container.name\",\"type\":\"string\",\"default\":\"\"},\"SECOR_AZURE_ENDPOINTS_PROTOCOL\":{\"description\":\"secor.azure.endpoints.protocol\",\"type\":\"string\",\"default\":\"https\"},\"SECOR_AZURE_PATH\":{\"description\":\"secor.azure.path\",\"type\":\"string\",\"default\":\"data\"},\"SECOR_FILE_READER_WRITER_FACTORY\":{\"description\":\"secor.file.reader.writer.factory\",\"type\":\"string\",\"enum\":[\"com.pinterest.secor.io.impl.AvroFileReaderWriterFactory\",\"com.pinterest.secor.io.impl.DelimitedTextFileReaderWriterFactory\",\"com.pinterest.secor.io.impl.SequenceFileReaderWriterFactory\",\"com.pinterest.secor.io.impl.JsonORCFileReaderWriterFactory\",\"com.pinterest.secor.io.impl.ProtobufParquetFileReaderWriterFactory\",\"com.pinterest.secor.io.impl.ThriftParquetFileReaderWriterFactory\",\"com.pinterest.secor.io.impl.AvroParquetFileReaderWriterFactory\"],\"default\":\"com.pinterest.secor.io.impl.DelimitedTextFileReaderWriterFactory\"},\"SECOR_KAFKA_TOPIC_FILTER\":{\"description\":\"secor.kafka.topic_filter scratch.secor-demo.dshtest\",\"type\":\"string\",\"default\":\"\"},\"SECOR_MAX_FILE_AGE_SECONDS\":{\"description\":\"secor.max.file.age.seconds\",\"type\":\"string\",\"default\":\"3600\"},\"SECOR_MAX_FILE_SIZE_BYTES\":{\"description\":\"secor.max.file.size.bytes\",\"type\":\"string\",\"default\":\"200000000\"},\"SECOR_MESSAGE_PARSER_CLASS\":{\"description\":\"secor.message.parser.class\",\"type\":\"string\",\"enum\":[\"com.pinterest.secor.parser.JsonMessageParser\",\"com.pinterest.secor.parser.AvroMessageParser\",\"com.pinterest.secor.parser.Iso8601MessageParser\",\"com.pinterest.secor.parser.MessagePackParser\",\"com.pinterest.secor.parser.ProtobufMessageParser\"],\"default\":\"com.pinterest.secor.parser.JsonMessageParser\"},\"SECOR_UPLOAD_MANAGER_CLASS\":{\"description\":\"secor.upload.manager.class\",\"type\":\"string\",\"enum\":[\"com.pinterest.secor.uploader.S3UploadManager\",\"com.pinterest.secor.uploader.AzureUploadManager\"],\"default\":\"com.pinterest.secor.uploader.S3UploadManager\"},\"ZOOKEEPER_QUORUM_REQUIRED\":{\"description\":\"zookeeper name\",\"type\":\"string\",\"default\":\"zk-{zookeeper-name}.{tenant}.marathon.mesos:2181\"}}},\"resources\":{\"allocation/${@tenant}/application/${@name}\":{\"cpus\":0.5,\"env\":{\"PKI_CONFIG_DIR\":\"/tmp\",\"aws.region\":\"${AWS_REGION}\",\"cloud.service\":\"${CLOUD_SERVICE}\",\"kafka.dual.commit.enabled\":\"false\",\"kafka.offsets.storage\":\"kafka\",\"secor.azure.account.name\":\"${SECOR_AZURE_ACCOUNT_NAME}\",\"secor.azure.container.name\":\"${SECOR_AZURE_CONTAINER_NAME}\",\"secor.azure.endpoints.protocol\":\"${SECOR_AZURE_ENDPOINTS_PROTOCOL}\",\"secor.azure.path\":\"${SECOR_AZURE_PATH}\",\"secor.file.reader.writer.factory\":\"${SECOR_FILE_READER_WRITER_FACTORY}\",\"secor.kafka.topic_filter\":\"${SECOR_KAFKA_TOPIC_FILTER}\",\"secor.local.path\":\"/tmp/secor_prod/message_logs/partition\",\"secor.max.file.age.seconds\":\"${SECOR_MAX_FILE_AGE_SECONDS}\",\"secor.max.file.size.bytes\":\"${SECOR_MAX_FILE_SIZE_BYTES}\",\"secor.message.parser.class\":\"${SECOR_MESSAGE_PARSER_CLASS}\",\"secor.s3.bucket\":\"${AWS_SECOR_S3_BUCKET_REQUIRED}\",\"secor.upload.manager.class\":\"${SECOR_UPLOAD_MANAGER_CLASS}\",\"zookeeper.quorum\":\"${ZOOKEEPER_QUORUM_REQUIRED}\"},\"image\":\"${@appcatalog}/release/kpn/secor:0.30.2\",\"instances\":1,\"mem\":2300,\"name\":\"${@name}\",\"needsToken\":true,\"secrets\":[{\"injections\":[{\"env\":\"aws.access.key\"}],\"name\":\"${AWS_ACCESS_KEY}\"},{\"injections\":[{\"env\":\"aws.secret.key\"}],\"name\":\"${AWS_SECRET_KEY}\"},{\"injections\":[{\"env\":\"secor.azure.account.key\"}],\"name\":\"${SECOR_AZURE_ACCOUNT_KEY}\"}],\"singleInstance\":false,\"user\":\"${@uid}:${@gid}\"}}}"
  },
  {
    "draft": false,
    "lastModified": 1691140726486.0,
    "payload": "{\"id\":\"kpn/keyring-service\",\"name\":\"keyring-service\",\"version\":\"0.4.3\",\"vendor\":\"KPN\",\"kind\":\"manifest\",\"apiVersion\":\"v0-alpha\",\"description\":\"KPN keyring service.\",\"moreInfo\":\"# keyring service\\n\\nThe **keyring service** is a service that exposes a REST-service that maps identifiers from a *domain* to a *codomain*. E.g. it can map Boss identifiers to KRN identifiers. The **keyring service** runs in your own tenant environment, under your own control. It optionally provides an OpenAPI specification of the REST-service and a Swagger UI to explore and test the REST-service and it can expose metrics for a Prometheus scraper.\\n\\nBe sure to check the [keyring documentation](https://keyring.dsh-prod.dsh.prod.aws.kpn.org/using/app-catalog.html) for more and important information.\\n\",\"contact\":\"greenbox@kpn.com\",\"configuration\":{\"$schema\":\"https://json-schema.org/draft/2019-09/schema\",\"type\":\"object\",\"properties\":{\"CODOMAINS\":{\"description\":\"select the codomain(s) for the relation separated by commas (allowed codomain names: BOSS, ECID, ETKN, KIDH, KRN, SBCM)\",\"type\":\"string\",\"enum\":[\"BOSS\",\"KRN\",\"SBCM\",\"BOSS,KRN,SBCM\",\"ECID\",\"ETKN\",\"KIDH\",\"ECID,ETKN,KIDH\"]},\"DOMAINS\":{\"description\":\"select the domain(s) for the relation separated by commas (allowed domain names: BOSS, ECID, ETKN, KIDH, KRN, SBCM)\",\"type\":\"string\",\"enum\":[\"BOSS\",\"KRN\",\"SBCM\",\"BOSS,KRN,SBCM\",\"ECID\",\"ETKN\",\"KIDH\",\"ECID,ETKN,KIDH\"]},\"ENABLE_METRICS_EXPORTER\":{\"description\":\"select whether to enable exporting metrics\",\"type\":\"string\",\"enum\":[\"false\",\"true\"],\"default\":\"true\"},\"ENABLE_OPEN_API_SPECIFICATION\":{\"description\":\"select whether to enable the openapi specification\",\"type\":\"string\",\"enum\":[\"false\",\"true\"],\"default\":\"true\"},\"ENABLE_SWAGGER_UI\":{\"description\":\"select whether to enable the swagger ui\",\"type\":\"string\",\"enum\":[\"false\",\"true\"],\"default\":\"true\"}}},\"resources\":{\"allocation/${@tenant}/application/${@name}\":{\"cpus\":1,\"env\":{\"BASE_DOMAINS_CONFIG_FILE\":\"base-domains.conf\",\"CODOMAINS\":\"${CODOMAINS}\",\"DEFAULT_CACHE_TYPE\":\"chronicle\",\"DOMAINS\":\"${DOMAINS}\",\"ENABLE_DERIVED_RELATIONS\":\"true\",\"ENABLE_JVM_METRICS_EXPORTER\":\"${ENABLE_METRICS_EXPORTER}\",\"ENABLE_METRICS_EXPORTER\":\"${ENABLE_METRICS_EXPORTER}\",\"ENABLE_OPEN_API_SPECIFICATION\":\"${ENABLE_OPEN_API_SPECIFICATION}\",\"ENABLE_SWAGGER_UI\":\"${ENABLE_SWAGGER_UI}\",\"GROUP_ID_PREFIX\":\"${@tenant}_\",\"HEAP_MEMORY\":\"384\",\"KEYRING_SERVICE_PORT\":\"8088\",\"LOG_LEVEL\":\"info\",\"LOG_LEVEL_AKKA\":\"error\",\"LOG_LEVEL_CACHE\":\"info\",\"LOG_LEVEL_CODOMAIN_VALUES\":\"info\",\"LOG_LEVEL_ENGINE\":\"info\",\"LOG_LEVEL_ENTRYPOINT\":\"error\",\"LOG_LEVEL_MASTER_RELATION\":\"info\",\"LOG_LEVEL_SERVICE\":\"info\",\"MASTER_RELATIONS_CONFIG_FILE\":\"master-relations.conf\",\"METRICS_EXPORTER_PREFIX\":\"\",\"SWAGGER_UI_CONTACT_NAME\":\"Greenbox Team\",\"SWAGGER_UI_CONTACT_URL\":\"https://confluence.kpn.org/display/KEYR/Keyring\",\"SWAGGER_UI_DOCUMENTATION_URL\":\"https://keyring.dsh-prod.dsh.prod.aws.kpn.org\"},\"exposedPorts\":{\"8088\":{\"auth\":\"system-fwd-auth@view,manage\",\"tls\":\"auto\",\"vhost\":\"{ vhost('${@name}.${@tenant}', 'public') }\"}},\"image\":\"${@appstore}/release/kpn/keyring-service:0.4.3\",\"imageConsole\":\"registry.cp.kpn-dsh.com/greenbox-dev/keyring-service:0.4.3\",\"instances\":1,\"mem\":2048,\"metrics\":{\"path\":\"/\",\"port\":9585},\"name\":\"${@name}\",\"needsToken\":true,\"singleInstance\":false,\"user\":\"${@uid}:${@gid}\"},\"allocation/${@tenant}/vhost/${@name}.${@tenant}@public\":\"${@name}.${@tenant}@public\"}}"
  },
  {
    "draft": false,
    "lastModified": 1706276036657.0,
    "payload": "{\"id\":\"kpn/schema-store-ui\",\"name\":\"Schema Store UI (beta)\",\"version\":\"0.0.10-beta\",\"vendor\":\"KPN\",\"kind\":\"manifest\",\"apiVersion\":\"v0-alpha\",\"description\":\"Swagger UI documentation of the Schema Store API\",\"moreInfo\":\"\",\"contact\":\"dsh-appcatalog@kpn.com\",\"configuration\":{\"$schema\":\"https://json-schema.org/draft/2019-09/schema\",\"type\":\"object\",\"properties\":{\"DNS_ZONE\":{\"description\":\"DNS zone for the vhost\",\"type\":\"dns-zone\"}}},\"resources\":{\"allocation/${@tenant}/application/${@name}\":{\"cpus\":0.1,\"env\":{\"SR_DOMAIN\":\"api.schema-store.dsh.marathon.mesos\",\"SR_PATH\":\"/\",\"SR_PORT\":\"8443\"},\"exposedPorts\":{\"8080\":{\"auth\":\"system-fwd-auth@view,manage\",\"vhost\":\"{ vhost('${@name}.${@tenant}','${DNS_ZONE}') }\"}},\"image\":\"${@appcatalog}/release/kpn/schema-store-ui:0.0.10\",\"instances\":1,\"mem\":256,\"name\":\"${@name}\",\"needsToken\":true,\"singleInstance\":false,\"user\":\"${@uid}:${@gid}\"},\"allocation/${@tenant}/vhost/${@name}.${@tenant}@${DNS_ZONE}\":\"${@name}.${@tenant}@${DNS_ZONE}\"}}"
  },
  {
    "draft": false,
    "lastModified": 1760614115490.0,
    "payload": "{\"id\":\"kpn/explorer\",\"name\":\"Explorer\",\"version\":\"0.2.5\",\"vendor\":\"KPN\",\"kind\":\"manifest\",\"apiVersion\":\"v0-alpha\",\"description\":\"An application that helps you explore data in your DSH tenant across various locations, such as Kafka topics or S3 buckets.\",\"moreInfo\":\"## Explorer \\nAn application to explore Apache Kafka topics and S3 buckets. \\n\\n### Features \\n- Explore Apache Kafka topics \\n- Explore S3 buckets\",\"contact\":\"unibox@kpn.com\",\"configuration\":{\"$schema\":\"https://json-schema.org/draft/2019-09/schema\",\"type\":\"object\",\"properties\":{\"CPU\":{\"description\":\"Amount of CPU cores per instance.\",\"type\":\"string\",\"default\":\"1\"},\"LOG_LEVEL\":{\"description\":\"The log level for the application. The default is INFO.\",\"type\":\"string\",\"enum\":[\"trace\",\"debug\",\"info\",\"warn\",\"error\"],\"default\":\"info\"},\"MEMORY\":{\"description\":\"Amount of memory per instance.\",\"type\":\"string\",\"default\":\"2048\"},\"VHOST_DNS_ZONE\":{\"description\":\"The DNS zone for the application: kpn.com (public) and kpn.org (private)\",\"type\":\"string\",\"enum\":[\"public\",\"private\"],\"default\":\"private\"}}},\"resources\":{\"allocation/${@tenant}/application/${@name}\":{\"cpus\":\"${CPU | number}\",\"env\":{\"DSH_ENVIRONMENT\":\"{ variables('DSH_ENVIRONMENT') }\",\"DSH_GID\":\"${@gid}\",\"DSH_PLATFORM_REGION\":\"{ variables('DSH_PLATFORM_REGION') }\",\"DSH_TENANT\":\"{ variables('DSH_TENANT') }\",\"DSH_UID\":\"${@uid}\",\"MAXIMUM_FILE_SIZE_BYTE\":\"1024000\",\"RUST_LOG\":\"${LOG_LEVEL}\",\"SERVER_IP_ADDRESS\":\"0.0.0.0:4000\"},\"exposedPorts\":{\"4000\":{\"auth\":\"system-fwd-auth@view,manage\",\"tls\":\"auto\",\"vhost\":\"{ vhost('${@name}.${@tenant}', '${VHOST_DNS_ZONE}') }\"}},\"image\":\"${@appcatalog}/release/kpn/greenbox-compact:0.2.5\",\"imageConsole\":\"registry.cp.kpn-dsh.com/greenbox-dev/greenbox-compact:0.2.5\",\"instances\":1,\"mem\":\"${MEMORY | number}\",\"metrics\":{\"path\":\"/\",\"port\":9009},\"name\":\"${@name}\",\"needsToken\":true,\"secrets\":[{\"injections\":[{\"env\":\"DSH_CLIENT_SECRET\"}],\"name\":\"system/rest-api-client\"},{\"injections\":[{\"env\":\"BUCKET_IDENTIFIER\"}],\"name\":\"system/objectstore/access_key_id\"},{\"injections\":[{\"env\":\"BUCKET_SECRET\"}],\"name\":\"system/objectstore/secret_access_key\"}],\"singleInstance\":true,\"user\":\"${@uid}:${@gid}\"},\"allocation/${@tenant}/bucket/${@name}\":{\"encrypted\":true,\"name\":\"${@name}\",\"versioned\":false},\"allocation/${@tenant}/vhost/${@name}.${@tenant}@${VHOST_DNS_ZONE}\":\"${@name}.${@tenant}@${VHOST_DNS_ZONE}\"}}"
  },
  {
    "draft": false,
    "lastModified": 1737115816172.0,
    "payload": "{\"id\":\"kpn/whoami\",\"name\":\"Who Am I\",\"version\":\"0.0.4\",\"vendor\":\"KPN\",\"kind\":\"manifest\",\"apiVersion\":\"v0-alpha\",\"description\":\"Webserver that prints OS information and HTTP request to output\",\"moreInfo\":\"https://klarrio.com\",\"contact\":\"dsh-appcatalog@kpn.com\",\"configuration\":{\"$schema\":\"https://json-schema.org/draft/2019-09/schema\",\"type\":\"object\",\"properties\":{\"DNS_ZONE\":{\"description\":\"DNS zone name for the vhost\",\"type\":\"dns-zone\"},\"LOG_LEVEL\":{\"description\":\"Log level\",\"type\":\"string\",\"enum\":[\"error\",\"warn\",\"info\"],\"default\":\"info\"}}},\"resources\":{\"allocation/${@tenant}/application/${@name}\":{\"cpus\":0.1,\"env\":{\"LOG_LEVEL\":\"${LOG_LEVEL}\"},\"exposedPorts\":{\"8080\":{\"vhost\":\"{ vhost('${@name}.${@tenant}', '${DNS_ZONE}') }\"}},\"image\":\"${@appcatalog}/release/klarrio/whoami:v1.10.3\",\"instances\":1,\"mem\":128,\"name\":\"${@name}\",\"needsToken\":false,\"singleInstance\":false,\"user\":\"${@uid}:${@gid}\"},\"allocation/${@tenant}/vhost/${@name}.${@tenant}@${DNS_ZONE}\":\"${@name}.${@tenant}@${DNS_ZONE}\"}}"
  },
  {
    "draft": false,
    "lastModified": 1703759217568.0,
    "payload": "{\"id\":\"kpn/dsh-database-ingester\",\"name\":\"DSH Database Ingester\",\"version\":\"0.4.0\",\"vendor\":\"KPN\",\"kind\":\"manifest\",\"apiVersion\":\"v0-alpha\",\"description\":\"Application to stream events from a Kafka topic to a relational database.\",\"moreInfo\":\"## Database Ingester for DSH \\nAn application to stream structured data from a Kafka topic in DSH to any relational database with a JDBC driver by extending the [Kafka Connect JDBC Sink Connector](https://docs.confluent.io/current/connect/kafka-connect-jdbc/sink-connector/index.html). \\n\\n**1. Data Source:** The source topic is configured via `DATABASE_SOURCE_TOPIC` and the `SCHEMA_TYPE` can be Avro, JSON or Protobuf and the schema should be registered in DSH Schema Store. \\n\\n**2. Kafka JDBC Database Ingester Application:** The application can be deployed with single or multiple workers. Cluster mode ensures fault tolerancy and HA. Furthermore, vertical scalability can also be achieved by increasing the number of tasks (`TASK_COUNT`). \\n\\n**3. Data Sink:** Currently, we support SQL Server, Postgres, MySQL, Oracle, SQLite, Sybase and DB2 for `DATABASE_SINK_TABLE`. A `DEAD_LETTER_QUEUE_TOPIC` can also be configured. \\n ### More Information\\nThe _Greenbox Team_ can be reached at **greenbox@kpn.com** for more information.\",\"contact\":\"greenbox@kpn.com\",\"configuration\":{\"$schema\":\"https://json-schema.org/draft/2019-09/schema\",\"type\":\"object\",\"properties\":{\"CPU\":{\"description\":\"Amount of CPU cores per instance.\",\"type\":\"string\",\"default\":\"0.1\"},\"DATABASE\":{\"description\":\"The database to connect to. Currently, we support Postgres (includes Yugabyte), MySQL, Oracle, SQLite, SAP Sybase, Microsoft SQL Server and IBM DB2.\",\"type\":\"string\",\"enum\":[\"postgresql\",\"mysql\",\"oracle\",\"sqlite\",\"sybase\",\"sqlserver\",\"db2\"],\"default\":\"sqlserver\"},\"DATABASE_JDBC_URL\":{\"description\":\"The database host to connect to. Ensure that the host is accessible from the application. Example: jdbc:teradata://tdp.kpn.org\",\"type\":\"string\",\"default\":\"jdbc:teradata://tdp.kpn.org\"},\"DATABASE_PASSWORD_FIELD_IN_SECRET_STORE\":{\"description\":\"Name of the field in the DSH secrets store that contains the Database password.\",\"type\":\"string\"},\"DATABASE_SINK_TABLE\":{\"description\":\"Name of the table in the database to stream the records to.\",\"type\":\"string\",\"default\":\"TABLE_NAME\"},\"DATABASE_SOURCE_TOPIC\":{\"description\":\"Name of the Kafka topic to stream the records from.\",\"type\":\"string\",\"default\":\"scratch.database-ingester-source.greenbox\"},\"DATABASE_TIMEZONE\":{\"description\":\"Used to recognize the timezone of timestamp values. The default is UTC. KPN Teradata uses Europe/Amsterdam.\",\"type\":\"string\",\"default\":\"UTC\"},\"DATABASE_USER\":{\"description\":\"Database user name. Make sure this user has the right permissions to execute the query.\",\"type\":\"string\"},\"DEAD_LETTER_QUEUE_TOPIC\":{\"description\":\"Name of the Kafka topic to stream the dead letter records to in case of errors.\",\"type\":\"string\",\"default\":\"scratch.dead-letter-queue.greenbox\"},\"INSTANCES\":{\"description\":\"The number of instances to deploy in distributed mode (cluster). The default is 1.\",\"type\":\"string\",\"enum\":[\"1\",\"2\",\"3\",\"4\",\"5\"],\"default\":\"1\"},\"LOG_LEVEL\":{\"description\":\"The log level for the application. The default is INFO.\",\"type\":\"string\",\"enum\":[\"TRACE\",\"DEBUG\",\"INFO\",\"WARN\",\"ERROR\"],\"default\":\"INFO\"},\"MEMORY\":{\"description\":\"Amount of memory per instance.\",\"type\":\"string\",\"default\":\"1024\"},\"SCHEMA_TYPE\":{\"description\":\"Schema type of the serialized data in the topic.\",\"type\":\"string\",\"enum\":[\"avro\",\"json\",\"protobuf\"],\"default\":\"avro\"},\"TASK_COUNT\":{\"description\":\"The number of tasks to deploy for better parallelism\",\"type\":\"string\",\"enum\":[\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\"],\"default\":\"3\"}}},\"resources\":{\"allocation/${@tenant}/application/${@name}\":{\"cpus\":\"${CPU | number}\",\"env\":{\"DATABASE\":\"${DATABASE}\",\"DATABASE_JDBC_URL\":\"${DATABASE_JDBC_URL}\",\"DATABASE_SINK_TABLE\":\"${DATABASE_SINK_TABLE}\",\"DATABASE_SOURCE_TOPIC\":\"${DATABASE_SOURCE_TOPIC}\",\"DATABASE_TIMEZONE\":\"${DATABASE_TIMEZONE}\",\"DATABASE_USER\":\"${DATABASE_USER}\",\"DEAD_LETTER_QUEUE_TOPIC\":\"${DEAD_LETTER_QUEUE_TOPIC}\",\"DEPLOYMENT\":\"distributed\",\"DISTRIBUTED_MODE_CONFIG_TOPIC\":\"scratch.${@name}-config.${@tenant}\",\"DISTRIBUTED_MODE_OFFSETS_TOPIC\":\"scratch.${@name}-offset.${@tenant}\",\"DISTRIBUTED_MODE_STATUS_TOPIC\":\"scratch.${@name}-status.${@tenant}\",\"KAFKA_OPTS\":\"-Xms256M -Xmx1000M\",\"LOG_LEVEL\":\"${LOG_LEVEL}\",\"SCHEMA_REGISTRY_URL\":\"https://api.schema-store.dsh.marathon.mesos:8443\",\"SCHEMA_TYPE\":\"${SCHEMA_TYPE}\",\"TASK_COUNT\":\"${TASK_COUNT}\"},\"image\":\"${@appcatalog}/release/kpn/database-ingester-app:0.4.0\",\"imageConsole\":\"registry.cp.kpn-dsh.com/greenbox/database-ingester-app:0.4.0\",\"instances\":\"${INSTANCES | number}\",\"mem\":\"${MEMORY | number}\",\"metrics\":{\"path\":\"/\",\"port\":9009},\"name\":\"${@name}\",\"needsToken\":true,\"secrets\":[{\"injections\":[{\"env\":\"DATABASE_PASSWORD\"}],\"name\":\"${DATABASE_PASSWORD_FIELD_IN_SECRET_STORE}\"}],\"singleInstance\":false,\"user\":\"${@uid}:${@gid}\"},\"allocation/${@tenant}/application/${@name}-ui\":{\"cpus\":0.1,\"env\":{\"CONNECT_URL\":\"${@name}:8083\",\"PORT\":\"8000\"},\"exposedPorts\":{\"8000\":{\"auth\":\"system-fwd-auth@view,manage\",\"tls\":\"auto\",\"vhost\":\"{ vhost('${@name}.${@tenant}', 'public') }\"}},\"image\":\"${@appcatalog}/release/kpn/database-ingester-app-ui:0.4.0\",\"imageConsole\":\"registry.cp.kpn-dsh.com/greenbox/database-ingester-app-ui:0.4.0\",\"instances\":1,\"mem\":8,\"name\":\"${@name}\",\"needsToken\":false,\"singleInstance\":false,\"user\":\"${@uid}:${@gid}\"},\"allocation/${@tenant}/topic/${@name}-config\":{\"kafkaProperties\":{\"cleanup.policy\":\"compact\"},\"name\":\"${@name}-config\",\"partitions\":1,\"replicationFactor\":3},\"allocation/${@tenant}/topic/${@name}-offset\":{\"kafkaProperties\":{\"cleanup.policy\":\"compact\"},\"name\":\"${@name}-offset\",\"partitions\":1,\"replicationFactor\":3},\"allocation/${@tenant}/topic/${@name}-status\":{\"kafkaProperties\":{\"cleanup.policy\":\"compact\"},\"name\":\"${@name}-status\",\"partitions\":1,\"replicationFactor\":3},\"allocation/${@tenant}/vhost/${@name}.${@tenant}@public\":\"${@name}.${@tenant}@public\"}}"
  },
  {
    "draft": false,
    "lastModified": 1699605702561.0,
    "payload": "{\"id\":\"kpn/dsh-database-ingester\",\"name\":\"DSH Database Ingester\",\"version\":\"0.3.0\",\"vendor\":\"KPN\",\"kind\":\"manifest\",\"apiVersion\":\"v0-alpha\",\"description\":\"Application to stream events from a Kafka topic to a relational database.\",\"moreInfo\":\"## Database Ingester for DSH \\nAn application to stream structured data from a Kafka topic in DSH to any relational database with a JDBC driver by extending the [Kafka Connect JDBC Sink Connector](https://docs.confluent.io/current/connect/kafka-connect-jdbc/sink-connector/index.html). \\n\\n**1. Data Source:** The source topic is configured via `DATABASE_SOURCE_TOPIC` and the `SCHEMA_TYPE` can be Avro, JSON or Protobuf and the schema should be registered in DSH Schema Store. \\n\\n**2. Kafka JDBC Database Ingester Application:** The application can be deployed with single or multiple workers. Cluster mode ensures fault tolerancy and HA. Furthermore, vertical scalability can also be achieved by increasing the number of tasks (`TASK_COUNT`). \\n\\n**3. Data Sink:** Currently, we support SQL Server, Postgres, MySQL, Oracle, SQLite, Sybase and DB2 for `DATABASE_SINK_TABLE`. A `DEAD_LETTER_QUEUE_TOPIC` can also be configured. \\n ### More Information\\nThe _Greenbox Team_ can be reached at **greenbox@kpn.com** for more information.\",\"contact\":\"greenbox@kpn.com\",\"configuration\":{\"$schema\":\"https://json-schema.org/draft/2019-09/schema\",\"type\":\"object\",\"properties\":{\"DATABASE\":{\"description\":\"The database to connect to. Currently, we support Postgres (includes Yugabyte), MySQL, Oracle, SQLite, SAP Sybase, Microsoft SQL Server and IBM DB2.\",\"type\":\"string\",\"enum\":[\"postgresql\",\"mysql\",\"oracle\",\"sqlite\",\"sybase\",\"sqlserver\",\"db2\"],\"default\":\"sqlserver\"},\"DATABASE_JDBC_URL\":{\"description\":\"The database host to connect to. Ensure that the host is accessible from the application. Example: jdbc:teradata://tdp.kpn.org\",\"type\":\"string\",\"default\":\"jdbc:teradata://tdp.kpn.org\"},\"DATABASE_PASSWORD_FIELD_IN_SECRET_STORE\":{\"description\":\"Name of the field in the DSH secrets store that contains the Database password.\",\"type\":\"string\"},\"DATABASE_SINK_TABLE\":{\"description\":\"Name of the table in the database to stream the records to.\",\"type\":\"string\",\"default\":\"TABLE_NAME\"},\"DATABASE_SOURCE_TOPIC\":{\"description\":\"Name of the Kafka topic to stream the records from.\",\"type\":\"string\",\"default\":\"scratch.database-ingester-source.greenbox\"},\"DATABASE_TIMEZONE\":{\"description\":\"Used to recognize the timezone of timestamp values. The default is UTC. KPN Teradata uses Europe/Amsterdam.\",\"type\":\"string\",\"default\":\"UTC\"},\"DATABASE_USER\":{\"description\":\"Database user name. Make sure this user has the right permissions to execute the query.\",\"type\":\"string\"},\"DEAD_LETTER_QUEUE_TOPIC\":{\"description\":\"Name of the Kafka topic to stream the dead letter records to in case of errors.\",\"type\":\"string\",\"default\":\"scratch.dead-letter-queue.greenbox\"},\"INSTANCES\":{\"description\":\"The number of instances to deploy in distributed mode (cluster). The default is 1.\",\"type\":\"string\",\"enum\":[\"1\",\"2\",\"3\",\"4\",\"5\"],\"default\":\"1\"},\"LOG_LEVEL\":{\"description\":\"The log level for the application. The default is INFO.\",\"type\":\"string\",\"enum\":[\"TRACE\",\"DEBUG\",\"INFO\",\"WARN\",\"ERROR\"],\"default\":\"INFO\"},\"SCHEMA_TYPE\":{\"description\":\"Schema type of the serialized data in the topic.\",\"type\":\"string\",\"enum\":[\"avro\",\"json\",\"protobuf\"],\"default\":\"avro\"},\"TASK_COUNT\":{\"description\":\"The number of tasks to deploy for better parallelism\",\"type\":\"string\",\"enum\":[\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\"],\"default\":\"3\"}}},\"resources\":{\"allocation/${@tenant}/application/${@name}\":{\"cpus\":0.3,\"env\":{\"DATABASE\":\"${DATABASE}\",\"DATABASE_JDBC_URL\":\"${DATABASE_JDBC_URL}\",\"DATABASE_SINK_TABLE\":\"${DATABASE_SINK_TABLE}\",\"DATABASE_SOURCE_TOPIC\":\"${DATABASE_SOURCE_TOPIC}\",\"DATABASE_TIMEZONE\":\"${DATABASE_TIMEZONE}\",\"DATABASE_USER\":\"${DATABASE_USER}\",\"DEAD_LETTER_QUEUE_TOPIC\":\"${DEAD_LETTER_QUEUE_TOPIC}\",\"DEPLOYMENT\":\"distributed\",\"DISTRIBUTED_MODE_CONFIG_TOPIC\":\"scratch.${@name}-config.${@tenant}\",\"DISTRIBUTED_MODE_OFFSETS_TOPIC\":\"scratch.${@name}-offset.${@tenant}\",\"DISTRIBUTED_MODE_STATUS_TOPIC\":\"scratch.${@name}-status.${@tenant}\",\"KAFKA_OPTS\":\"-Xms256M -Xmx1000M\",\"LOG_LEVEL\":\"${LOG_LEVEL}\",\"SCHEMA_REGISTRY_URL\":\"https://api.schema-store.dsh.marathon.mesos:8443\",\"SCHEMA_TYPE\":\"${SCHEMA_TYPE}\",\"TASK_COUNT\":\"${TASK_COUNT}\"},\"image\":\"${@appcatalog}/release/kpn/database-ingester-app:0.3.0\",\"imageConsole\":\"registry.cp.kpn-dsh.com/greenbox/database-ingester-app:0.3.0\",\"instances\":\"${INSTANCES | number}\",\"mem\":1024,\"metrics\":{\"path\":\"/\",\"port\":9009},\"name\":\"${@name}\",\"needsToken\":true,\"secrets\":[{\"injections\":[{\"env\":\"DATABASE_PASSWORD\"}],\"name\":\"${DATABASE_PASSWORD_FIELD_IN_SECRET_STORE}\"}],\"singleInstance\":false,\"user\":\"${@uid}:${@gid}\"},\"allocation/${@tenant}/application/${@name}-ui\":{\"cpus\":0.1,\"env\":{\"CONNECT_URL\":\"${@name}:8083\",\"PORT\":\"8000\"},\"exposedPorts\":{\"8000\":{\"auth\":\"system-fwd-auth@view,manage\",\"tls\":\"auto\",\"vhost\":\"{ vhost('${@name}.${@tenant}', 'public') }\"}},\"image\":\"${@appcatalog}/release/kpn/database-ingester-app-ui:0.3.0\",\"imageConsole\":\"registry.cp.kpn-dsh.com/greenbox/database-ingester-app-ui:0.3.0\",\"instances\":1,\"mem\":16,\"name\":\"${@name}\",\"needsToken\":false,\"singleInstance\":false,\"user\":\"${@uid}:${@gid}\"},\"allocation/${@tenant}/topic/${@name}-config\":{\"name\":\"${@name}-config\",\"partitions\":1,\"replicationFactor\":3},\"allocation/${@tenant}/topic/${@name}-offset\":{\"name\":\"${@name}-offset\",\"partitions\":1,\"replicationFactor\":3},\"allocation/${@tenant}/topic/${@name}-status\":{\"name\":\"${@name}-status\",\"partitions\":1,\"replicationFactor\":3},\"allocation/${@tenant}/vhost/${@name}.${@tenant}@public\":\"${@name}.${@tenant}@public\"}}"
  },
  {
    "draft": false,
    "lastModified": 1744987928273.0,
    "payload": "{\"id\":\"kpn/kafka-data-archiver\",\"name\":\"Kafka Data Archiver\",\"version\":\"1.6.0\",\"vendor\":\"KPN\",\"kind\":\"manifest\",\"apiVersion\":\"v0-alpha\",\"description\":\"Service to store kafka messages in an object storage like s3 and adls gen2\",\"moreInfo\":\"## Kafka Data Archiver (KDA)\\nA dsh service to store kafka messages in an object storage like S3 or ADLS Gen2. It also allows to convert kafka message into another format using schemas.\\n\\nTo run a KDA, an application configuration should be provided (stored in dsh secrets).\\nHere is an example application configuration file for storing json messages from kafka into AWS S3 bucket as apache parquet format;\\n```\\ninclude \\\"base_application.conf\\\" # Contains application configuration with their default values\\ninclude \\\"base_kafka.conf\\\" # Contains kafka configuration with default values\\n\\nkafka.consumerBootstrapServersOverride = false\\n\\nkafka.consumerBootstrapServers = []\\n\\nkafka.consumerBootstrapServersOverride = false\\n\\nkafka.producerBootstrapServers = []\\nkafka.readTopics = [\\n  \\\"input_topic1\\\"\\n]\\nkafka.groupId = \\\"example_groupid_1\\\"\\n\\napp.maxMessageCountToUpload = 1000 # This is the threshold for number of messages written in a local file before uploading to object storage.\\napp.timeIntervalToUpload = 60.seconds # This is timeout threshold to upload file to object storage.\\npartitionerType = \\\"timebased\\\" # If you want KDA to create timebased partition folders in object storage.\\npartitionerFormat = \\\"yyyy-MM-dd\\\" # Timebased Partition format. (Reference: https://docs.oracle.com/en/java/javase/17/docs/api/java.base/java/time/format/DateTimeFormatter.html#patterns))\\nparquet.compression = \\\"snappy\\\"\\n\\n# Topic input format mapping. You can define multiple topic which contains different message formats.\\ntopic.inputFormat { \\n  \\\"input_topic1\\\": json\\n}\\n\\n# Topic ouput format mapping. You can define multiple topics with is stored in different formats.\\ntopic.outputFormat {\\n  \\\"input_topic1\\\": parquet\\n}\\n\\n# Topic schema id mapping.\\ntopic.schemaIds {\\n  \\\"input_topic1\\\": \\\"schema-v1.avsc\\\"\\n}\\n\\n# Topic s3 bucket mapping. It's possible to map each topic to different s3 bucket and/or prefix.\\ntopic.s3Buckets {\\n  \\\"input_topic1\\\" {\\n    bucket: \\\"bucket1\\\"\\n    prefix: \\\"archived\\\"\\n  }\\n}\\n\\nstorage.type = S3 # Valid values S3 or ADLS_GEN2\\n\\n# Schema Registry Url is needed if message conversion if needed. \\n# Schemas can be stored in s3, adls_gen2 or http service\\nschemaRegistry.url = \\\"s3://bucket1/schemas\\\"\\n\\n# Define aws_access_key_id for each bucket\\nstorage.s3.accessKeyIds {\\n  \\\"bucket1\\\": \\\"value of aws_access_key_id\\\"\\n}\\n\\n# Define aws_secret_key for each bucket\\nstorage.s3.secretKeys {\\n  \\\"bucket1\\\": \\\"value of aws_secret_key\\\"\\n}\\n\\n# Alternatively (do not use both!) if storage.type = adls_gen2\\nschemaRegistry.url = \\\"abfss://testcontainer@testaccount.dfs.core.windows.net/schemas\\\"\\n\\ntopic.adlsAccountNames {\\n  \\\"input_topic2\\\": {\\n    accountName: \\\"testaccount\\\"\\n    container: \\\"testcontainer\\\"\\n    prefix: \\\"archived\\\"\\n  }\\n}\\n\\n# Each input_topic needs to have associated credentials, either on account level or SAS keys\\nstorage.adls.accountKeys {\\n  \\\"testaccount\\\": \\\"\\u003cACCOUNT KEY\\u003e\\\"\\n}\\n\\nstorage.adls.sasKeys {\\n  \\\"testaccount\\\": {\\n    \\\"test\\\": {\\n      \\\"schemas\\\": \\\"\\u003cSAS TOKEN\\u003e\\\"\\n    }\\n    \\\"test\\\": {\\n      \\\"archived\\\": \\\"\\u003cSAS TOKEN\\u003e\\\"\\n    }\\n  }\\n}\\n\\n# What to do with messages that cannot be processed (parsing fails with schema)\\nerror.tolerance = \\\"ignore\\\"\\nerror.routing {\\n  \\\"input_topic1\\\": \\\"dead-letter-queue1\\\"\\n}\\n```\\n\",\"contact\":\"dsh-lfm@kpn.com\",\"configuration\":{\"$schema\":\"https://json-schema.org/draft/2019-09/schema\",\"type\":\"object\",\"properties\":{\"APP_CONFIG_SECRET_NAME\":{\"description\":\"Name of the dsh secret which contains kda application configuration.\",\"type\":\"string\"},\"CPU\":{\"description\":\"Amount of CPU cores per instance.\",\"type\":\"string\",\"default\":\"0.1\"},\"INSTANCES\":{\"description\":\"The number of instances to deploy in distributed mode (cluster). The default is 1.\",\"type\":\"string\",\"default\":\"1\"},\"JVM_MAX_RAM_PERCENTAGE\":{\"description\":\"Sets -XX:MaxRAMPercentage JVM argument\",\"type\":\"string\",\"default\":\"80\"},\"LOG_LEVEL\":{\"description\":\"\",\"type\":\"string\",\"enum\":[\"TRACE\",\"DEBUG\",\"INFO\",\"WARN\",\"ERROR\"],\"default\":\"INFO\"},\"MEMORY\":{\"description\":\"Amount of memory per instance.\",\"type\":\"string\",\"default\":\"512\"}}},\"resources\":{\"allocation/${@tenant}/application/${@name}\":{\"cpus\":\"${CPU | number}\",\"env\":{\"ARCHIVER_LOG_LEVEL\":\"${LOG_LEVEL}\",\"DSH_LOG_LEVEL\":\"${LOG_LEVEL}\",\"MAIN_LOG_LEVEL\":\"${LOG_LEVEL}\",\"MAX_RAM_PERCENTAGE\":\"${JVM_MAX_RAM_PERCENTAGE}\"},\"image\":\"${@appcatalog}/release/kpn/kafka-data-archiver:20250408.21\",\"imageConsole\":\"registry.cp.kpn-dsh.com/kpn-lfm-01/lfm-kda_prod:20250408.21\",\"instances\":\"${INSTANCES | number}\",\"mem\":\"${MEMORY | number}\",\"metrics\":{\"path\":\"/metrics\",\"port\":9090},\"name\":\"${@name}\",\"needsToken\":true,\"secrets\":[{\"injections\":[{\"env\":\"APP_CONF\"}],\"name\":\"${APP_CONFIG_SECRET_NAME}\"}],\"singleInstance\":false,\"user\":\"${@uid}:${@gid}\"}}}"
  },
  {
    "draft": false,
    "lastModified": 1724936904752.0,
    "payload": "{\"id\":\"kpn/airflow-ephemeral\",\"name\":\"Airflow ephemeral\",\"version\":\"0.9.0\",\"vendor\":\"KPN\",\"kind\":\"manifest\",\"apiVersion\":\"v0-alpha\",\"description\":\"Airflow ephemeral instance to test airflow dags. When the instance is down, all state, such as logs and dags will dissappear\",\"moreInfo\":\"## Airflow ephemeral \\nThe airflow ephemeral instance provides a sandbox environment to experiment with Airflow. The python code (DAGs) is pulled in using a gitsync. \\n\\nAirflow version: 2.8.3 \\n\\nPython version: 3.9 \\n\\nGitsync version: v3.6.5 \\n\\n**Disclaimer: This is an ephemeral container, no state is stored. If the container or the platform restarts, you will lose your data.** \\n### Before you start \\nAdd the following secrets to the DSH secret store: \\n- airflow admin user password (create a password that you will use for the airflow ui) \\n- git sync password (can also be a personal access token) \\n\\nThese secrets will be used for the container configuration. \\n### Airflow \\nThe airflow app is an ephemeral app. When it stops or restarts, all state and logs are deleted. \\nTo login to the airflow instance, the default user is `admin` the password is provided by the tenant using the `TENANT_AIRFLOW_ADMIN_USER_PASSWORD_FIELD_IN_SECRET_STORE` environment variable. Which takes the secret you have created earlier from the DSH secrets store and injects it into the container. \\n#### DAGs\\nThe core concept of airflow is a DAG (Directed Acyclic Graph), collecting Tasks together, organized with dependencies and relationships to say how they should run. [airflow dags](https://airflow.apache.org/docs/apache-airflow/stable/core-concepts/dags.html) \\n\\nAll of the code specified in airflow is being wrapped around in tasks and orchestrated by the DAGs. To understand how they work, have a look at the following [Fundamental concepts](https://airflow.apache.org/docs/apache-airflow/stable/tutorial/fundamentals.html). The page also provides a turorial with example DAGs that you can try out.\\n#### Importing packages\\nAt the moment the airflow setup from the app catalog is limited to the pip dependencies that came pre-installed with it. In the future we will provide the option for tenants to install packages to the airflow app. For now you can make use of the [PythonVirtualenvOperator](https://airflow.apache.org/docs/apache-airflow/stable/howto/operator/python.html#pythonvirtualenvoperator)\\n\\nWith the PythonVirtualenvOperator you can pull-in any public package.\\n\\n**Caution: Tenants are responsible for the python packages they import into their airflow container.** \\n### Gitsync \\n\\nThe gitsync will pull in a git branch and put the code in the specified directory. For the gitsync configuration see all variables with the prefix `GIT_SYNC`. You can put all of your DAGs and python code in this git repository, here the python project structure applies. \\n\\nFor the configuration, make sure to set the following variables to the right values:\\n- GIT_SYNC_DEST: the destination on the container where you would like to have the dags pushed to.  e.g. `/opt/airflow/dags/dex`\\n- GIT_SYNC_REPO: url of the git repo to sync.\\n- GIT_SYNC_BRANCH: git branch to pull in.irflow ephemeral \\nThe airflow ephemeral instance provides a sandbox environment to experiment with Airflow. The python code (DAGs) is pulled in using a gitsync. \\n\\nAirflow version: 2.8.3 \\n\\nPython version: 3.9 \\n\\nGitsync version: v3.6.5 \\n\\n**Disclaimer: This is an ephemeral container, no state is stored. If the container or the platform restarts, you will lose your data.** \\n### Before you start \\nAdd the following secrets to the DSH secret store: \\n- airflow admin user password (create a password that you will use for the airflow ui) \\n- git sync password (can also be a personal access token) \\n\\nThese secrets will be used for the container configuration. \\n### Airflow \\nThe airflow app is an ephemeral app. When it stops or restarts, all state and logs are deleted. \\n\\nTo login to the airflow instance, the default user is `admin` the password is provided by the tenant using the `TENANT_AIRFLOW_ADMIN_USER_PASSWORD_FIELD_IN_SECRET_STORE` environment variable. Which takes the secret you have created earlier from the DSH secrets store and injects it into the container. \\n#### DAGs\\nThe core concept of airflow is a DAG (Directed Acyclic Graph), collecting Tasks together, organized with dependencies and relationships to say how they should run. [airflow dags](https://airflow.apache.org/docs/apache-airflow/stable/core-concepts/dags.html)\\n\\nAll of the code specified in airflow is being wrapped around in tasks and orchestrated by the DAGs. To understand how they work, have a look at the following [Fundamental concepts](https://airflow.apache.org/docs/apache-airflow/stable/tutorial/fundamentals.html). The page also provides a turorial with example DAGs that you can try out.\\n#### Importing packages\\nAt the moment the airflow setup from the app catalog is limited to the pip dependencies that came pre-installed with it. In the future we will provide the option for tenants to install packages to the airflow app. For now you can make use of the [PythonVirtualenvOperator](https://airflow.apache.org/docs/apache-airflow/stable/howto/operator/python.html#pythonvirtualenvoperator)\\n\\nWith the PythonVirtualenvOperator you can pull-in any public package.\\n\\n**Caution: Tenants are responsible for the python packages they import into their airflow container.** \\n ### Gitsync \\nThe gitsync will pull in a git branch and put the code in the right directory. For the gitsync configuration see all variables with the prefix `GIT_SYNC`. \\nFor the configuration, make sure to set the following variables to the right values:\\n- GIT_SYNC_DEST: the destination on the container where you would like to have the dags pushed to.  e.g. `/opt/airflow/dags/dex`\\n- GIT_SYNC_REPO: url of the git repo to sync.\\n- GIT_SYNC_BRANCH: git branch to pull in.\\n- GIT_SYNC_WAIT: number of seconds between syncs.\\n- GIT_SYNC_ROOT: where on the root should you do the git operations. the default is sufficient.\\n- GIT_SYNC_ONE_TIME: specifies if the gitsync should continuously pull or exit after the first pull. Default is false. \\n- GIT_SYNC_USERNAME: username of the user to pull in the git repo \\n- GIT_SYNC_PASSWORD_FIELD_IN_SECRET_STORE: the field in the secret store that contains the password of your git account \\n\\nFor some more background have a look at the [gitsync](https://github.com/kubernetes/git-sync/tree/v3.6.5?tab=readme-ov-file#primary-flags) repo. \\n### More Information \\nThe _DEX Team_ can be reached at **dex@kpn.com** for more information.\",\"contact\":\"dex@kpn.com\",\"configuration\":{\"$schema\":\"https://json-schema.org/draft/2019-09/schema\",\"type\":\"object\",\"properties\":{\"CPU\":{\"description\":\"Amount of CPU cores per instance.\",\"type\":\"string\",\"default\":\"2\"},\"GIT_SYNC_BRANCH\":{\"description\":\"git branch to pull in.\",\"type\":\"string\",\"default\":\"dev\"},\"GIT_SYNC_DEST\":{\"description\":\"Folder destination on the host machine where to pull git repo towards.\",\"type\":\"string\",\"default\":\"/opt/airflow/dags/repo_name\"},\"GIT_SYNC_ONE_TIME\":{\"description\":\"specify to only pull on startup or continuously, if true the gitsync exits after the first sync.\",\"type\":\"string\",\"enum\":[\"true\",\"false\"],\"default\":\"false\"},\"GIT_SYNC_PASSWORD_FIELD_IN_SECRET_STORE\":{\"description\":\"Name of the field in the DSH secrets store that contains the Git Sync password.\",\"type\":\"string\"},\"GIT_SYNC_REPO\":{\"description\":\"url of the git repo to sync.\",\"type\":\"string\",\"default\":\"\"},\"GIT_SYNC_ROOT\":{\"description\":\"the root directory for git-sync operations, under which --dest will be created.\",\"type\":\"string\",\"default\":\"/tmp/git\"},\"GIT_SYNC_USERNAME\":{\"description\":\"username of the user to pull in the git repo.\",\"type\":\"string\",\"default\":\"user@github.com\"},\"GIT_SYNC_WAIT\":{\"description\":\"the number of seconds between syncs.\",\"type\":\"string\",\"default\":\"60\"},\"INSTANCES\":{\"description\":\"The number of instances to deploy in distributed mode (cluster). The default is 1.\",\"type\":\"string\",\"enum\":[\"1\",\"2\",\"3\",\"4\",\"5\"],\"default\":\"1\"},\"MEMORY\":{\"description\":\"Amount of memory per instance.\",\"type\":\"string\",\"default\":\"4096\"},\"TENANT_AIRFLOW_ADMIN_USER_PASSWORD_FIELD_IN_SECRET_STORE\":{\"description\":\"Name of the field in the DSH secrets store that contains the airflow admin user password.\",\"type\":\"string\"}}},\"resources\":{\"allocation/${@tenant}/application/${@name}\":{\"cpus\":\"${CPU | number}\",\"env\":{\"GIT_SYNC_BRANCH\":\"${GIT_SYNC_BRANCH}\",\"GIT_SYNC_DEST\":\"${GIT_SYNC_DEST}\",\"GIT_SYNC_ONE_TIME\":\"${GIT_SYNC_ONE_TIME}\",\"GIT_SYNC_REPO\":\"${GIT_SYNC_REPO}\",\"GIT_SYNC_ROOT\":\"${GIT_SYNC_ROOT}\",\"GIT_SYNC_USERNAME\":\"${GIT_SYNC_USERNAME}\",\"GIT_SYNC_WAIT\":\"${GIT_SYNC_WAIT}\"},\"exposedPorts\":{\"8080\":{\"auth\":\"system-fwd-auth@view,manage\",\"vhost\":\"{ vhost('${@name}.${@tenant}','public') }\"}},\"image\":\"${@appcatalog}/release/kpn/airflow-ephemeral:0.9.0\",\"imageConsole\":\"registry.cp.kpn-dsh.com/dex-dev/airflow-ephemeral:0.9.0\",\"instances\":\"${INSTANCES | number}\",\"mem\":\"${MEMORY | number}\",\"name\":\"${@name}\",\"needsToken\":true,\"secrets\":[{\"injections\":[{\"env\":\"GIT_SYNC_PASSWORD\"}],\"name\":\"${GIT_SYNC_PASSWORD_FIELD_IN_SECRET_STORE}\"},{\"injections\":[{\"env\":\"TENANT_AIRFLOW_ADMIN_USER_PASSWORD\"}],\"name\":\"${TENANT_AIRFLOW_ADMIN_USER_PASSWORD_FIELD_IN_SECRET_STORE}\"}],\"singleInstance\":false,\"user\":\"${@uid}:${@gid}\"},\"allocation/${@tenant}/vhost/${@name}.${@tenant}@public\":\"${@name}.${@tenant}@public\"}}"
  },
  {
    "draft": false,
    "lastModified": 1748443791540.0,
    "payload": "{\"id\":\"kpn/oidc-fwd-auth\",\"name\":\"OIDC forward-auth\",\"version\":\"1.1.0\",\"vendor\":\"KPN\",\"kind\":\"manifest\",\"apiVersion\":\"v0-alpha\",\"description\":\"Use custom OIDC forward auth for your services.\",\"moreInfo\":\"# OIDC Forward-Auth\\n\\nUse OIDC forward authentication for your custom services on the DSH.\\n\\n## Description\\n\\nThis app is a custom deployment of [thomseddon/traefik-forward-auth](https://github.com/thomseddon/traefik-forward-auth).\\n\\nThe OIDC Forward-Auth app functions as authentication middleware between your custom service on the DSH, and your OpenID Connect (OIDC) provider:\\n\\n- An external client sends a request to your custom service.\\n- The custom service forwards the request to the OIDC Forward-Auth app, using the latter's endpoint in the service definition.\\n- The OIDC Forward-Auth app checks the request with the OIDC provider.\\n- The OIDC Forward-Auth app forwards the response from the OIDC provider to your custom service.\\n\\nSee [Vhost authentication](https://docs.kpn-dsh.com/kpn-dsh-user-doc-console/custom-services-management/the-service-definition/#vhost-authentication) in the DSH's manual for more information.\\n\\nFollow the steps below to set up forward authentication with the OIDC Forward-Auth app.\\n\\n## Set up OIDC\\n\\nYou can choose from many OIDC providers: KPN's Grip, Microsoft, Google, Keycloak, etc. In order to deploy the OIDC Forward-Auth app, you need the following information from your OIDC provider:\\n\\n- The URL for your client at the OIDC provider\\n- The client ID\\n- The client secret\\n\\n## Configure the OIDC Forward-Auth app\\n\\nBefore deploying the OIDC Forward-Auth app, you need to create 2 secrets on the DSH:\\n\\n- A DSH secret with the client secret that you obtained from your OIDC provider\\n- A DSH secret with a random key to encrypt the cookie with\\n\\nYou can then click \\\"Configure\\\" to set up the app. Fill out the fields below:\\n\\n- **App name**: Fill out a name for your app:\\n  - Only use lowercase letters (a–z), numbers (0–9), or hyphens (`-`).\\n  - The name can’t be longer than 45 characters.\\n  - The name can’t end in a hyphen (`-`) and should start with a lowercase letter.\\n- **Client ID**: The ID of the client at your OIDC provider\\n- **Client secret**: The name of the DSH secret that contains the secret for the client at your OIDC provider\\n- **Cookie secret**: The name of the DSH secret that contains the random key to encrypt the cookie with\\n- **Log level**: Select \\\"Info\\\" for informational logging, or \\\"Debug\\\" for detailed logging for debugging purposes\\n- **OIDC URL**: The URL for the client at your OIDC provider\\n- **UMA authorization**: Whether you want to use User-Managed Access (UMA) authorization\\n\\nClick \\\"Deploy\\\" to deploy the app.\\n\\n## Configure your custom service\\n\\nOnce the OIDC Forward-Auth app is up and running, you can configure one or more of your custom services to actually use it. You can manage this in the service definition of you custom service, see [Vhost authentication](https://docs.kpn-dsh.com/kpn-dsh-user-doc-console/custom-services-management/the-service-definition/#vhost-authentication) in the DSH's manual for more information.\\n\\nIn the service definition, you need to enter the following key and value if you use the OIDC Forward-Auth app:\\n\\n```JSON\\n\\\"auth\\\": \\\"fwd-auth@\\u003capp-name\\u003e@\\u003cheaders\\u003e\\\"`\\n```\\n\\n- `\\u003capp-name\\u003e`: Enter the name that you chose for the OIDC Forward-Auth app.\\n- `\\u003cheaders\\u003e`: A comma separated list of headers to forward to the custom service. The service can then use it to grant permissions based on the user's token.\\n  - `X-Forwarded-Id-Token`: Forward the ID token\\n  - `X-Forwarded-Access-Token`: Forward the access token\",\"contact\":\"sre@kpn-dsh.com\",\"configuration\":{\"$schema\":\"https://json-schema.org/draft/2019-09/schema\",\"type\":\"object\",\"properties\":{\"CLIENT_ID\":{\"description\":\"Enter the ID of the client at your OIDC provider.\",\"type\":\"string\"},\"CLIENT_SECRET\":{\"description\":\"OIDC client secret (name of the DSH secret that contains the value). Enter the name of the DSH secret that contains the client secret at your OIDC provider.\",\"type\":\"string\"},\"COOKIE_SECRET\":{\"description\":\"Random key to encrypt cookie with (name of the DSH secret that contains the value). Enter the name of the DSH secret that contains the random key to encrypt the cookie with.\",\"type\":\"string\"},\"LOG_LEVEL\":{\"description\":\"Choose between informational logging, or detailed logging for debugging purposes.\",\"type\":\"string\",\"enum\":[\"debug\",\"info\"],\"default\":\"info\"},\"OIDC_ISSUER\":{\"description\":\"OIDC issuer url. Enter the the URL for the client at your OIDC provider.\",\"type\":\"string\"},\"TOKEN_VALIDATOR_ENABLED\":{\"description\":\"Add an extra validation step to call the token-introspection endpoint on every request. Default false.\",\"type\":\"string\",\"enum\":[\"true\",\"false\"],\"default\":\"false\"},\"UMA_AUTHORIZATION\":{\"description\":\"Enable UMA authorization (fine grained authorization). Indicate whether you want to activate User-Managed Access (UMA) authorization.\",\"type\":\"string\",\"enum\":[\"true\",\"false\"],\"default\":\"false\"}}},\"resources\":{\"allocation/${@tenant}/application/${@name}\":{\"cpus\":0.1,\"env\":{\"CLIENT_ID\":\"${CLIENT_ID}\",\"COOKIE_NAME\":\"__Secure-${@tenant}_${@name}\",\"COOKIE_SECURE\":\"true\",\"CSRF_COOKIE_NAME\":\"__Secure-${@tenant}_${@name}_csrf\",\"INFO_COOKIE_NAME\":\"__Secure-${@tenant}_${@name}_info\",\"LOG_LEVEL\":\"${LOG_LEVEL}\",\"OIDC_ISSUER\":\"${OIDC_ISSUER}\",\"SECURE\":\"true\",\"TOKEN_VALIDATOR_ENABLED\":\"${TOKEN_VALIDATOR_ENABLED}\",\"UMA_AUTHORIZATION\":\"${UMA_AUTHORIZATION}\"},\"image\":\"${@appcatalog}/release/kpn/traefik-fwd-auth:3.1.0\",\"instances\":1,\"mem\":64,\"name\":\"${@name}\",\"needsToken\":false,\"secrets\":[{\"injections\":[{\"env\":\"CLIENT_SECRET\"}],\"name\":\"${CLIENT_SECRET}\"},{\"injections\":[{\"env\":\"SECRET\"}],\"name\":\"${COOKIE_SECRET}\"}],\"singleInstance\":false,\"user\":\"${@uid}:${@gid}\"}}}"
  },
  {
    "draft": true,
    "lastModified": 1736335477853.0,
    "payload": "{\"id\":\"kpn/greenbox\",\"name\":\"Greenbox\",\"version\":\"0.0.6\",\"vendor\":\"KPN\",\"kind\":\"manifest\",\"apiVersion\":\"v0-alpha\",\"description\":\"An application that helps you explore data in your DSH tenant across various locations, such as Kafka topics or S3 buckets, and manage the compliance of your data.\",\"moreInfo\":\"## Greenbox \\nAn application to explore Apache Kafka topics and S3 buckets. \\n\\n### Features \\n- Explore Apache Kafka topics \\n- Explore S3 buckets \\n- Manage data compliance \\n- Integrated analytics tools \\n\\n### Contact \\nFor more information, please contact\",\"contact\":\"unibox@kpn.com\",\"configuration\":{\"$schema\":\"https://json-schema.org/draft/2019-09/schema\",\"type\":\"object\",\"properties\":{\"AWS_DEFAULT_REGION\":{\"description\":\"The default AWS region.\",\"type\":\"string\",\"enum\":[\"eu-west-1\",\"eu-west-2\",\"eu-west-3\",\"eu-central-1\",\"eu-north-1\",\"us-east-1\",\"us-east-2\",\"us-west-1\",\"us-west-2\",\"ap-south-1\",\"ap-northeast-1\"],\"default\":\"eu-west-1\"},\"CPU\":{\"description\":\"Amount of CPU cores per instance.\",\"type\":\"string\",\"default\":\"0.1\"},\"DSH_REALM\":{\"description\":\"The realm of the DSH tenant.\",\"type\":\"string\",\"enum\":[\"tt-dsh\",\"dev-lz-dsh\",\"prod-lz-dsh\",\"prod-azure-dsh\",\"poc-dsh\"],\"default\":\"dev-lz-dsh\"},\"GREENBOX_CHECK_SERVICE_INTERVAL_SEC\":{\"description\":\"How often greenbox will check dependent services status on dsh in sec.\",\"type\":\"string\",\"enum\":[\"5\",\"15\",\"30\",\"60\",\"75\",\"90\"],\"default\":\"5\"},\"LOG_LEVEL\":{\"description\":\"The log level for the application. The default is INFO.\",\"type\":\"string\",\"enum\":[\"trace\",\"debug\",\"info\",\"warn\",\"error\"],\"default\":\"info\"},\"MEMORY\":{\"description\":\"Amount of memory per instance.\",\"type\":\"string\",\"default\":\"256\"},\"NOTEBOOK_SERVICE_ENABLED\":{\"description\":\"Include Notebook functionality in Greenbox.\",\"type\":\"string\",\"enum\":[\"true\",\"false\"],\"default\":\"true\"},\"SUPERSET_ADMIN_PASSWORD\":{\"description\":\"Name of the dsh secret which contains superset admin password\",\"type\":\"string\"},\"SUPERSET_POSTGRESQL_PASSWORD_SECRET_NAME\":{\"description\":\"Name of the dsh secret which contains superset postgres password secret.\",\"type\":\"string\"},\"SUPERSET_POSTGRES_URI\":{\"description\":\"Name of the dsh secret which contains superset postgres uri. ex: postgresql+psycopg2://sdp:[your postgres password]$@${@name}-superset-postgres:5432/sdp-app?options=-c%20search_path=superset\",\"type\":\"string\"},\"SUPERSET_SECRET_KEY\":{\"description\":\"Name of the dsh secret which contains superset secret key\",\"type\":\"string\"},\"SUPERSET_SERVICE_ENABLED\":{\"description\":\"Include Superset functionality in Greenbox.\",\"type\":\"string\",\"enum\":[\"true\",\"false\"],\"default\":\"true\"},\"SUPERSET_TEST_PASSWORD\":{\"description\":\"Name of the dsh secret which contains superset test user password\",\"type\":\"string\"}}},\"resources\":{\"allocation/${@tenant}/application/${@name}\":{\"cpus\":\"${CPU | number}\",\"env\":{\"AWS_DEFAULT_REGION\":\"${AWS_DEFAULT_REGION}\",\"DSH_GID\":\"${@gid}\",\"DSH_REALM\":\"${DSH_REALM}\",\"DSH_TENANT\":\"${@tenant}\",\"DSH_UID\":\"${@uid}\",\"GREENBOX_CHECK_SERVICE_INTERVAL_SEC\":\"${GREENBOX_CHECK_SERVICE_INTERVAL_SEC}\",\"GREENBOX_EVENT_TOPIC\":\"scratch.${@name}-event.${@tenant}\",\"NOTEBOOK_SERVICE_ENABLED\":\"${NOTEBOOK_SERVICE_ENABLED}\",\"NOTEBOOK_SERVICE_URL\":\"{ vhost('${@name}-jupyterhub.${@tenant}','private') }\",\"RUST_LOG\":\"${LOG_LEVEL}\",\"SERVER_IP_ADDRESS\":\"127.0.0.1:4000\",\"SUPERSET_SERVICE_ENABLED\":\"${SUPERSET_SERVICE_ENABLED}\",\"SUPERSET_SERVICE_URL\":\"{ vhost('${@name}-superset.${@tenant}','private') }\"},\"exposedPorts\":{\"3000\":{\"auth\":\"system-fwd-auth@view,manage\",\"paths\":[{\"prefix\":\"/\"}],\"tls\":\"auto\",\"vhost\":\"{ vhost('${@name}.${@tenant}', 'private') }\"}},\"image\":\"${@appcatalog}/draft/kpn/greenbox:0.0.7\",\"imageConsole\":\"registry.cp.kpn-dsh.com/greenbox-dev/greenbox:0.0.7\",\"instances\":\"${INSTANCES | number}\",\"mem\":\"${MEMORY | number}\",\"metrics\":{\"path\":\"/\",\"port\":9009},\"name\":\"${@name}\",\"needsToken\":true,\"secrets\":[{\"injections\":[{\"env\":\"DSH_CLIENT_SECRET\"}],\"name\":\"rest_client_secret\"},{\"injections\":[{\"env\":\"AWS_ACCESS_KEY_ID\"}],\"name\":\"system/objectstore/access_key_id\"},{\"injections\":[{\"env\":\"AWS_SECRET_ACCESS_KEY\"}],\"name\":\"system/objectstore/secret_access_key\"}],\"singleInstance\":false,\"user\":\"${@uid}:${@gid}\"},\"allocation/${@tenant}/application/${@name}-check-service\":{\"cpus\":0.1,\"env\":{},\"image\":\"${@appcatalog}/draft/kpn/greenbox-check-service:0.0.3\",\"imageConsole\":\"registry.cp.kpn-dsh.com/greenbox-dev/greenbox-check-service:0.0.3\",\"instances\":0,\"mem\":128,\"name\":\"${@name}-check-service-off\",\"needsToken\":false,\"singleInstance\":false,\"user\":\"${@uid}:${@gid}\"},\"allocation/${@tenant}/application/${@name}-jupyterhub\":{\"cpus\":0.5,\"env\":{\"REALM_NAME\":\"${DSH_REALM}\",\"TENANT_NAME\":\"${@tenant}\"},\"exposedPorts\":{\"8000\":{\"auth\":\"system-fwd-auth@view,manage\",\"tls\":\"auto\",\"vhost\":\"{ vhost('${@name}-jupyterhub.${@tenant}', 'private') }\"}},\"image\":\"${@appcatalog}/draft/kpn/greenbox-jupyterhub:0.0.2\",\"imageConsole\":\"registry.cp.kpn-dsh.com/greenbox-dev/greenbox-jupyterhub:0.0.2\",\"instances\":1,\"mem\":1024,\"name\":\"${@name}-jupyterhub\",\"needsToken\":true,\"singleInstance\":false,\"user\":\"${@uid}:${@gid}\"},\"allocation/${@tenant}/application/${@name}-superset\":{\"cpus\":0.75,\"env\":{\"FLASK_APP\":\"superset\",\"SUPERSET_CONFIG_PATH\":\"/home/superset/superset_config.py\",\"TENANT_SUPERSET_TEST_USER_USERNAME\":\"test\"},\"exposedPorts\":{\"8088\":{\"auth\":\"system-fwd-auth@view,manage\",\"vhost\":\"{ vhost('${@name}-superset.${@tenant}', 'private') }\"}},\"image\":\"${@appcatalog}/draft/kpn/superset:pooria.20250107.1\",\"imageConsole\":\"registry.cp.kpn-dsh.com/greenbox-dev/superset-ubuntu:pooria.20250107.1\",\"instances\":1,\"mem\":3072,\"name\":\"${@name}-superset\",\"needsToken\":false,\"secrets\":[{\"injections\":[{\"env\":\"TENANT_SUPERSET_ADMIN_USER_PASSWORD\"}],\"name\":\"${SUPERSET_ADMIN_PASSWORD}\"},{\"injections\":[{\"env\":\"SUPERSET_SECRET_KEY\"}],\"name\":\"${SUPERSET_SECRET_KEY}\"},{\"injections\":[{\"env\":\"TENANT_SUPERSET_TEST_USER_PASSWORD\"}],\"name\":\"${SUPERSET_TEST_PASSWORD}\"},{\"injections\":[{\"env\":\"E2E_SUPERSET_SQLALCHEMY_DATABASE_URI\"}],\"name\":\"${SUPERSET_POSTGRES_URI}\"}],\"singleInstance\":false,\"user\":\"${@uid}:${@gid}\"},\"allocation/${@tenant}/application/${@name}-superset-export\":{\"cpus\":0.1,\"env\":{},\"image\":\"${@appcatalog}/draft/kpn/greenbox-superset-export-service:0.0.1\",\"imageConsole\":\"registry.cp.kpn-dsh.com/greenbox-dev/greenbox-superset-export-service:0.0.1\",\"instances\":0,\"mem\":128,\"name\":\"${@name}-superset-export-service-off\",\"needsToken\":false,\"singleInstance\":false,\"user\":\"${@uid}:${@gid}\"},\"allocation/${@tenant}/application/${@name}-superset-postgres\":{\"cpus\":0.3,\"env\":{\"POSTGRESQL_DATABASE\":\"sdp-app\",\"POSTGRESQL_REPLICATION_USE_PASSFILE\":\"zomaarwat\",\"POSTGRESQL_USERNAME\":\"sdp\"},\"image\":\"${@appcatalog}/draft/kpn/postgres:pooria.20241211.1\",\"imageConsole\":\"registry.cp.kpn-dsh.com/greenbox-dev/postgres:pooria.20241211.1\",\"instances\":1,\"mem\":1024,\"name\":\"${@name}-superset-postgres\",\"needsToken\":false,\"secrets\":[{\"injections\":[{\"env\":\"POSTGRESQL_PASSWORD\"}],\"name\":\"${SUPERSET_POSTGRESQL_PASSWORD_SECRET_NAME}\"}],\"singleInstance\":false,\"user\":\"${@uid}:${@gid}\",\"volumes\":{\"/bitnami/postgresql/\":{\"name\":\"{ volume('${@name}-superset-postgres') }\"}}},\"allocation/${@tenant}/topic/${@name}-event\":{\"kafkaProperties\":{\"cleanup.policy\":\"compact\"},\"name\":\"${@name}-event\",\"partitions\":1,\"replicationFactor\":3},\"allocation/${@tenant}/vhost/${@name}-jupyterhub.${@tenant}@private\":\"${@name}-jupyterhub.${@tenant}@private\",\"allocation/${@tenant}/vhost/${@name}-superset.${@tenant}@private\":\"${@name}-superset.${@tenant}@private\",\"allocation/${@tenant}/vhost/${@name}.${@tenant}@private\":\"${@name}.${@tenant}@private\",\"allocation/${@tenant}/volume/${@name}-superset-postgres\":{\"name\":\"${@name}-superset-postgres\",\"size\":5}}}"
  },
  {
    "draft": false,
    "lastModified": 1703759245782.0,
    "payload": "{\"id\":\"kpn/keyring-kafka-database-extractor\",\"name\":\"DSH Database Extractor\",\"version\":\"0.4.0\",\"vendor\":\"KPN\",\"kind\":\"manifest\",\"apiVersion\":\"v0-alpha\",\"description\":\"Application to bulk load/sync records from a relational database to Kafka in DSH\",\"moreInfo\":\"## Database Extractor for Apache Kafka in DSH \\nAn application to import data from any relational database with a JDBC driver into an Apache Kafka topic by extending the [Kafka Connect JDBC Source connector](https://docs.confluent.io/current/connect/kafka-connect-jdbc/source-connector/index.html) project. \\n\\n**1. Data Source:** Currently, we support Teradata, Postgres (includes Yugabyte), MySQL, Oracle, SQLite, SAP Sybase, Microsoft SQL Server and IBM DB2.\\n\\n**2. Kafka JDBC Database Extractor Application:** The application can be deployed with single or multiple workers. Cluster mode ensures fault tolerancy and HA. The data extraction mode can be configured by setting the `MODE` environment variable. These can be bulk, timestamp, incrementing or timestamp+incrementing. Furthermore, the `POLL_INTERVAL` environment variable can be used to determine the degree of realtime sync or periodicity of sync. \\n\\n**3. Data Sink:** The data is stored in JSON format in the kafka topic `DATABASE_OUTPUT_TOPIC`.\\n ### More Information\\nThe _Greenbox Team_ can be reached at **greenbox@kpn.com** for more information.\",\"contact\":\"greenbox@kpn.com\",\"configuration\":{\"$schema\":\"https://json-schema.org/draft/2019-09/schema\",\"type\":\"object\",\"properties\":{\"CPU\":{\"description\":\"Amount of CPU cores per instance.\",\"type\":\"string\",\"default\":\"0.1\"},\"DATABASE\":{\"description\":\"The database to connect to. Currently, we support Teradata, Postgres (includes Yugabyte), MySQL, Oracle, SQLite, SAP Sybase, Microsoft SQL Server and IBM DB2.\",\"type\":\"string\",\"enum\":[\"teradata\",\"postgresql\",\"mysql\",\"oracle\",\"sqlite\",\"sybase\",\"sqlserver\",\"db2\"],\"default\":\"teradata\"},\"DATABASE_JDBC_URL\":{\"description\":\"The database host to connect to. Ensure that the host is accessible from the application. Example: jdbc:teradata://tdp.kpn.org\",\"type\":\"string\",\"default\":\"jdbc:teradata://tdp.kpn.org\"},\"DATABASE_OUTPUT_TOPIC\":{\"description\":\"Name of the Kafka topic to write the records to.\",\"type\":\"string\",\"default\":\"scratch.database-extractor-output.greenbox\"},\"DATABASE_PASSWORD_FIELD_IN_SECRET_STORE\":{\"description\":\"Name of the field in the DSH secrets store that contains the Database password.\",\"type\":\"string\"},\"DATABASE_QUERY\":{\"description\":\"The SQL query to execute. The query should be a valid SQL query for the database specified in the `DATABASE` environment variable.\",\"type\":\"string\",\"default\":\"SELECT * FROM \\u003ctable_name\\u003e\"},\"DATABASE_TIMEZONE\":{\"description\":\"Used to recognize the timezone of timestamp values. The default is UTC. KPN Teradata uses Europe/Amsterdam.\",\"type\":\"string\",\"default\":\"UTC\"},\"DATABASE_USER\":{\"description\":\"Database user name. Make sure this user has the right permissions to execute the query.\",\"type\":\"string\"},\"INCREMENTING_COLUMN_NAME\":{\"description\":\"This is used in the `incrementing` and `timestamp+incrementing` extraction modes. Only relevant for these two modes. The column name should be a valid autoincrementing column in the database table.\",\"type\":\"string\",\"default\":\"\"},\"INSTANCES\":{\"description\":\"The number of instances to deploy in distributed mode (cluster). The default is 1.\",\"type\":\"string\",\"enum\":[\"1\",\"2\",\"3\",\"4\",\"5\"],\"default\":\"1\"},\"LOG_LEVEL\":{\"description\":\"The log level for the application. The default is INFO.\",\"type\":\"string\",\"enum\":[\"TRACE\",\"DEBUG\",\"INFO\",\"WARN\",\"ERROR\"],\"default\":\"INFO\"},\"MEMORY\":{\"description\":\"Amount of memory per instance.\",\"type\":\"string\",\"default\":\"1024\"},\"MODE\":{\"description\":\"The data extraction mode can be `bulk`, `timestamp`, `incrementing` and `timestamp+incrementing`. Bulk is used for batch extraction whereas the other modes are used for realtime extraction.\",\"type\":\"string\",\"enum\":[\"bulk\",\"timestamp\",\"incrementing\",\"timestamp+incrementing\"],\"default\":\"timestamp\"},\"POLL_INTERVAL\":{\"description\":\"Interval in milliseconds for polling (24 hours is 86400000 milliseconds).\",\"type\":\"string\",\"default\":\"60000\"},\"TIMESTAMP_COLUMN_NAME\":{\"description\":\"This is used in the `timestamp` and `timestamp+incrementing` extraction modes. Only relevant for these two modes. The column name should be a valid date/timestamp column in the database table.\",\"type\":\"string\",\"default\":\"\"}}},\"resources\":{\"allocation/${@tenant}/application/${@name}\":{\"cpus\":\"${CPU | number}\",\"env\":{\"DATABASE\":\"${DATABASE}\",\"DATABASE_JDBC_URL\":\"${DATABASE_JDBC_URL}\",\"DATABASE_OUTPUT_TOPIC\":\"${DATABASE_OUTPUT_TOPIC}\",\"DATABASE_QUERY\":\"${DATABASE_QUERY}\",\"DATABASE_TIMEZONE\":\"${DATABASE_TIMEZONE}\",\"DATABASE_USER\":\"${DATABASE_USER}\",\"DEPLOYMENT\":\"distributed\",\"DISTRIBUTED_MODE_CONFIG_TOPIC\":\"scratch.${@name}-config.${@tenant}\",\"DISTRIBUTED_MODE_OFFSETS_TOPIC\":\"scratch.${@name}-offset.${@tenant}\",\"DISTRIBUTED_MODE_STATUS_TOPIC\":\"scratch.${@name}-status.${@tenant}\",\"INCREMENTING_COLUMN_NAME\":\"${INCREMENTING_COLUMN_NAME}\",\"KAFKA_OPTS\":\"-Xms256M\",\"LOG_LEVEL\":\"${LOG_LEVEL}\",\"MODE\":\"${MODE}\",\"POLL_INTERVAL\":\"${POLL_INTERVAL}\",\"TIMESTAMP_COLUMN_NAME\":\"${TIMESTAMP_COLUMN_NAME}\"},\"image\":\"${@appcatalog}/release/kpn/keyring-kafka-database-extractor:0.4.0\",\"imageConsole\":\"registry.cp.kpn-dsh.com/greenbox/keyring-kafka-database-extractor:0.4.0\",\"instances\":\"${INSTANCES | number}\",\"mem\":\"${MEMORY | number}\",\"metrics\":{\"path\":\"/\",\"port\":9009},\"name\":\"${@name}\",\"needsToken\":true,\"secrets\":[{\"injections\":[{\"env\":\"DATABASE_PASSWORD\"}],\"name\":\"${DATABASE_PASSWORD_FIELD_IN_SECRET_STORE}\"}],\"singleInstance\":false,\"user\":\"${@uid}:${@gid}\"},\"allocation/${@tenant}/application/${@name}-ui\":{\"cpus\":0.1,\"env\":{\"CONNECT_URL\":\"${@name}:8083\",\"PORT\":\"8000\"},\"exposedPorts\":{\"8000\":{\"auth\":\"system-fwd-auth@view,manage\",\"tls\":\"auto\",\"vhost\":\"{ vhost('${@name}.${@tenant}', 'public') }\"}},\"image\":\"${@appcatalog}/release/kpn/keyring-kafka-database-extractor-ui:0.4.0\",\"imageConsole\":\"registry.cp.kpn-dsh.com/greenbox/keyring-kafka-database-extractor-ui:0.4.0\",\"instances\":1,\"mem\":8,\"name\":\"${@name}\",\"needsToken\":false,\"singleInstance\":false,\"user\":\"${@uid}:${@gid}\"},\"allocation/${@tenant}/topic/${@name}-config\":{\"kafkaProperties\":{\"cleanup.policy\":\"compact\"},\"name\":\"${@name}-config\",\"partitions\":1,\"replicationFactor\":3},\"allocation/${@tenant}/topic/${@name}-offset\":{\"kafkaProperties\":{\"cleanup.policy\":\"compact\"},\"name\":\"${@name}-offset\",\"partitions\":1,\"replicationFactor\":3},\"allocation/${@tenant}/topic/${@name}-status\":{\"kafkaProperties\":{\"cleanup.policy\":\"compact\"},\"name\":\"${@name}-status\",\"partitions\":1,\"replicationFactor\":3},\"allocation/${@tenant}/vhost/${@name}.${@tenant}@public\":\"${@name}.${@tenant}@public\"}}"
  },
  {
    "draft": false,
    "lastModified": 1697190232049.0,
    "payload": "{\"id\":\"kpn/dsh-database-ingester\",\"name\":\"DSH Database Ingester\",\"version\":\"0.1.2\",\"vendor\":\"KPN\",\"kind\":\"manifest\",\"apiVersion\":\"v0-alpha\",\"description\":\"Application to stream events from a Kafka topic to a relational database.\",\"moreInfo\":\"## Database Ingester for DSH \\nAn application to stream structured data from a Kafka topic in DSH to any relational database with a JDBC driver by extending the [Kafka Connect JDBC Sink Connector](https://docs.confluent.io/current/connect/kafka-connect-jdbc/sink-connector/index.html). \\n\\n**1. Data Source:** The source topic is configured via `DATABASE_SOURCE_TOPIC` and the `SCHEMA_TYPE` can be Avro, JSON or Protobuf and the schema should be registered in DSH Schema Store. \\n\\n**2. Kafka JDBC Database Ingester Application:** The application can be deployed with single or multiple workers. Cluster mode ensures fault tolerancy and HA. Furthermore, vertical scalability can also be achieved by increasing the number of tasks (`TASK_COUNT`). \\n\\n**3. Data Sink:** Currently, we support SQL Server, Postgres, MySQL, Oracle, SQLite, Sybase and DB2 for `DATABASE_SINK_TABLE`. A `DEAD_LETTER_QUEUE_TOPIC` can also be configured. \\n ### More Information\\nThe _Greenbox Team_ can be reached at **greenbox@kpn.com** for more information.\",\"contact\":\"greenbox@kpn.com\",\"configuration\":{\"$schema\":\"https://json-schema.org/draft/2019-09/schema\",\"type\":\"object\",\"properties\":{\"DATABASE\":{\"description\":\"The database to connect to. Currently, we support Postgres (includes Yugabyte), MySQL, Oracle, SQLite, SAP Sybase, Microsoft SQL Server and IBM DB2.\",\"type\":\"string\",\"enum\":[\"postgresql\",\"mysql\",\"oracle\",\"sqlite\",\"sybase\",\"sqlserver\",\"db2\"],\"default\":\"sqlserver\"},\"DATABASE_JDBC_URL\":{\"description\":\"The database host to connect to. Ensure that the host is accessible from the application. Example: jdbc:teradata://tdp.kpn.org\",\"type\":\"string\",\"default\":\"jdbc:teradata://tdp.kpn.org\"},\"DATABASE_PASSWORD_FIELD_IN_SECRET_STORE\":{\"description\":\"Name of the field in the DSH secrets store that contains the Database password.\",\"type\":\"string\"},\"DATABASE_SINK_TABLE\":{\"description\":\"Name of the table in the database to stream the records to.\",\"type\":\"string\",\"default\":\"TABLE_NAME\"},\"DATABASE_SOURCE_TOPIC\":{\"description\":\"Name of the Kafka topic to stream the records from.\",\"type\":\"string\",\"default\":\"scratch.database-ingester-source.greenbox\"},\"DATABASE_TIMEZONE\":{\"description\":\"Used to recognize the timezone of timestamp values. The default is UTC. KPN Teradata uses Europe/Amsterdam.\",\"type\":\"string\",\"default\":\"UTC\"},\"DATABASE_USER\":{\"description\":\"Database user name. Make sure this user has the right permissions to execute the query.\",\"type\":\"string\"},\"DEAD_LETTER_QUEUE_TOPIC\":{\"description\":\"Name of the Kafka topic to stream the dead letter records to in case of errors.\",\"type\":\"string\",\"default\":\"scratch.dead-letter-queue.greenbox\"},\"INSTANCES\":{\"description\":\"The number of instances to deploy in distributed mode (cluster). The default is 1.\",\"type\":\"string\",\"enum\":[\"1\",\"2\",\"3\",\"4\",\"5\"],\"default\":\"1\"},\"LOG_LEVEL\":{\"description\":\"The log level for the application. The default is INFO.\",\"type\":\"string\",\"enum\":[\"TRACE\",\"DEBUG\",\"INFO\",\"WARN\",\"ERROR\"],\"default\":\"INFO\"},\"SCHEMA_TYPE\":{\"description\":\"Schema type of the serialized data in the topic.\",\"type\":\"string\",\"enum\":[\"avro\",\"json\",\"protobuf\"],\"default\":\"avro\"},\"TASK_COUNT\":{\"description\":\"The number of tasks to deploy for better parallelism\",\"type\":\"string\",\"enum\":[\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\"],\"default\":\"3\"}}},\"resources\":{\"allocation/${@tenant}/application/${@name}\":{\"cpus\":0.3,\"env\":{\"DATABASE\":\"${DATABASE}\",\"DATABASE_JDBC_URL\":\"${DATABASE_JDBC_URL}\",\"DATABASE_SINK_TABLE\":\"${DATABASE_SINK_TABLE}\",\"DATABASE_SOURCE_TOPIC\":\"${DATABASE_SOURCE_TOPIC}\",\"DATABASE_TIMEZONE\":\"${DATABASE_TIMEZONE}\",\"DATABASE_USER\":\"${DATABASE_USER}\",\"DEAD_LETTER_QUEUE_TOPIC\":\"${DEAD_LETTER_QUEUE_TOPIC}\",\"DEPLOYMENT\":\"distributed\",\"DISTRIBUTED_MODE_CONFIG_TOPIC\":\"scratch.${@name}-config.${@tenant}\",\"DISTRIBUTED_MODE_OFFSETS_TOPIC\":\"scratch.${@name}-offset.${@tenant}\",\"DISTRIBUTED_MODE_STATUS_TOPIC\":\"scratch.${@name}-status.${@tenant}\",\"KAFKA_OPTS\":\"-Xms256M -Xmx1000M\",\"LOG_LEVEL\":\"${LOG_LEVEL}\",\"SCHEMA_REGISTRY_URL\":\"https://api.schema-store.dsh.marathon.mesos:8443\",\"SCHEMA_TYPE\":\"${SCHEMA_TYPE}\",\"TASK_COUNT\":\"${TASK_COUNT}\"},\"image\":\"${@appcatalog}/release/kpn/database-ingester-app:0.1.2\",\"imageConsole\":\"registry.cp.kpn-dsh.com/greenbox/database-ingester-app:0.1.2\",\"instances\":\"${INSTANCES | number}\",\"mem\":1024,\"metrics\":{\"path\":\"/\",\"port\":9009},\"name\":\"${@name}\",\"needsToken\":true,\"secrets\":[{\"injections\":[{\"env\":\"DATABASE_PASSWORD\"}],\"name\":\"${DATABASE_PASSWORD_FIELD_IN_SECRET_STORE}\"}],\"singleInstance\":false,\"user\":\"${@uid}:${@gid}\"},\"allocation/${@tenant}/application/${@name}-ui\":{\"cpus\":0.1,\"env\":{\"CONNECT_URL\":\"${@name}:8083\",\"PORT\":\"8000\"},\"exposedPorts\":{\"8000\":{\"auth\":\"system-fwd-auth@view,manage\",\"tls\":\"auto\",\"vhost\":\"{ vhost('${@name}.${@tenant}', 'public') }\"}},\"image\":\"${@appcatalog}/release/kpn/database-ingester-app-ui:0.1.2\",\"imageConsole\":\"registry.cp.kpn-dsh.com/greenbox/database-ingester-app-ui:0.1.2\",\"instances\":1,\"mem\":16,\"name\":\"${@name}\",\"needsToken\":false,\"singleInstance\":false,\"user\":\"${@uid}:${@gid}\"},\"allocation/${@tenant}/topic/${@name}-config\":{\"name\":\"${@name}-config\",\"partitions\":1,\"replicationFactor\":3},\"allocation/${@tenant}/topic/${@name}-offset\":{\"name\":\"${@name}-offset\",\"partitions\":1,\"replicationFactor\":3},\"allocation/${@tenant}/topic/${@name}-status\":{\"name\":\"${@name}-status\",\"partitions\":1,\"replicationFactor\":3},\"allocation/${@tenant}/vhost/${@name}.${@tenant}@public\":\"${@name}.${@tenant}@public\"}}"
  },
  {
    "draft": false,
    "lastModified": 1747751564762.0,
    "payload": "{\"id\":\"kpn/kafka2kafka\",\"name\":\"Kafka2Kafka\",\"version\":\"1.1.0\",\"vendor\":\"KPN\",\"kind\":\"manifest\",\"apiVersion\":\"v0-alpha\",\"description\":\"Service to forward messages from Kafka topic to Kafka topic\",\"moreInfo\":\"## Kafka2Kafka(K2K)\\nA dsh service to forward kafka messages from one kafka topic to one or multiple kafka topics.\\n\\nTo run a K2K, an application configuration should be provided (stored in dsh secrets).\\nHere is an example application configuration file for forwarding kafka messages from topic A to topic B;\\n```batch_size = 5000\\nbatch_duration= 10 #millis\\nprometheus_port = 9090\\n[kafka_consumer]\\ngroup_id = \\\"tenant-name_group_id\\\"\\ntopics = [ \\\"input_topic1\\\" ]\\nstats_interval_ms = 30000\\nuse_ssl_auth = false #Default is false since DSH doesn't support conditional fields in app catalog\\n[kafka_sink_config]\\nbatch_msg_size = 5000 # Amount of messages to be batched before sending to kafka\\nqueue_size = 10000\\nqueue_max_ms = 100\\nlinger_ms = 100\\n[kafka_sink_config.routing_map]\\n\\\"input_topic1\\\" = \\\"output_topic1\\\"\\n\",\"contact\":\"dsh-lfm@kpn.com\",\"configuration\":{\"$schema\":\"https://json-schema.org/draft/2019-09/schema\",\"type\":\"object\",\"properties\":{\"APP_CONFIG_SECRET_NAME\":{\"description\":\"Name of the dsh secret which contains kda application configuration.\",\"type\":\"string\"},\"CPU\":{\"description\":\"Amount of CPU cores per instance.\",\"type\":\"string\",\"default\":\"0.1\"},\"INSTANCES\":{\"description\":\"The number of instances to deploy in distributed mode (cluster). The default is 1.\",\"type\":\"string\",\"default\":\"1\"},\"LOG_LEVEL\":{\"description\":\"\",\"type\":\"string\",\"enum\":[\"trace\",\"debug\",\"info\",\"warn\",\"error\"],\"default\":\"info\"},\"MEMORY\":{\"description\":\"Amount of memory per instance.\",\"type\":\"string\",\"default\":\"300\"}}},\"resources\":{\"allocation/${@tenant}/application/${@name}\":{\"cpus\":\"${CPU | number}\",\"env\":{\"RUST_LOG\":\"${LOG_LEVEL},common_kafka=${LOG_LEVEL},rustls::client=info,hyper=info,reqwest=info,librdkafka=trace,rdkafka::consumer=trace\"},\"image\":\"${@appcatalog}/release/kpn/kafka2kafka:20250520.286\",\"imageConsole\":\"registry.cp.kpn-dsh.com/kpn-lfm-01/glp-rust-lfm-forwarder-kafka_dev:20250520.286\",\"instances\":\"${INSTANCES | number}\",\"mem\":\"${MEMORY | number}\",\"metrics\":{\"path\":\"/metrics\",\"port\":9090},\"name\":\"${@name}\",\"needsToken\":true,\"secrets\":[{\"injections\":[{\"env\":\"APP_CONF\"}],\"name\":\"${APP_CONFIG_SECRET_NAME}\"}],\"singleInstance\":false,\"user\":\"${@uid}:${@gid}\"}}}"
  },
  {
    "draft": false,
    "lastModified": 1752588531777.0,
    "payload": "{\"id\":\"kpn/airflow-ephemeral\",\"name\":\"Airflow ephemeral\",\"version\":\"3.0.2\",\"vendor\":\"KPN\",\"kind\":\"manifest\",\"apiVersion\":\"v0-alpha\",\"description\":\"Airflow ephemeral instance to test airflow dags. When the instance is down, all state, such as logs and dags will dissappear\",\"moreInfo\":\"## Airflow ephemeral\\n\\nThe airflow ephemeral instance provides a sandbox environment to experiment with Airflow. The python code (DAGs) is pulled in using a gitsync.\\n\\nVersion: 3.0.2\\n\\n- Airflow:  3.0.2\\n- Python:   3.12\\n- Gitsync:  4.4.0\\n- Statsd exporter: 0.28.0\\n\\n### Before you start\\n\\nAdd the following secrets to the DSH secret store:\\n\\n1. airflow admin user password (create a password that you will use for the airflow ui)\\n2. git sync password (can also be a personal access token)\\n\\nThese secrets will be used for the container configuration.\\n\\n### Airflow\\n\\nThe airflow app is an ephemeral app. When it stops or restarts, all state and logs are deleted.\\n\\nTo login to the airflow instance, the default user is `admin` the password is provided by the tenant using the `TENANT_AIRFLOW_ADMIN_USER_PASSWORD_FIELD_IN_SECRET_STORE` environment variable. Which takes the secret you have created earlier from the DSH secrets store and injects it into the container.\\n\\n#### DAGs\\n\\nThe core concept of airflow is a DAG (Directed Acyclic Graph), collecting Tasks together, organized with dependencies and relationships to say how they should run. [airflow dags](https://airflow.apache.org/docs/apache-airflow/stable/core-concepts/dags.html)\\n\\nAll of the code specified in airflow is being wrapped around in tasks and orchestrated by the DAGs. To understand how they work, have a look at the following [Fundamental concepts](https://airflow.apache.org/docs/apache-airflow/stable/tutorial/fundamentals.html). The page also provides a turorial with example DAGs that you can try out.\\n\\n#### Importing packages\\n\\nAt the moment the airflow setup from the app catalog is limited to the pip dependencies that came pre-installed with it. In the future we will provide the option for tenants to install packages to the airflow app. For now you can make use of the [PythonVirtualenvOperator](https://airflow.apache.org/docs/apache-airflow/stable/howto/operator/python.html#pythonvirtualenvoperator)\\n\\nWith the PythonVirtualenvOperator you can pull-in any public package.\\n\\n**Caution: Tenants are responsible for the python packages they import into their airflow container. Please follow the KPN Security Policy ([KSP](https://ciso-ksp.kpnnet.org/welcome/KSP)) as a guideline.**\\n\\n### Gitsync\\n\\nThe gitsync will pull in a git branch and put the code in the specified directory. For the gitsync configuration see all variables with the prefix `GIT_SYNC`. You can put all of your DAGs and python code in this git repository, here the python project structure applies.\\n\\nFor the configuration, make sure to set the following variables to the right values:\\n\\n- GIT_SYNC_DEST: the destination on the container where you would like to have the dags pushed to.  e.g. `/opt/airflow/dags/dex`\\n- GITSYNC_REPO: url of the git repo to sync.\\n- GITSYNC_REF: git branch to pull in.\\n- GITSYNC_PERIOD: number of seconds between syncs.\\n- GITSYNC_ROOT: where on the root should you do the git operations. the default is sufficient.\\n- GITSYNC_ONE_TIME: specifies if the gitsync should continuously pull or exit after the first pull. Default is false.\\n- GITSYNC_USERNAME: username of the user to pull in the git repo.\\n- GITSYNC_PASSWORD_FIELD_IN_SECRET_STORE: the field in the secret store that contains the password of your git account.\\n\\nFor some more background have a look at the [gitsync](https://github.com/kubernetes/git-sync/tree/v3.6.5?tab=readme-ov-file#primary-flags) repo.\\n\\n### More Information\\n\\nThe _DEX Team_ can be reached at **\\u003cdex@kpn.com\\u003e** for more information.\",\"contact\":\"dex@kpn.com\",\"configuration\":{\"$schema\":\"https://json-schema.org/draft/2019-09/schema\",\"type\":\"object\",\"properties\":{\"AIRFLOW_CPU\":{\"description\":\"Amount of CPU cores per instance.\",\"type\":\"string\",\"default\":\"1\"},\"AIRFLOW_MEMORY\":{\"description\":\"Amount of memory per instance.\",\"type\":\"string\",\"default\":\"4096\"},\"GITSYNC_LINK\":{\"description\":\"Folder destination on the host machine where to pull git repo towards.\",\"type\":\"string\",\"default\":\"/opt/airflow/dags/repo_name\"},\"GITSYNC_ONE_TIME\":{\"description\":\"specify to only pull on startup or continuously, if true the gitsync exits after the first sync.\",\"type\":\"string\",\"enum\":[\"true\",\"false\"],\"default\":\"false\"},\"GITSYNC_PASSWORD_FIELD_IN_SECRET_STORE\":{\"description\":\"Name of the field in the DSH secrets store that contains the Git Sync password.\",\"type\":\"string\"},\"GITSYNC_PERIOD\":{\"description\":\"the number of seconds between syncs.\",\"type\":\"string\",\"default\":\"60s\"},\"GITSYNC_REF\":{\"description\":\"git branch to pull in.\",\"type\":\"string\",\"default\":\"main\"},\"GITSYNC_REPO\":{\"description\":\"url of the git repo to sync.\",\"type\":\"string\",\"default\":\"\"},\"GITSYNC_USERNAME\":{\"description\":\"username of the user to pull in the git repo. In github the PAT indicates the username, so username will be oauth2.\",\"type\":\"string\",\"default\":\"oauth2\"},\"TENANT_AIRFLOW_ADMIN_USER_PASSWORD_FIELD_IN_SECRET_STORE\":{\"description\":\"Name of the field in the DSH secrets store that contains the airflow admin user password.\",\"type\":\"string\"}}},\"resources\":{\"allocation/${@tenant}/application/${@name}\":{\"cpus\":\"${AIRFLOW_CPU | number}\",\"env\":{\"AIRFLOW__CORE__LOAD_EXAMPLES\":\"false\",\"AIRFLOW__SCHEDULER__SCHEDULER_ZOMBIE_TASK_THRESHOLD\":\"600\",\"APPLICATION_NAME\":\"${@name}\",\"DSH_GIT_SYNC_ENABLED\":\"true\",\"GITSYNC_LINK\":\"${GITSYNC_LINK}\",\"GITSYNC_ONE_TIME\":\"${GITSYNC_ONE_TIME}\",\"GITSYNC_PASSWORD_FILE\":\"/home/dsh/git-pat-sdp\",\"GITSYNC_PERIOD\":\"${GITSYNC_PERIOD}\",\"GITSYNC_REF\":\"${GITSYNC_REF}\",\"GITSYNC_REPO\":\"${GITSYNC_REPO}\",\"GITSYNC_ROOT\":\"/tmp/git\",\"GITSYNC_USERNAME\":\"${GITSYNC_USERNAME}\"},\"exposedPorts\":{\"8080\":{\"auth\":\"system-fwd-auth@view,manage\",\"vhost\":\"{ vhost('${@name}.${@tenant}','private') }\"}},\"image\":\"${@appcatalog}/release/kpn/airflow-ephemeral:3.0.2\",\"imageConsole\":\"registry.cp.kpn-dsh.com/dex-dev/airflow-ephemeral:3.0.2\",\"instances\":1,\"mem\":\"${AIRFLOW_MEMORY | number}\",\"name\":\"${@name}\",\"needsToken\":true,\"secrets\":[{\"injections\":[{\"env\":\"GH_PAT_TOKEN\"}],\"name\":\"${GITSYNC_PASSWORD_FIELD_IN_SECRET_STORE}\"},{\"injections\":[{\"env\":\"TENANT_AIRFLOW_ADMIN_USER_PASSWORD\"}],\"name\":\"${TENANT_AIRFLOW_ADMIN_USER_PASSWORD_FIELD_IN_SECRET_STORE}\"}],\"singleInstance\":true,\"user\":\"${@uid}:${@gid}\"},\"allocation/${@tenant}/vhost/${@name}.${@tenant}@private\":\"${@name}.${@tenant}@private\"}}"
  },
  {
    "draft": false,
    "lastModified": 1708693329209.0,
    "payload": "{\"id\":\"kpn/cmdline\",\"name\":\"Cmd Line\",\"version\":\"1.1.6\",\"vendor\":\"KPN\",\"kind\":\"manifest\",\"apiVersion\":\"v0-alpha\",\"description\":\"A browser based terminal with kafka command line utilities\",\"moreInfo\":\"Access a command line terminal on DSH through your browser. Includes the following tools:\\n  - openssl\\n  - curl\\n  - jq\\n  - kcl\\n  - dshkcl\\n  - nc\\n  - get_signed_certificate.sh\\n\\nkcl is an open source, 'one-stop shop' to do anything with Kafka. - https://github.com/twmb/kcl\\ndshkcl is a wrapper for kcl to deserialize consumed messages from DSH's custom envelope into JSON.\\njq is available for JSON data manipulation - https://stedolan.github.io/jq/\\n\",\"contact\":\"dsh-appcatalog@kpn.com\",\"configuration\":{\"$schema\":\"https://json-schema.org/draft/2019-09/schema\",\"type\":\"object\",\"properties\":{\"DNS_ZONE\":{\"description\":\"DNS zone name for the vhost\",\"type\":\"dns-zone\"}}},\"resources\":{\"allocation/${@tenant}/application/${@name}\":{\"cpus\":0.25,\"env\":{},\"exposedPorts\":{\"8080\":{\"auth\":\"system-fwd-auth@view,manage\",\"tls\":\"auto\",\"vhost\":\"{ vhost('${@name}.${@tenant}', '${DNS_ZONE}') }\"}},\"image\":\"${@appcatalog}/release/kpn/cmdline:1.1.4\",\"instances\":1,\"mem\":256,\"name\":\"${@name}\",\"needsToken\":true,\"singleInstance\":false,\"user\":\"${@uid}:${@gid}\"},\"allocation/${@tenant}/vhost/${@name}.${@tenant}@${DNS_ZONE}\":\"${@name}.${@tenant}@${DNS_ZONE}\"}}"
  },
  {
    "draft": false,
    "lastModified": 1697190900075.0,
    "payload": "{\"id\":\"kpn/keyring-service\",\"name\":\"keyring-service\",\"version\":\"0.5.0\",\"vendor\":\"KPN\",\"kind\":\"manifest\",\"apiVersion\":\"v0-alpha\",\"description\":\"KPN keyring service.\",\"moreInfo\":\"# keyring service\\n\\nThe **keyring service** is a service that exposes a REST-service that maps identifiers from a *domain* to a *codomain*. E.g. it can map Boss identifiers to KRN identifiers.\\n\\n* The **keyring service** runs in your own tenant environment, under your own control.\\n\\n* It optionally provides an OpenAPI specification of the REST-service to import into Postman or a similar application.\\n\\n* It optionally provides a Swagger UI to explore and test the REST-service.\\n\\n* It can expose metrics for a Prometheus scraper.\\n\\nBe sure to check the [keyring documentation](https://keyring.dsh-prod.dsh.prod.aws.kpn.org/using/app-catalog.html) for more and important information.\\n\",\"contact\":\"greenbox@kpn.com\",\"configuration\":{\"$schema\":\"https://json-schema.org/draft/2019-09/schema\",\"type\":\"object\",\"properties\":{\"CODOMAINS\":{\"description\":\"select the codomain(s) for the relation separated by commas (allowed codomain names: BOSS, ECID, ETKN, GPID, KIDH, KRN, SBCM)\",\"type\":\"string\",\"enum\":[\"BOSS\",\"KRN\",\"SBCM\",\"BOSS,KRN,SBCM\",\"ECID\",\"ETKN\",\"GPID\",\"KIDH\",\"ECID,ETKN,GPID,KIDH\",\"BOSS,ECID,ETKN,GPID,KIDH,KRN,SBCM\"]},\"DOMAINS\":{\"description\":\"select the domain(s) for the relation separated by commas (allowed domain names: BOSS, ECID, ETKN, GPID, KIDH, KRN, SBCM)\",\"type\":\"string\",\"enum\":[\"BOSS\",\"KRN\",\"SBCM\",\"BOSS,KRN,SBCM\",\"ECID\",\"ETKN\",\"GPID\",\"KIDH\",\"ECID,ETKN,GPID,KIDH\",\"BOSS,ECID,ETKN,GPID,KIDH,KRN,SBCM\"]},\"ENABLE_METRICS_EXPORTER\":{\"description\":\"select whether to enable exporting metrics\",\"type\":\"string\",\"enum\":[\"false\",\"true\"],\"default\":\"true\"},\"ENABLE_OPEN_API_SPECIFICATION\":{\"description\":\"select whether to enable the openapi specification\",\"type\":\"string\",\"enum\":[\"false\",\"true\"],\"default\":\"true\"},\"ENABLE_SWAGGER_UI\":{\"description\":\"select whether to enable the swagger ui\",\"type\":\"string\",\"enum\":[\"false\",\"true\"],\"default\":\"true\"}}},\"resources\":{\"allocation/${@tenant}/application/${@name}\":{\"cpus\":1,\"env\":{\"BASE_DOMAINS_CONFIG_FILE\":\"base-domains.conf\",\"CODOMAINS\":\"${CODOMAINS}\",\"DEFAULT_CACHE_TYPE\":\"chronicle\",\"DOMAINS\":\"${DOMAINS}\",\"ENABLE_DERIVED_RELATIONS\":\"true\",\"ENABLE_JVM_METRICS_EXPORTER\":\"${ENABLE_METRICS_EXPORTER}\",\"ENABLE_METRICS_EXPORTER\":\"${ENABLE_METRICS_EXPORTER}\",\"ENABLE_OPEN_API_SPECIFICATION\":\"${ENABLE_OPEN_API_SPECIFICATION}\",\"ENABLE_SWAGGER_UI\":\"${ENABLE_SWAGGER_UI}\",\"GROUP_ID_PREFIX\":\"${@tenant}_\",\"HEAP_MEMORY\":\"384\",\"KEYRING_SERVICE_PORT\":\"8088\",\"LOG_LEVEL\":\"info\",\"LOG_LEVEL_CACHE\":\"info\",\"LOG_LEVEL_CODOMAIN_VALUES\":\"info\",\"LOG_LEVEL_ENGINE\":\"info\",\"LOG_LEVEL_ENTRYPOINT\":\"error\",\"LOG_LEVEL_MASTER_RELATION\":\"info\",\"LOG_LEVEL_NETTY\":\"error\",\"LOG_LEVEL_SERVICE\":\"info\",\"MASTER_RELATIONS_CONFIG_FILE\":\"master-relations.conf\",\"METRICS_EXPORTER_PREFIX\":\"\",\"SWAGGER_UI_CONTACT_NAME\":\"Greenbox Team\",\"SWAGGER_UI_CONTACT_URL\":\"https://confluence.kpn.org/display/KEYR/Keyring\",\"SWAGGER_UI_DOCUMENTATION_URL\":\"https://keyring.dsh-prod.dsh.prod.aws.kpn.org\"},\"exposedPorts\":{\"8088\":{\"auth\":\"system-fwd-auth@view,manage\",\"tls\":\"auto\",\"vhost\":\"{ vhost('${@name}.${@tenant}', 'public') }\"}},\"image\":\"${@appstore}/release/kpn/keyring-service:0.5.0\",\"imageConsole\":\"registry.cp.kpn-dsh.com/greenbox-dev/keyring-service:0.5.0\",\"instances\":1,\"mem\":2048,\"metrics\":{\"path\":\"/\",\"port\":9585},\"name\":\"${@name}\",\"needsToken\":true,\"singleInstance\":false,\"user\":\"${@uid}:${@gid}\"},\"allocation/${@tenant}/vhost/${@name}.${@tenant}@public\":\"${@name}.${@tenant}@public\"}}"
  },
  {
    "draft": false,
    "lastModified": 1706273876964.0,
    "payload": "{\"id\":\"kpn/schema-store-ui\",\"name\":\"Schema Store UI (beta)\",\"version\":\"0.0.12\",\"vendor\":\"KPN\",\"kind\":\"manifest\",\"apiVersion\":\"v0-alpha\",\"description\":\"Swagger UI documentation of the Schema Store API\",\"moreInfo\":\"\",\"contact\":\"dsh-appcatalog@kpn.com\",\"configuration\":{\"$schema\":\"https://json-schema.org/draft/2019-09/schema\",\"type\":\"object\",\"properties\":{\"DNS_ZONE\":{\"description\":\"DNS zone for the vhost\",\"type\":\"string\",\"enum\":[\"public\",\"private\"],\"default\":\"public\"}}},\"resources\":{\"allocation/${@tenant}/application/${@name}\":{\"cpus\":0.1,\"env\":{\"SR_DOMAIN\":\"api.schema-store.dsh.marathon.mesos\",\"SR_PATH\":\"/\",\"SR_PORT\":\"8443\"},\"exposedPorts\":{\"8080\":{\"auth\":\"system-fwd-auth@view,manage\",\"vhost\":\"{ vhost('${@name}.${@tenant}','${DNS_ZONE}') }\"}},\"image\":\"${@appcatalog}/release/kpn/schema-store-ui:0.0.12\",\"instances\":1,\"mem\":256,\"name\":\"${@name}\",\"needsToken\":true,\"singleInstance\":false,\"user\":\"${@uid}:${@gid}\"},\"allocation/${@tenant}/vhost/${@name}.${@tenant}@public\":\"${@name}.${@tenant}@${DNS_ZONE}\"}}"
  },
  {
    "draft": true,
    "lastModified": 1727692954523.0,
    "payload": "{\"id\":\"kpn/eavesdropper\",\"name\":\"eavesdropper\",\"version\":\"0.9.3\",\"vendor\":\"KPN\",\"kind\":\"manifest\",\"apiVersion\":\"v0-alpha\",\"description\":\"Web application to visualize messages on Kafka topics in realtime\",\"moreInfo\":\"## Realtime record visualization\\n\\nThis app enables you to view records on your DSH Kafka topics in realtime. It can show the record's keys, values and headers in json, text or binary format, and also recognizes some custom formats for some special topics. Furthermore, it allows you to:\\n\\n* unwrap a record from the envelope that the DSH platform enforces on stream topics, showing the envelope metadata and the envelope payload separately,\\n* filter records based on regular expressions and/or throttling,\\n* show records individually or in list view,\\n* download/copy record values in json, text or binary format.\\n\\nWhen started from the App Catalog the Eavesdropper will be available with the same SSO authorization as the DSH console, and will expose all topics that your tenant is entitled to see.\",\"contact\":\"wilbert.schelvis@kpn.com\",\"configuration\":{\"$schema\":\"https://json-schema.org/draft/2019-09/schema\",\"type\":\"object\",\"properties\":{\"ALLOW_CUSTOM_GROUP_ID\":{\"description\":\"allow custom group id\",\"type\":\"string\",\"enum\":[\"true\",\"false\"],\"default\":\"true\"},\"ENVELOPE_DESERIALIZERS\":{\"description\":\"enable envelope deserializers\",\"type\":\"string\",\"enum\":[\"\",\"dsh-envelope\",\"gzip\",\"dsh-envelope,gzip\"],\"default\":\"\"},\"LOG_LEVEL\":{\"description\":\"application log level\",\"type\":\"string\",\"enum\":[\"error\",\"warn\",\"info\"],\"default\":\"info\"},\"REPRESENTATION_DESERIALIZERS\":{\"description\":\"enable representation deserializers\",\"type\":\"string\",\"enum\":[\"\",\"codomain-values-record\",\"greenbox-ri-avro,greenbox-ri-protobuf\",\"schema-store-deserializer\",\"codomain-values-record,greenbox-ri-avro,greenbox-ri-protobuf,schema-store-deserializer\"],\"default\":\"\"}}},\"resources\":{\"allocation/${@tenant}/application/${@name}\":{\"cpus\":0.1,\"env\":{\"ALLOW_CUSTOM_GROUP_ID\":\"${ALLOW_CUSTOM_GROUP_ID}\",\"CONSUMER_BUILDER\":\"dsh-datastreams-properties\",\"DEFAULT_GROUP_IDS\":\"*\",\"ENVELOPE_DESERIALIZERS\":\"${ENVELOPE_DESERIALIZERS}\",\"EXCLUDED_TOPICS\":\"\",\"EXCLUDED_TOPICS_REGEX\":\"\",\"GROUP_ID_PREFIX\":\"${@tenant}_\",\"INCLUDED_TOPICS\":\"\",\"INCLUDED_TOPICS_REGEX\":\"\",\"INCLUDE___CONSUMER_OFFSETS_TOPIC\":\"false\",\"INSTANCE_IDENTIFIER\":\"${@tenant}_${@name}\",\"LOG_LEVEL\":\"${LOG_LEVEL}\",\"LOG_LEVEL_ENTRYPOINT\":\"${LOG_LEVEL}\",\"LOG_LEVEL_GREENBOX\":\"error\",\"LOG_LEVEL_KAFKA_CLIENT\":\"error\",\"LOG_LEVEL_MONITOR\":\"${LOG_LEVEL}\",\"LOG_LEVEL_RECDES\":\"info\",\"LOG_LEVEL_SERVICE\":\"${LOG_LEVEL}\",\"REPRESENTATION_DESERIALIZERS\":\"${REPRESENTATION_DESERIALIZERS}\"},\"exposedPorts\":{\"8081\":{\"auth\":\"system-fwd-auth@view,manage\",\"tls\":\"auto\",\"vhost\":\"{ vhost('${@name}.${@tenant}', 'public') }\"}},\"image\":\"${@appcatalog}/draft/kpn/eavesdropper:0.9.3\",\"instances\":1,\"mem\":384,\"name\":\"${@name}\",\"needsToken\":true,\"singleInstance\":false,\"user\":\"${@uid}:${@gid}\"},\"allocation/${@tenant}/vhost/${@name}.${@tenant}@public\":\"${@name}.${@tenant}@public\"}}"
  }
]
